# MOD 11
## NetFlow Framework
### Netflow Components
- To obtain NetFlow data for analysis, three components are required to collect, transport, and analyze the traffic:
  - NetFlow **exporter**: Not a dedicated physical device. Rather, it is a router, switch, or firewall, for example. This device has an interface (or interfaces) configured to gather packets into flows. Once gathered, the flows and flow records are sent to the NetFlow collector over User Datagram Protocol (UDP).
  - NetFlow **collector**: A system that ingests the flows sent from the exporter and then stores and processes the flow data. A variety of open-source and licensed collector software is available, such as NfSen, ElastiFlow (which works with Elastic and Kibana), ntop, and Plixer Scrutinizer. A collector can be run on a variety of network systems.
  - NetFlow **analyzer**: A software application that provides the statistics and insights into the flow data. Some examples of analyzers include SolarWinds NetFlow Traffic Analyzer (NTA), ManageEngine NetFlow Analyzer, and Paessler PRTG Network Monitor. 

## NetFlow Applications
### NetFlow Applications
- Certain limitations and scenarios can make NetFlow an alternative solution to PCAP and other solutions for monitoring and analyzing network traffic.
- The following are examples of when NetFlow would be advantageous.

#### Insight on Less Critical Network Segments
- Occasionally, sections of a network (subnets, Virtual Local Area Networks [VLAN], and other segments) are not as important as others.
- Whether they serve as backups, guest networks, or other nonessential segments, these parts of the network do not require the same level of attention that goes into a mission-critical network.
- Therefore, network administrators may forgo the deployment and configuration of full network sensors for these networks or sections.
- However, if high-level insight is still desired, NetFlow can be configured to provide data without the heft of full PCAPs created by sensors. 

#### Limited Quantities of Sensors 
- Sensor coverage may be limited due to the size of the network and the lack of enough sensors to capture each segment.
- Due to this, NetFlow can be used to supplement the need for monitoring.
- By combining PCAP generated by sensors with flows generated by NetFlow-configured routers, switches, and firewalls, administrators can still obtain data on their networks without the full use of sensors. 

#### SPAN Is Unavailable 
- Switched Port Analyzer (SPAN), also known as port mirroring, allows the user to copy traffic from a specific interface on a switch and output it into a monitoring interface.
- From there, the traffic can be viewed in a program such as Wireshark and analyzed.
- However, limitations must be considered for the use of SPAN. First, SPAN requires an available port in order to output its copied traffic to a configured destination.
- If a switch is using all its ports, SPAN cannot be used.
- Second, the destination port must be able to handle the source’s traffic input.
- If an entire VLAN is being copied into a single interface, the port may not be able to handle the bandwidth demands. 
- NetFlow can address these issues. Unlike SPAN, NetFlow does not require a “free” port.
- Instead, interfaces are configured to create NetFlow data from the passing traffic, which is then sent to a collector in intervals rather than as a continuous stream. 

#### Storage Limitations
- Packet capture is a method of capturing traffic in a computer network and then analyzing the contents, or packets.
- This is critically important for CDAs, as it provides oversight to a network’s operations.
- PCAPs have a wide range of uses, such as building audit trails, monitoring bandwidth, and tracking devices.
- However, PCAP data files grow quickly as information is captured by a sensor or interface.
- When storage capacities are limited, the sizes of these files can become a problem.
- Although wiping PCAP logs is a solution, losing potential past forensic data is a risk.
- Additionally, examining massive quantities of PCAP data is time consuming for analysts. 

- As discussed before, NetFlow provides an alternative to PCAP.
- NetFlow is simpler, collecting only the high-level information, which requires less storage space.
- This is especially relevant in high-traffic environments, such as a data center or a Demilitarized Zone (DMZ).
- When massive quantities of traffic are being processed, capturing full copies of the packets places a strain on storage. 


## Anomaly Detection
### NetFlow Search Query Basics
<img width="1999" height="1394" alt="150ccb94-fa9c-4c90-a77f-6fc344e98d94" src="https://github.com/user-attachments/assets/eac69f8a-ca97-4f17-93b3-a9a2ba49f229" />

#### Understanding NefFlow Data in SecOnion
- Network devices (such as routers or switches) using protocols like **NetFlow v5**, **NetFlow v9**, or Internet Protocol Flow Information Export (**IPFIX**), send data to a collector.
- For Security Onion, that collector is **Filebeat** (specifically collected through Filebeat’s NetFlow module). 

#### NetFlow Data Ingestion
- The Filebeat NetFlow module listens on a configured port ([UDP] or [TCP]) for incoming raw NetFlow data from these devices.
- When NetFlow records arrive, the Filebeat NetFlow module **normalizes and enriches** them into one of the nine data types described above, saving the data into **netflow.log**.
- It then forwards the normalized records to Elasticsearch for long-term storage and analysis.

#### Log Generation and Processing
- Filebeat processes the incoming NetFlow data using a pipeline.
- For this lesson’s version of Security Onion, the pipeline is **filebeat-7.17-1-netflow-log-pipeline**. 
- The ingested NetFlow data is broken into hundreds of fields, depending on the NetFlow version and the values present in the data.
- Each field has an associated data type.
- For example, netflow.destination_ipv4_address is an ip type, and netflow.destination_transport_port is an integer type.

- The pipeline performs field extraction, parsing, and normalization to create structured records with predefined fields.
- Common fields that may be used to perform NetFlow queries include, but are not limited to, the following:
  - event.action: netflow_flow
  - event.dataset: netflow.log
  - event.module: netflow
  - flow.id: (random assigned id)
  - input.type: netflow
  - service.type: netflow
  - netflow.destination_ipv4_address
  - netflow.destination_transport_port

### NetFlow Search Queries
- View entire count of NetFlow data: `* | groupby event.dataset`
- Show Source IP with highest occurance: `* | groupby event.dataset netflow.source_ipv4_address`
- Add an additinal column for desination IP" `* | groupby event.dataset netflow.source_ipv4_address netflow.destination_ipv4_address`
- View source port numbers: `* | groupby event.dataset netflow.source_transport_port`
- Create a column of destination ports: `* | groupby event.dataset netflow.source_transport_port netflow.destination_transport_port`

#### NetFlow Queries Using Kibana
- Query for NetFlow data: `event.dataset: "netflow.log"`
  - Add the following fields to the output: `source.ip`, `destination.ip`, `source.port`, `destination.port`
- Refine search to identify IP addresses using port 0: `event.dataset: "netflow.log" and (netflow.source_transport_port: 0 and netflow.destination_transport_port: 0)`
- Search for flows larger than .095MB or larger than 100,000 bytes: `event.dataset: "netflow.log" and network.bytes > 100000`


## Identifying DDos Attack
### Denial of Service Identification
- The goal of a DoS attack is to prevent legitimate users from accessing a service or resource.
- This loss of access can range from a single service or machine to an entire network or multiple networks.
- There are many different categories of DoS attacks, each with their own characteristics and varying degrees of impact.
- A (DDoS) attack is a DoS attack that is launched from more than one attacker-controlled host.
- Hosts used in a DDoS attack might be from a large number of attackers coordinating their attack or from unwitting bots under the control of a single attacker.

- NetFlow data is useful in determining both the presence and magnitude of a DoS attack.
- Although an infinite number of possibilities exists, depending on the network and attack, several common attacks and indicators within NetFlow can be observed.
- Examples of attacks and indicators include high bandwidth/slow performance, Synchronize (**SYN**) floods, amplification Domain Name System (**DNS**) floods, and amplification reflection Synchronize-Acknowledge (**SYN-ACK**) floods. 

#### High Bandwidth and Slow Performance
- Slow performance is an almost-universal sign of a DoS attack.
- Because the goal of a DoS attack is to deny service, causing resources to become disruptively slow is an acceptable outcome from an attacker’s perspective.
- DDoS attacks use multiple systems to overwhelm a target, so higher-than-normal levels of traffic are observed within NetFlow data.
- Depending on the type of attack, these elevated traffic levels may be seen as spikes at precise times or as continuous floods.  

#### SYN Floods 
- The well-known “three-way handshake” used when establishing a (TCP) connection is a frequent target of abuse in a DoS attack.
- Normally, a client sends a SYN message to a server, which then causes the server to reply with a SYN-ACK message.
- Upon receipt of the SYN-ACK message, the client sends an ACK message back to the server. 
<img width="1999" height="476" alt="b2dc2861-072d-4a0a-b64a-9bca7b5018e0" src="https://github.com/user-attachments/assets/2e377feb-3015-4c1c-96e7-f144732ae229" />

- A SYN flood takes advantage of the server’s willingness to send SYN-ACK responses to incoming SYN messages.
- An attacker rapidly sends large quantities of SYN packets to open ports on the target’s server.
- The unwitting server then begins to send SYN-ACK replies to the attacker.
  - However, the attacker’s Internet Protocol (IP) address is often spoofed or fake.
- This means that the SYN-ACK reply never reaches its intended client, and the victim server waits for the acknowledgement of the SYN-ACK with an open connection.
- Although the server can close the port by sending a Reset (RST) packet, another malicious SYN packet arrives before the server can do so, starting the cycle again.
- This state, in which the server awaits ACK packets and cannot close the ports with a RST packet, is known as being half-open.
- Finally, when a legitimate user tries to access the server, the SYN flood attack has left all its connections exhausted, denying the user access. 
<img width="1999" height="776" alt="15ca2769-21d6-4312-abd6-51650bac00e8" src="https://github.com/user-attachments/assets/b0c3a2c1-2a8e-4a38-9109-2520a69c2739" />

- NetFlow data can uniquely distinguish a SYN flood from other types of DoS/DDoS attacks by using the TCP Flags field as an indicator.
- This field is exported as a number. At first glance, the use of a number seems unintuitive, but in practice it allows more information to be included without the use of lengthy strings.
- Decoding the TCP Flags field involves counting binary numbers.
- The TCP flags (URG = Urgent, ACK = Acknowledge, PSH = Push, RST = Reset, SYN = Synchronize, and FIN = Finish) correspond to numbers, as depicted below: 
<img width="1999" height="208" alt="2673928e-8b92-42f6-8333-2e0f76d099f3" src="https://github.com/user-attachments/assets/0a881746-33b3-4f51-9923-154711652b08" />

- To determine which TCP flags are included in the flow, the columns are counted from left to right.
- Because each flag equates to a binary number, they are added together to get the value for the flag.
- For example, a flag with a value of 2 means that the flow has only SYN flags. Additionally, a flag with a value of 17 corresponds to ACK and FIN. 
- This identification of TCP flags can be used in detecting SYN flood attacks, as large amounts of SYN messages are sent.
- Examination of NetFlow data generated during a SYN flood shows large amounts of TCP Flags fields that contain the value 2.
- Using the method of decoding from above, this shows that only a SYN flag was included in the flow. 

#### SYN-ACK Flood
- SYN-ACK floods, like SYN floods, target the server instead of another client.
- In this case, the attacker sends SYN-ACK packets at a high rate to the server.
- When the server receives these packets, significant amounts of processing power are required to determine why these packets are being received out of order (because it is expecting a SYN packet first).
- This high-resource consumption can deny service to legitimate users.   
- Identifying SYN-ACK floods using NetFlow is similar to identifying a SYN flood. Using the TCP flags of the flow, seeing disproportionately high numbers of SYN-ACK flags is a key indicator of the attack. 

#### Amplification DNS Flood
- All amplification attacks rely on the ability of a single system to generate small requests with low effort (low bandwidth, processing power, and memory) and cause a server to generate large, high-effort responses.
- This imbalance of proportions can be exploited in various attacks. 
- In an amplification DNS flood attack, multiple bots are employed by a single attacker.
- Each bot spoofs the IP address of a single victim and proceeds to generate high volumes of requests to a DNS resolver.
- To cause as much traffic congestion as possible, the attacker crafts the DNS request to generate a large response.
- When the bots send requests to the DNS resolver, the massive quantities of traffic are actually sent to the legitimate victim’s IP address, not the attacker’s systems, which are spoofing the address.
- The act of impersonating a victim’s IP address and then having a server send its responses to the actual victim’s IP address is known as reflection.
- This attack can be further modified by sending requests to multiple DNS servers, which all point their responses to the single victim’s machine.
- The result is a massive influx of DNS traffic to the victim’s system, causing a denial of service. 
- DNS flood attacks can be seen in NetFlow by observing the volume of DNS requests and responses.
- Although abnormally high numbers of requests can indicate a DNS flood, the proportion and size of the responses can be a key indicator of an amplification attack using DNS.
- Additionally, DNS floods show high amounts of User Datagram Protocol (UDP) traffic, as the protocol is connectionless and can be spoofed easily. 
<img width="1999" height="1206" alt="7b254876-9875-408e-8405-bb41a91bc4ab" src="https://github.com/user-attachments/assets/02db1f1b-5258-4c56-9533-e9579f492ed7" />

#### Amplification Reflection SYN-ACK Flood
- Amplification reflection SYN-ACK floods operate on the principles of both amplification attacks and reflection attacks.
- In this type of attack, a threat actor spoofs a victim client’s IP address and then sends SYN packets to a server.
- Unlike a SYN flood, in which the attack targets the server itself, a SYN-ACK flood attack targets another client.
- Once the server receives the SYN packets, it attempts to respond to them by sending SYN-ACK packets to the victim’s machine (whose IP address is being spoofed by the attacker).
- Like other amplification attacks, the attacker can send multiple SYN packets to multiple servers, which all concentrate their SYN-ACK responses to the same client. 
- Detecting an amplification reflection SYN-ACK flood with NetFlow data is similar to detecting a SYN flood.
- However, both the SYN-ACK and SYN counts are abnormally high. Under normal circumstances, the SYN, SYN-ACK, and ACK counts constitute a roughly 1:1:1 ratio.
- In a SYN or SYN-ACK flood, the SYN and SYN-ACK numbers are significantly higher than the number of ACK responses. 

<img width="779" height="320" alt="image" src="https://github.com/user-attachments/assets/060f2aa1-2a19-4108-a420-9b753154d356" />

<img width="748" height="383" alt="image" src="https://github.com/user-attachments/assets/8c2a87f1-bcce-4a2e-80d1-6e1050f97592" />

### Mitigating DoS Attacks
- Two DoS attacks were identified in the preceding questions.
- The use of NetFlow data has assisted in identifying the presence of a DoS attack as well as analyzing the specific type of DoS within the network.
- The final step in this process is to identify mechanisms for mitigation.
- These controls help reduce the chances of future attacks of the same nature. 

#### SYN-ACK Flood
﻿- For the time range Jun 3, 2021 @ 23:58:00.000 to Jun 4, 2021 @ 00:00:00.000, the NetFlow data showed that a SYN-ACK flood occurred.
- This was identified by two examples in the data.
  - First, there was only one destination IP, but there were dozens of source IP addresses.
  - Second, the ratio of SYN to SYN-ACK to ACK was massively disproportionate.
    - There were only one SYN packet and three ACK packets, but there were 6,650 SYN-ACK packets.
- These two pieces of evidence show that the attacker in this scenario spoofed multiple IP addresses and then overwhelmed the destination with SYN-ACK packets.
- The server consumes resources while trying to determine why SYN-ACK packets, rather than SYN packets, are arriving first.

- To mitigate this SYN-ACK flood, DoS/DDoS prevention software can be **configured to drop out-of-order SYN-ACK responses** that cause servers to become overwhelmed.
  - Additionally, **tweaking the connection timeout of servers** to prevent exhaustion avoids slowdowns. 

<img width="780" height="312" alt="image" src="https://github.com/user-attachments/assets/a635e800-44f0-4033-b91a-19763901787e" />



#### DNS Flood 
﻿- For the time range Jun 11, 2021 @ 18:28:00.000 to Jun 11, 2021 @ 18:30:00.000, the NetFlow data showed that a DNS flood occurred. 
- This attack was identified by three examples in the data. 
  - First, nearly all traffic in the NetFlow data was UDP rather than TCP.
    - This is a key indicator in a DNS flood, as UDP is connectionless and easier to spoof than TCP.
  - Second, and most obvious, is the high count for DNS traffic in the data.
    - This shows that the attacker was rapidly sending DNS requests through the network.
  - Third, there were dozens of source IP addresses and only one destination IP address. This indicates that the attacker spoofed IP addresses and then sent DNS queries to the single server. 

- To mitigate a DNS flood, well-known techniques may be employed.
- The first mitigation is to **disallow unsolicited DNS responses**.
  - In a legitimate DNS query, a resolver makes a request to a server, and the server sends a response to the resolver.
  - In the case of a DNS flood, the attacker sends responses to a resolver without any requests in the first place.
  - By blocking unsolicited responses, this attack can be reduced.
  - Second, **dropping rapid retransmissions from the same IP address** prevents single spoofed IP addresses from making a large impact.
  - Following this, **enabling a timeout on responses to individual queries** prevents the resolver from becoming overloaded with responding to every malicious request. 
<img width="705" height="350" alt="image" src="https://github.com/user-attachments/assets/c4b09fea-7a04-4d41-8514-d6f245fd8463" />


# MOD 12
## Distributed Network Architectures
### Distributed Network Architecture
- In a distributed network architecture, network devices and resources are divided over several smaller networks.
- Examples of “smaller networks” include single branch locations or buildings within a larger enterprise.
- In addition to distributing the network architecture and components, a distributed network architecture model allows the placement of systems closer to those that use them, which has the potential to decrease overall network utilization while increasing performance to end users.

- Distributed networks rely heavily on **routing** and **switching** protocols, as **each section** of a distributed network is effectively **its own network segment** connected to a larger, overarching network.
- Without the use of routing protocols, changes made in one network segment would not automatically be pushed into other segments of the network.
- This would require significant manual intervention in updating routing information on other network segments.
- Additionally, manual configuration of routing is susceptible to human error, as incorrect configurations can cause significantly adverse impacts to the network at large.

- Within individual network segments, switching protocols are also of importance.
- It is common to have multiple switches in use on a single network segment.
- The use of switching protocols ensures that such phenomena as switching loops do not occur.

<img width="621" height="521" alt="84e5ad82-a769-4d42-9241-ce85bb6ae241" src="https://github.com/user-attachments/assets/a0df0510-aaa1-4879-b52b-d06574afbace" />

### Distributed Network Routing
- Distributed networks rely on dynamic routing protocols to ease the configuration burden on network administrators.
- Dynamic routing protocols are split into two categories:
  - Interior Gateway Protocols (IGP)
    - IGPs are dynamic routing protocols intended for use within a single routing domain.
    - These are the most common protocols used within enterprise networks.
    - The IGP protocols Router Information Protocol (RIP) and Open Shortest Path First (OSPF) are explained in this lesson, but there are numerous other IGP protocols as well, such as Intermediate System to Intermediate System (IS-IS), Interior Gateway Routing Protocol (IGRP), and Enhanced Interior Gateway Routing Protocol (EIGRP).
      ##### Routing Information Protocol (RIP)
      - RIP is one of the earliest-developed dynamic routing protocols.
      - RIP is a distance-vector routing protocol that uses hop count as the primary routing metric and uses UDP port 520.
      - Hop count is a simple metric that represents how many routers traffic travels through on the way to its destination.
      - In the case of RIP, traffic is routed based on the route with the smallest number of hops to the destination.
        <img width="393" height="253" alt="897a95a6-7976-4e1d-ae0d-6cc059b26afb" src="https://github.com/user-attachments/assets/8f3d5773-ad33-479e-adaa-058f50e54492" />
         - Three paths exist between R1 and R8:
          - Red path (solid line): Traverses through two routers, a total of three hops.
          - Green path (dashed line): Traverses through one router, a total of two hops.
          - Blue path (dotted line): Traverses through three routers, a total of four hops.
        - Because RIP routes traffic based on the smallest number of hops, the green path (dashed line) is the chosen route in this scenario.
      ##### Open Shortest Path First
      - OSPF is a link-state protocol.
      - A link may be thought of as an interface on an OSPF-enabled router.
      - The collection of all link-states constitutes the Link-State Database (LSDB).
      - OSPF does not use a transport protocol and encapsulates its traffic directly in Internet Protocol (IP) packets using protocol number 89.
      - Cost is the metric used in OSPF routing to determine the best path to a destination.
      - In an OSPF implementation, each interface of a router has a cost value.
      - The cost is directly related to the bandwidth of an individual interface.
      - The higher bandwidth an interface has, the lower its cost is.
      <img width="393" height="253" alt="d034590e-b5ce-436a-85b7-62fa8871d576" src="https://github.com/user-attachments/assets/e4056c36-fdaa-4b2c-81c7-03f9df9d8050" />
          
          - Three paths exist between R1 and R8:
            - Red path (solid line): Traverses through two routers; the cumulative link cost is 30.
            - Green path (dashed line): Traverses through one router; the cumulative link cost is 40.
            - Blue path (dotted line): Traverses through three routers; the cumulative link cost is 35.
        - In the above OSPF scenario, the red path (solid line) is the best route between R1 and R8, as its cumulative link cost between all routers is the lowest.
        - Even though only one router is between R1 and R8 on the green path (dashed line), the link costs are significantly higher than the red path (solid line).
        - This is likely due to the links on the red path (solid line) having lower bandwidth capabilities.
        - When updating the topology, each router places itself at the root of a tree and calculates the shortest path to each destination based on cumulative cost to reach the destination.
        - This causes each router to have its own topology view, as the shortest path to a destination may be different from different routers.
        - Figure 12.2-4 shows an example of the shortest path tree seen from the perspective of five different interconnected routers.
          <img width="602" height="1109" alt="4d6c9f61-d5d3-433f-ba87-091508b436fb" src="https://github.com/user-attachments/assets/d340be64-f50f-401d-bdee-5c6f36ff1089" />

  - Exterior Gateway Protocols
    - EGPs are routing protocols designed for routing traffic across large-scale networks, such as the internet.
    - Although EGPs are most commonly used on the internet backbone, they may also be used within large enterprise networks.
    ##### Border Gateway Protocol
    - BGP is the premier protocol used throughout the internet and large-scale enterprise networks for routing between Autonomous Systems (AS).
    - BGP is a path-vector protocol that determines the best path to a destination based on available paths and preconfigured rules.
    - Path Attributes (PA) are assigned variables (the rules) that routers use to determine the best path to a destination network.
    - BGP enables routers to exchange information about network prefixes under the responsibility of an AS. In BGP, updates are sent only when changes occur.
    - BGP works by establishing paths, which are a sequence of Autonomous System Numbers (ASN) that traffic must be routed through to reach a destination network.
    - When a BGP router advertises a new network, BGP peers receive the advertisement and append their ASN to the AS_PATH variable of the BGP message.
    - This update is then sent to neighboring BGP peers, which perform the same process.
    - The end result is that, through BGP, routers can determine what paths are available to a destination.
    - Figure 12.2-5 demonstrates how the AS path value is modified as it propagates through BGP peers:
      <img width="721" height="421" alt="8de9df3c-d5ed-4083-9044-e8da043beb05" src="https://github.com/user-attachments/assets/7519c669-b869-4598-ac80-972b1d15e9fc" />
        - In Figure 12.2-5, AS 62262 advertises that it is now the AS for the network 64.210.1.0/24.
        - This route change is advertised to BGP neighbors.
        - The neighboring AS 174 receives the advertisement, appends its ASN to the beginning of the AS_PATH value, and sends an update to its neighbors.
        - In this manner, a full path is generated between AS 62262 and AS 6939.

### Distributed network Switching
#### Spanning Tree Protocol
- In large and distributed networks, it is common for multiple switches to be interconnected.
- Because switches still allow packets to be broadcast to all ports when intended, switched networks are susceptible to broadcast storms when multiple switches are interconnected with redundant connections.
- In the case of interconnected switches, a broadcast storm occurs when a frame marked for broadcast is broadcast out of all ports of a switch and the broadcast repeats through the series of interconnected switches.
- A broadcast storm can consume sufficient network resources to render the network unable to transport normal traffic. 
<img width="392" height="294" alt="b78b91c1-c156-42de-85c1-6acbba1f3064" src="https://github.com/user-attachments/assets/e345ebc1-7edb-4d6c-ae6f-1d9f91fe2390" />

- In Figure 12.2-6, a broadcast packet is sent from a workstation to a switch, and each iteration of the packet is noted by a number in the drawing.
- The switch (interconnected with two other switches) broadcasts the packet out of all ports.
- Each of the two interconnected switches receives the packet and then broadcasts the packet again.
- Due to the presence of a loop in the switching topology, the broadcast continues indefinitely.
- STP was introduced to switching environments to solve this problem.
  - STP is a switching protocol that builds a loop-free topology of an Ethernet network.
  - STP works by creating a tree based on its relationship to other bridges and determining a single path to forward traffic through the topology while blocking traffic on redundant paths that could cause a loop.
  - In STP, switches communicate with their neighbors in the Local Area Network (LAN) using Bridge Protocol Data Units (BPDU).
    - BPDUs are the data frames that contain information about STP and are used to transfer STP information between switches. They are sent by switches on a regular interval.
  - The first goal of STP is to identify the root bridge in the topology.
    - When a switch comes online, it assumes itself as the root bridge and proceeds to use its local bridge ID as the root bridge ID.
      - The bridge ID is a concatenation of the bridge priority (a configurable value on most enterprise switches) and the Media Access Control (MAC) address of the switch.
    - The switch then listens for BPDUs from neighbor switches and performs the following:
      - If the switch receives a BPDU with a higher bridge ID, the ID is ignored and the switch continues to see itself as the root bridge.
      - If the switch receives a BPDU with a lower bridge ID, the switch updates its BPDUs to include the new root bridge ID (the switch with the lowest bridge ID) along with an updated root path cost to reach the new root bridge.
    - The above process continues until all switches in the topology have identified the root bridge switch.
<img width="471" height="431" alt="c618bd18-450f-4586-8812-548187adf01a" src="https://github.com/user-attachments/assets/c91c1cbe-da1b-48b5-bc22-021b60343e69" />

- Figure 12.2-7 shows four interconnected switches and the default bridge priority, MAC address, and resulting bridge ID of each switch.
- All the switches shown are using the default bridge priority, which means the root bridge is determined by selecting the switch with the lowest MAC address.
- In the figure, this is switch C, as its MAC address is a lower value than all the other switches.
- If a network administrator desired a specific switch to be designated as the root bridge, the value of the bridge priority would need to be decreased on that bridge so that it would have the lowest bridge ID.

- Once the root bridge has been elected, non-root switches need to determine the root port.
- The root port is the port used by a switch to forward packets toward the root bridge.
- Each switch determines the root port by identifying the port with the lowest path cost to the root bridge.
- Ports on a switch that facilitate a path to the root bridge for other switches are called designated ports. 
<img width="711" height="221" alt="91651334-b8d6-4ae2-9701-64974a8eb622" src="https://github.com/user-attachments/assets/50a1f394-93d9-4737-b9ce-f5278deb3ee5" />

- Figure 12.2-8 shows the root bridge SW1 sending BPDUs with the path cost to the root bridge to SW2 and SW3.
- SW2 is shown sending a BPDU with the total path cost to the root bridge to SW3.
- Using the information from the BPDUs received at SW3, SW3 designates the lowest cost path (directly to SW1) as the root port (shown as RP in the figure).
- SW2 also designates the path directly to SW1 as the root port.
- The port from SW3 to SW2 is set to a blocking state (shown as B in the figure), as this is a higher cost path to the root bridge and having it enabled would introduce a switching loop.
- Ports in a blocking state are not used to forward any traffic from the switch.
- However, BPDUs continue to be sent on this port so switches are constantly aware of the topology and can adapt to changes dynamically.

- STP is a legacy protocol and has been replaced with newer protocols that accomplish the same goal in most networks.
- However, newer protocols simply build on the foundation of STP and enhance its capabilities.
- Rapid Spanning Tree Protocol (RSTP) is a common and newer implementation of STP that significantly speeds up the functions of STP.
- Whereas STP may require 30 seconds or more to respond to topology changes, RSTP can adapt in less than 3 seconds.
- Cisco switches commonly employ a proprietary standard called Per-VLAN Spanning Tree (PVST) that allows Cisco switches to build separate spanning trees for each Virtual Local Area Network (VLAN) configured on a switch.
- Multiple Spanning Tree Protocol (MSTP) is similar to PVST and incorporates spanning trees for use on VLANs; it is not a proprietary protocol.
- Shortest Path Bridging (SPB) combines technologies from many of these switching protocols and allows for multiple equal cost paths to be active at the same time. 

<img width="766" height="314" alt="image" src="https://github.com/user-attachments/assets/5585da63-cfc6-4d49-a888-153156929837" />


### Distributed Network Architecture and Sensor Placement
#### Sensor Placement Considerations
- The objectives of an engagement must be understood and applied to determine proper sensor placement in a distributed network environment.
- The following questions help to determine proper placement of sensors.

##### What should be avoided in a switched network topology?
- A mission may require the placement of sensors to capture network traffic between devices on the same network segment.
- If the network segment contains multiple interconnected switches, careful consideration must be given to the implementation of network taps and Switched Port Analyzer (SPAN) ports, as STP may impact how packets move between switches. 

<img width="711" height="221" alt="d9dc94ca-ed2a-487b-962c-69e09fcf7419" src="https://github.com/user-attachments/assets/18738afc-5915-4f87-aeea-576d14cc6a96" />

- Figure 12.2-9 shows a network tap placed inline between SW2 and SW3.
- Because STP has designated SW1 as the root bridge and SW2 and SW3 have a direct path to SW1, the implementation of the tap between SW2 and SW3 is not effective.
- Because SW3 has set its port toward SW2 in a blocking state, only STP traffic traverses through the network tap. Standard user traffic is not captured.

<img width="711" height="222" alt="e6b80567-726a-4564-944d-a0700f6ff426" src="https://github.com/user-attachments/assets/3c8e75bd-c040-4e27-8deb-7ee3aa3bf23d" />

<img width="711" height="222" alt="252b637b-5b24-4828-8e58-909df29f5a30" src="https://github.com/user-attachments/assets/8fe03d4f-2bc9-4281-a4b0-8038dc1d0be4" />

- In Figure 12.2-10, the implementation of network taps between SW1 and SW2 and between SW1 and SW3 ensures that intra-network traffic between switches is captured, as the links the taps are implemented on are the designated paths determined by STP.
- In STP, because all traffic destined for another switch is forwarded up to the root bridge, an alternative approach is to implement a SPAN port on the root bridge switch and connect it to a sensor, as shown in Figure 12.2-11.

##### What should be avoided within a routed network topology?
- When traffic capture in a distributed architecture is desired, thought must be given to the operation of routing protocols that are in use within the environment.
- When multiple routers are interconnected, multiple paths may exist between points within the network. 

<img width="551" height="552" alt="a91e1a70-6f00-4994-8b13-6d73a9e1efd4" src="https://github.com/user-attachments/assets/840dae97-5775-4cff-97a4-02b519efa693" />

- Figure 12.2-12 illustrates a distributed network topology.
- OSPF is used within the routing environment shown.
- A mission objective requires capturing and monitoring traffic between routers r1 and r3.
- The OSPF shortest path between the two routers is shown by the green path (dashed line) through router r2.
- Given that only traffic between r1 and r3 is of importance given the objective, a network tap could be implemented on either segment of the shortest (green, dashed-line) path to capture all traffic between r1 and r3.

- Because changes may occur within a routing topology, consideration must be given to the placement of sensors.
- In Figure 12.2-12, assume that the link between r2 and r3 is severed and that the next shortest path (based on OSPF routing) from r3 to r1 is through r5 to r1. In this scenario, placing an additional sensor, such as a network tap, on this path would be ideal to capture traffic when the primary shortest path is severed.

##### What are the trust boundaries?
- Trust boundaries are connections between trusted and untrusted networks or between two networks with different levels of trust and security.
- Trust boundaries are often ideal sensor placement locations, as placing sensors in this location provides the ability to capture all traffic for analysis that ingresses and egresses the trusted zone. 


#### Distributed Network Sensor Placement
- Answering the questions from the previous section guides analysts in determining the ideal placement for monitoring sensors. 

<img width="713" height="513" alt="83e5182b-d367-425f-b015-aa29613cef16" src="https://github.com/user-attachments/assets/77405547-91bc-4ae5-ab70-fb6bb63f5043" />

- Figure 12.2-13 shows a distributed architecture with three switching domains and a routing domain between them.
- Switches that have been elected as the root bridge via STP are marked in red.
- The routers r1, r2, and r3 all perform Network Address Translation (NAT) and use private addresses for the workstations interior to each network segment.

- Consider intelligence for a mission that states that malicious traffic has been observed within the network behind r2.
- Systems within the network behind r2 have been observed making network connections to systems in the network behind r3, which is abnormal behavior.
- The objective of the mission is to place sensors within the switching domains behind r2 and r3 to capture intra-network traffic and to place sensors within the routing domain to capture traffic between r2 and r3.
- The objective also states that traffic should be captured from the interior and exterior perspectives from r2.
- In the switching domain behind r2, the use of network taps is preferred, as this is the origin of the known malicious traffic, and ensuring packets are not lost is critical to the mission.
- (SPAN ports on switches may experience packet drops if the switch becomes overloaded. This does not occur with taps.)

- Given this scenario, sensors may be applied to the architecture, as shown in Figure 12.2-14:

<img width="713" height="563" alt="c2c39678-c61d-4107-9306-bfc51a21e9cc" src="https://github.com/user-attachments/assets/277570e1-5151-4fe7-bb43-d1cc56ba0a07" />

  - Placing a network tap behind r2 ensures that traffic is captured as it enters or leaves the network behind r2 while keeping interior addressing intact.
  - The route behind r2 and r3 is a directly connected route and is used to steer traffic between the networks. A tap at this location ensures that traffic between the networks is captured.
  - Although it is not the shortest path, placing a sensor between r3 and r4 is prudent, as this is the link that is used to route traffic between r2 and r3 if the link between those routers is severed.
  - Implementing a tap between the root bridge switch and the other two switches in the switching domain behind r2 ensures that intra-network traffic between the switches is captured.
    - Placing a tap between the non-root bridges is not an ideal placement, as this would
  - be a blocked path by STP and would carry only STP traffic. Additionally, the use of taps instead of SPAN ports ensures that all traffic is captured, even in the event the switch becomes overloaded.
  - A SPAN port is implemented on the root bridge switch behind r3. The SPAN port may be configured to mirror all traffic from both switches.

<img width="603" height="599" alt="image" src="https://github.com/user-attachments/assets/2b8a3101-9a6f-4380-a67d-70067574cf46" />

<img width="772" height="346" alt="image" src="https://github.com/user-attachments/assets/a4da38e4-7e4a-4905-b734-7bd0c7c4c875" />

<img width="752" height="322" alt="image" src="https://github.com/user-attachments/assets/a58b5e50-785d-41c5-bb82-33d09fde705e" />

<img width="762" height="293" alt="image" src="https://github.com/user-attachments/assets/c5347b11-9af6-4ba3-93cb-bd4498bd1608" />


## Prioritizing Capture Locations
### Capturing Known Threats
- Adversaries in cyberspace may have objectives that target specific industries or organizations, and organizations involved in industries similar to each other may have commonalities in their attack surfaces.
- For example, the energy sector is commonly targeted by threat actors.
- Because organizations in this sector conduct industrial operations, their Operational Technology (OT) networks are a likely target for adversaries.
- For other industries, such as manufacturing, the data within an organization (such as manufacturing plans and drawings) may be of more value to an adversary.
- Understanding an organization’s attack surface in tandem with analyzing intelligence on threat actors can guide analysts in implementing effective monitoring controls based on the operation of the organization.

- Using Cyber Threat Intelligence (CTI), attacks from tracked threat groups may be broken down into patterns of behavior.
- Using this information, analysts can identify key areas in which attacks are more likely to occur, which can guide the effective placement of monitoring sensors and controls.

- Consider the following examples of CTI and Tactics, Techniques, and Procedures (TTP) and how sensors and collection techniques may be implemented to identify potential Malicious Cyber Activity (MCA):

#### Example 1
- **CTI**: A threat actor is known to use phishing emails and establish Command-and-Control (C2) channels to an external C2 infrastructure.
- **TTP**: Place network sensors between user network segments (where target workstations reside) and the edge firewall.
  - Placing sensors internal to the network keeps internal addresses intact, allowing compromised systems to be easily identified.
  - Placing sensors between the firewall and each network segment used by end users ensures full sensor coverage across all potential target workstations.

#### Example 2
- **CTI**: A threat actor is known to use brute force attacks on Virtual Private Network (VPN) endpoints at the network boundary to gain access to a network.
- **TTP**: Collect logs from the network boundary device or edge firewall. Connection logs on these devices may indicate attempts at gaining access to the network.

#### Example 3
- **CTI**: A threat actor is known to target vulnerable services on public-facing web servers and use those vulnerable services to compromise systems and establish C2 channels.
- **TTP**: Place a sensor on the link between the edge firewall and the Demilitarized Zone (DMZ).
  - A sensor in this location allows any potential MCA directed toward the public-facing web server to be captured.
- **TTP**: Collect logs from the network boundary device or edge firewall.
  - CTI may be applied to connection logs to identify if attacks are being attempted or have already occurred.

- Given the examples above, consider the following intelligence:
  - The Advanced Persistent Threat APT41 is assessed to be actively attacking healthcare organizations.
  - Intelligence analysts assess that C2 infrastructure exists in the 104.53.200.0/24 range, and observed TTPs indicate that phishing emails are used for initial compromise and Domain Name System (DNS) is used for C2 communications.
-
- Figure 12.3-1 indicates sensor placement and collection techniques that can capture this activity:

<img width="671" height="632" alt="0b65b550-6718-4a5a-b74c-58555f5b2fb5" src="https://github.com/user-attachments/assets/17715d65-bf7d-43d8-8e18-40552e82377b" />

- Figure 12.3-1 indicates a compromised workstation in the Accounting subnet.
- The compromised workstation is using DNS as a C2 channel to the attacker system, indicated by the red path through the network.
- The callout 1 indicates a sensor placed between the core router (core-rtr) and the Services segment that contains the internal DNS server.
- This is an ideal placement for a sensor, as it captures all internal DNS traffic that can be further inspected for potentially malicious DNS traffic.

<img width="587" height="643" alt="image" src="https://github.com/user-attachments/assets/bb3f23e2-6e48-435a-8d3e-6876cce55162" />

### Internal and External Traffic
- Placing too many sensors on a network may cause the same data to be captured multiple times.
- This consumes resources for storage and also increases the amount of data that analysts must sift through.
- On the other hand, not placing enough sensors on a network may cause important data to be missed.
- Thought should always be given to mission objectives to determine the best placement of sensors.

- Private networks may be divided into two sections: external and internal.
- A firewall or router at the edge of the private network is typically the demarcation point between these two segments.
- The edge device typically performs Network Address Translation (NAT) between the external and internal networks and is also responsible for determining what traffic is allowed to traverse the boundary.
- The nature of traffic internal to a network is also significantly different from that on the external side of the network, and placement of sensors in these two locations may have differing outcomes.

#### External Sensors
- The external section of a network is the connection to an organization’s Internet Service Provider (ISP) or a backbone network service.
- Placing sensors external to the network can provide valuable information, but pitfalls must be considered to ensure that data collection from external sensors is effective:
  - Because an external sensor is exterior to the boundary of a network, it can capture all traffic visible to the external interface of the network boundary devices.
    - This includes legitimate traffic but also traffic that does not belong to an established session within the network (such as network reconnaissance or malicious activity).
  - External sensors are prone to false positives.
    - An external sensor observes all traffic destined to the network, regardless of whether it is legitimate traffic. In scenarios in which an edge firewall is blocking malicious activity, an alert is still generated on an external sensor.
  - When NAT is in use between the internal and external sides of the network, an external sensor sees only the global addresses associated with traffic.
    - To see local addresses for traffic inside the network, an internal sensor must be used.

#### Internal Sensors
- Internal sensors may be implemented to capture intra-network traffic and to capture traffic as it moves toward the network boundary.
- Capturing traffic inside the network boundary also ensures that the captured traffic contains the internal local addresses associated with the traffic.
- Consider the following network diagram:

<img width="551" height="492" alt="17d292ea-97af-4de3-8d75-16c0ff211ba3" src="https://github.com/user-attachments/assets/244169bf-61b9-4c01-abd1-4db11c25d39d" />

- When mission objectives require capturing internal traffic before it leaves the network, a sensor on the segment just inside the network boundary is an ideal placement.
- A sensor in this location ensures that all traffic entering or leaving the network is captured from an internal perspective (including local Internet Protocol [IP] addresses associated with the traffic).
- CTI may be applied to information captured at this sensor to help determine if malicious activity is occurring between internal hosts and malicious actors.
- Although the sensor placement in Figure 12.3-3 is ideal when it is necessary to monitor all traffic moving to or from the network boundary, the single sensor alone does not provide a complete picture of network activity.
- Consider a threat actor who has compromised a system in the Engineering network because a user opened a malicious email attachment.
- The malicious payload attempts to make lateral movement to systems located in the Accounting and Processing networks.
- Although a sensor exists between core-rtr and edge-fw that would capture any C2 communications, network traffic illustrating lateral movement to the Engineering and Processing networks would not be captured, as no sensors exist between these segments.
- Consider another example provided in Figure 12.3-4, which is updated from Figure 12.3-3:

<img width="551" height="552" alt="0722cabc-9d54-468d-92f9-9a94ce5b510b" src="https://github.com/user-attachments/assets/f99d6994-b0c0-4c72-a8a2-99d1c19a7f7e" />

- Figure 12.3-4 indicates the addition of a sensor between the Engineering network segment and core-rtr.
- Placing a sensor in this location allows attempted lateral movement to other network segments to be captured.
- In the event that workstations in other segments become compromised, the presence of the sensor between core-rtr and edge-fw ensures that potential C2 communications are captured and would further drive placement of additional sensors as required.

<img width="694" height="319" alt="image" src="https://github.com/user-attachments/assets/df2271d0-807e-44fb-b18c-27644b482741" />

### Applying Internal vs. External Sensors
- Capturing traffic on an internal network may provide information to determine presence of malicious activity inside the network, but it does not provide a complete picture of all activity.
- Internal sensors cannot provide insight into activity that is attempting to penetrate the network boundary. 

- Edge firewalls and monitoring sensors exterior to the network provide valuable information on potential MCA that cannot be obtained using internal sensors and data points alone.
- Firewall logs are a great source of data when identifying penetrations of the network boundary, as they log activity that is both allowed and rejected.
- Information for both allowed and rejected connections can be used to pivot and correlate data from other sensors throughout the network infrastructure.

- Placement of external sensors can provide CTI that becomes a critical tool for analysts.
- Using external sensors, analysts can accomplish the following objectives:
  - Identify scanning of the network boundary.
  - Identify adversarial tactics attempted before the network is breached.

- External sensors can provide valuable information, but they must be tuned properly to collect useful and actionable information while keeping noise and false positives to a minimum.
- For this reason, external sensors should implement filtering so that only explicitly intended traffic is captured for analysis.

- Consider the following CTI:
  - _An adversary has been observed using brute force techniques in an attempt to gain access to externally accessible VPN endpoints._
  - _The adversary is known to use systems within the range 104.53.200.0/24, and the Hypertext Transfer Protocol Secure (HTTPS) landing page for the VPN gateway is being targeted._ 

- Below are a few examples analysts can follow to tune external sensors so that activity as indicated in the CTI is monitored for:
  - External sensors capturing Packet Capture (PCAP) files may be configured with Berkeley Packet Filters (BPF) so that only traffic from the assessed malicious IP space, or traffic to the HTTPS VPN endpoint, is captured.
  - Firewall logs may be filtered to identify any connections from the assessed threat actor’s IP space.
  - External sensors may be configured to filter traffic destined to the threat actor’s IP space to determine if a compromise has already occurred.

<img width="774" height="370" alt="image" src="https://github.com/user-attachments/assets/ee2a6d51-bb9b-4c09-adc8-3b24d0b7b613" />

<img width="771" height="400" alt="image" src="https://github.com/user-attachments/assets/bc30ad05-cf00-4363-8100-380a98c9d37f" />

<img width="667" height="341" alt="image" src="https://github.com/user-attachments/assets/c25ff031-80be-4672-9155-32215e7dd4b0" />

<img width="729" height="445" alt="image" src="https://github.com/user-attachments/assets/14dd7f45-458c-45ff-b5d7-59107cc0cef2" />


## Internal Threats
### Internal Data Sources
- Observation of internal threats requires effective deployment of internal network sensors.
- This lesson comprises a lab exercise in which analysts access and analyze information captured from network sensors to identify active threats within the network.
- The lab uses the attached documents. One document provides a network topology map, and one document provides a list of TTPs associated with a specific Advanced Persistent Threat (APT) group and a list of network assets.
- In the network map, internal sensors (called onion sensors) are placed between the core router and the edge firewall and between the core router and all other network segments.
- The placement of sensors in this manner ensures a complete picture of internal network traffic. Because this exercise focuses on identifying threats internal to the network, no external sensors are leveraged.



# MOD 13
## Defense in Depth
### Identifying Security Gaps
- The term security gap refers to anywhere in a network that is not protected by existing controls.
- The ability to identify and remediate security gaps is critical in any assessment of a network.
- To complete a security gap assessment, the analyst must first fully understand the environment.
- This understanding requires several pieces of accurate information, including an **asset inventory**, **network maps**, and **identification of Key Terrain in Cyberspace (KT-C)** compiled into a KT-C list.
- All three of these relevant pieces of information are often out of date and should be updated with help from the mission partner before a security gap assessment begins.
- Asset lists should include such information as the Operating System (OS) version and the services each asset uses.
- Identifying KT-C also empowers the analyst to prioritize security controls that remediate security gaps based on risk to assets and mission impact.
- In the identification of KT-C, analysts should note possible avenues of attack as areas to increase visibility.

- Once the information is collected and verified, the analyst can begin to assess the current controls and network visibility.
- Looking for quick wins — controls that can be implemented without additional equipment or architectural changes — is a great way to start the security gap analysis.
- Some common examples of quick wins are as follows:
  - Enable Web Application Firewall (WAF) on an Apache server.
  - Forward logs to an existing syslog server.
  - Use Access Control Lists (ACL) on existing equipment.

### Network Visibility
- Understanding the flow of network traffic and maximizing network visibility are the next steps in a security gap analysis.
- Full network visibility involves collecting logs and network traffic from every device on the network with no blind spots.
- This is not always possible, as financial and practical limitations often force analysts to prioritize what data to collect.
- Identification of KT-C and performance of risk analysis are necessary to prioritize network visibility.
- As part of the risk assessment, analysts should take into consideration situations in which technical controls are not possible.
- For example, if a known vulnerability exists on a service but the service cannot be updated, ensuring visibility and monitoring can be a way to minimize that particular risk.

- To maximize network visibility with limited resources, it is often necessary to create network **choke points**, which involve **directing traffic through a limited number of routes** that can be easily monitored.
- Creating choke points may require **redesigning network architecture** or **installing new devices**.
- To **prevent overcommitting resources** or **monitoring irrelevant information**, keep available resources and mission requirements in mind.
- The following are examples of choke points in network architecture:
  - Force outgoing web traffic through a proxy server.
  - Use an internal Domain Name System (DNS) server to recursively handle all DNS requests.
  - Enforce ACLs to only allow necessary traffic between Local Area Network (LAN) segments.
  - Use a private Virtual LAN (VLAN) in workstation networks to reduce lateral movement.
  - Apply egress firewall rules to restrict outgoing traffic and enforce other choke points (proxy and DNS servers).

### Blind Spots
- Blind spots are areas of a network that existing sensors do not collect data from; they can be another form of a security gap.
- Identifying this form of security gap requires an understanding of the traffic flow of a network and an understanding of which traffic is useful.
- A good example of a blind spot is a sensor that captures all traffic flowing in and out of a router but that may not be able to see traffic in subnets connected to the router.
- In this scenario, a blind spot would exist for the traffic between the subnets, which would render the sensor blind to evidence of lateral movement between the segments.

### Layering Technology
- **Defense in depth** is a security concept that involves _layering defenses to minimize the risk of a single failure of a control from exposing the network_.
- The layered defenses often involve implementing controls at multiple levels.
- These levels can vary, but the following are common examples:
  - **Perimeter security**: Physical security and other administrative controls.
  - **Network security**: Log collection, network firewalls, and Network Intrusion Detection Systems (NIDS).
  - **Endpoint security**: Antivirus and Endpoint Detection and Response (EDR).
  - **Application security**: Email security and Web Application Firewalls (WAF).
  - **Users**: Implementation of training methods to heighten users’ security awareness.
  - **Data**: Data encryption and Data Loss Prevention (DLP) solutions.
<img width="1999" height="1052" alt="d69a490d-4765-47aa-abca-50b2dc04a922" src="https://github.com/user-attachments/assets/abf59ae6-e488-42a6-b8ed-841d41e9bb5c" />

- Defense in depth also applies to layering data collection technologies to minimize gaps in visibility, especially if technical controls do not exist to close those gaps.
- The most common example of this is to collect logs from applications that already have controls in place.
- These logs provide visibility for abuse of an application in the event of failure of other controls that can be used to generate alerts.


### Layering Data Collection
- The application of multiple collection techniques provides an opportunity to layer defensive capabilities and enhance the quality of collected data.
- This methodology minimizes the downside of each individual technology and reduces blind spots.

- A prime example of layered collection is the collection of both Packet Capture (PCAP) data and Cisco® IOS® NetFlow.
- Although PCAP contains all the information available in NetFlow (in fact, NetFlow can be generated using PCAP), storing and analyzing PCAP is resource intensive and has diminishing returns over a long period of time. NetFlow, on the other hand, contains records of session data and, as a result, is much less resource intensive to store and process.
- NetFlow can be stored for a much longer time period than PCAP, which is rolled over frequently in most environments.
- Collecting data for both PCAP and NetFlow allows analysts to investigate recent traffic with PCAP and have a long-term record to help understand typical traffic patterns.
- NetFlow data also allows an analyst to perform investigations over a longer period of time, albeit with lower fidelity than PCAP data.

- Web proxy servers are another great example of layered data collection.
- Proxy servers can be configured to forward logs about encrypted traffic that can give analysts insights into traffic not accessible using PCAP.
- Proxy servers are only able to view encrypted traffic in plaintext if the proxy has Secure Sockets Layer/Transport Layer Security (SSL/TLS) decryption capabilities.
- Not all web proxy servers have these capabilities, and external factors, such as mission requirements, classification, and legal concerns, may limit the viability of this solution.

### Solutions to Layering Technology
- No catch-all solution exists for defense in depth for a network.
- Each solution should be determined as part of an iterative process between the analysts and the mission partner and should take into account the special limitations and requirements of each situation.
- Making these determinations requires an understanding of the various technologies available and should be applied in a way that can meet or exceed mission requirements.
- The table below identifies common technologies that can be layered.
<img width="1740" height="1999" alt="cba9557d-c99f-46f2-8bb1-64aede23c758" src="https://github.com/user-attachments/assets/ef441edc-0e98-4e23-b4d3-a180eeb3cd95" />

- For example, if a mission partner required a solution to limit access to the entire network, the analyst could configure an ACL in a firewall.
- If the mission partner hosts a website, implementing a WAF can use advanced application level inspection to stop and alert on known web attacks, such as Structured Query Language (SQL) injection.
- In most implementations, both of these firewall solutions can be configured to forward logs to a sensor for centralization and further analysis.
- This network sensor can be configured to include such alerting capabilities as Zeek or Suricata that can turn the sensor into a NIDS.
- By consolidating multiple data sources into one or more sensors, analysts can maintain layering capabilities while keeping costs low and centralizing sources making it easier to access.

- Once the initial recommendations have been made, analysts should perform another security gap analysis, keeping in mind the new changes.
- This process should be repeated until all gaps have been closed or a determination is made that the gap must remain open.
- These cases are rare and should be documented.

<img width="748" height="379" alt="image" src="https://github.com/user-attachments/assets/27e1b75d-28e0-4d46-a7fe-6e7fbada1060" />

## Sensor Technologies
### Network Sensor Tools
- Network sensors are devices connected to a network designed to collect logs and traffic for use by an analyst.
- Sensors may be as simple as a small laptop, but they are usually servers with a large storage capacity and a dedicated Network Interface Card (NIC) with multiple interfaces.
- Sensors are often customized for each deployment and contain only necessary tools. Some of the most common network tools are discussed in this section.

### Network Metadata Tools
#### Zeek
- Zeek, formerly Bro, is an open-source Network Security Monitoring (NSM) framework capable of extracting high-fidelity network transaction logs and file content from network traffic.
- Zeek is customizable and extremely scalable, capable of operating in high-performance, high-throughput networks.
- Zeek is built upon a flexible scripting language that supports custom analysis and alerting based on simple signatures, event sequences, and patterns in traffic behavior.

- By default, Zeek logs are in Tab-Separated Values (**TSV**) format.
- However, Zeek also supports writing log output in JavaScript Object Notation (**JSON**) format.
  - The **TSV** format is ideal for **analyzing logs in the command line** with tools like **zeek-cut** or **Awk**, and the **JSON** format is ideal for **ingestion into analysis platforms** like (**SIEM**).
  - All logs contain a Unique Identifier (**UID**) field that aids analysts in correlating events between log sources. 

- The following is a list of common Zeek logs and the information they store:
  - conn.log: Contains records of both stateful and stateless connections (i.e., Transmission Control Protocol [TCP] and User Datagram Protocol [UDP]).
    - Stores data as sessions.
  - dns.log: Contains Domain Name System (DNS) queries and responses at the application level.
    - Cannot record encrypted DNS traffic.
  - http.log: Contains records of Hypertext Transfer Protocol (HTTP) sessions.
    - Does not have capabilities to record Hypertext Transfer Protocol Secure (HTTPS) sessions without a solution to downgrade HTTPS to HTTP.
  - files.log: Contains a record of all files observed in network traffic.
    - Zeek can be configured to extract and save observed files to disk.
  - ftp.log: Contains records of File Transfer Protocol (FTP) connections.
  - ssl.log: Contains records of encrypted traffic, including HTTPS.
    - Does not have the capability to recognize what protocol is encrypted using Transport Layer Security (TLS).
    - Stores information, including the TLS version.
  - x509.log: Contains records of encryption negotiation that can be correlated with records in ssl.log.
  - smtp.log: Contains records of Simple Mail Transfer Protocol (SMTP) traffic.
    - Unencrypted SMTP traffic (usually over port 25) contains some fields such as the Mail User Agent (MUA) not present in records of encrypted SMTP (usually over port 465 or 587).
  - ssh.log: Contains records of Secure Shell (SSH) traffic. Fields include information on negotiation algorithms and authorization attempts.
  - pe.log: Similar to files.log, identifies Microsoft binaries in the Portable Executable (PE) format.
  - dhcp.log: Contains records of Dynamic Host Configuration Protocol (DHCP).
    - Because the DHCP handshake process consists of two separate connections in dhcp.log, each entry has two UIDs.
  - ntp.log: Contains records of Network Time Protocol (NTP) communication.
  - irc.log: Contains records of Internet Relay Chat (IRC) connections and messages.
  - rdp.log: Contains records of Remote Desktop Protocol (RDP) connections.
  - traceroute.log: Detects and records traceroute attempts over multiple protocols.
  - tunnel.log: Detects and records encapsulated traffic using multiple techniques.
  - dpd.log: Dynamic Port Detection (DPD) analyzes and records the protocol used in a connection, regardless of port number.
  - known_*.log
    - known_certs.log: Contains information about SSL/TLS certificates on the network.
    - known_hosts.log: Records an Internet Protocol (IP) address and a timestamp when new systems connect to the network.
    - known_services.log: Records new services when they appear on the network.
  - software.log: Records information about applications and software when they appear on the network.
  - weird.log: Records connection information whenever Zeek encounters something unexpected, usually at the protocol level.
    - This log is useful for creating alerts to identify anomalous traffic.
  - notice.log: Usually contains records similar to weird.log. Can be customized to generate alerts.
  - capture_loss.log: Detects and records gaps in traffic, usually by analyzing TCP sequence numbers.
  - reporter.log: Records internal Zeek alerts.

#### Suricata
- **Suricata** is an open-source (**IDS**), (**IPS**), and **NSM** engine that allows researchers to define rules that help detect and identify Malicious Cyberspace Activity (**MCA**).
- Although Suricata contains network-monitoring capabilities similar to Zeek, Suricata works best with signature-based threat detection.
- Suricata rules are simple to write, and many repositories of Suricata rules are updated on a regular basis with Indicators of Compromise (IOC) for emerging threats.
- Suricata can also be used to generate network metadata that can be forwarded to a SIEM.


#### NetFlow
- NetFlow is a network protocol created by Cisco® that records IP traffic metadata from a router, switch, or host.
- NetFlow was designed to provide insights to network engineers, but it is also used by security analysts to access and correlate connections.
- Because NetFlow was designed with a focus on network engineering and not security, it can lack information that is collected in security-focused network metadata such as Zeek and Suricata.

### Packet Capture Tools
#### Stenographer
- Stenographer is a lightweight Packet Capture (PCAP) tool designed to capture and write packets to disk very quickly (up to 10 gigabits per second [Gbps]).
- Stenographer also manages disk usage and automatically rolls over the oldest packets when it runs out of storage.
- Because Stenographer was designed for speed, it lacks common PCAP functionality like a web interface, custom query language, and complex TCP stream reassembly.
- Stenographer indexes packets only up to the Transport layer of the Open Systems Interconnection (OSI) model, and queries are written in Berkeley Packet Filter (BPF) syntax.
- There are many web platforms, such as Docket, that can be used to deliver Stenographer more efficiently.
- Stenographer’s quick write speed also limits the ability to read packet data, which requires analysts to be specific in their queries.

- The Stenographer tool is composed of the following processes:
  - **Stenotype**: Reads packets off the wire and writes them to disk.
  - **Stenographer**: Manages Stenotype, monitors disk usage, deletes old packets, and responds to queries.
  - **Stenocurl**: Controls access to queries and performs authentication.
  - **Stenoread**: Passes queries to Stenocurl and passes answers using Tcpdump.
- Stenographer is ideal in busy environments due to its speed. However, its indexing and query limitations should be kept in mind when choosing to implement this tool.

#### Arkime
- Arkime, formerly Moloch, is a PCAP platform that prioritizes indexing and search functionality.
- Arkime uses Elasticsearch to index traffic through the Application layer of the OSI model.
- The web interface and Application Programming Interface (API) make the captured packets and protocol-specific analysis accessible to analysts.
- Because of Arkime’s enhanced indexing and analysis capabilities, it requires more resources than competitors like Stenographer.
- However, it can be scaled in clusters to capture any level of traffic.

### SIEM Collection and The Elastic Stack
#### SIEM Collection
- SIEMs collect and organize data in a central location.
- This allows analysts to create queries to identify and investigate events.
- Modern SIEMs use visualizations and dashboards that enhance investigative capabilities.
- Kibana and Splunk are two of the most common SIEMs in use today.

#### The Elastic Stack
- The Elastic Stack is an open-source platform often used as a SIEM.
- The Elastic Stack comprises Elasticsearch, Logstash, Kibana, and Beats, which work together to collect, index, search, and present data.
- Beats collect data, including logs, on different devices that are forwarded to Logstash to be indexed by Elasticsearch.
- Once the data has been indexed, Kibana is used to interact with the data by querying the Elasticsearch database.
- Each component is discussed in more detail in this lesson, but a general understanding of the flow of data is useful.
<img width="1999" height="883" alt="3c78016a-f012-43bd-83f6-4e976032487e" src="https://github.com/user-attachments/assets/7e608b00-1bad-4704-8141-250eb575f9fc" />

### Log Shipment
#### Osquery
- Osquery is a Structured Query Language (SQL) database that stores information about endpoints.
- Osquery stores information about running processes, network connections, browser plugins, and hardware.
- It uses an agent (Osqueryd) that regularly generates logs and reports changes on the endpoint.
- The logs collected can be forwarded to existing log aggregators, including The Elastic Stack.

#### Beats
- Beats are open-source data shippers used to collect and forward logs to Logstash or directly to Elasticsearch.
- Many types of Beats are available, based on the data to be collected, and are installed on the system.
- One way to visualize Beats is as police officers walking a beat.
- Just as police officers walk a beat and report what they see to their station, Beats sit on endpoints and report what they see to Logstash.

- Common types of Beats are as follows:
  - **Filebeat**: Collects logs from security devices, containers, and hosts using a wide variety of modules.
  - **Metricbeat**: Collects metrics from systems and services. Metrics can include information about CPU usage, memory, filesystem, and network statistics.
  - **Packetbeat**: Collects and analyzes packets from hosts.
  - **Winlogbeat**: Collects Windows event logs.
  - **Heartbeat**: Collects uptime and downtime data on services and devices.

#### Logstash
- Logstash is the data-processing pipeline of The Elastic Stack that ingests and parses different data sources and formats.
- Data is shipped from a log source to Logstash, where the data is stored temporarily and processed.
- Once the data has been processed, it is forwarded to the Elasticsearch database.

#### Kafka
- Apache Kafka is an event-streaming platform used to compile events and data from disparate sources to generate metrics.
- Because Kafka is not a SIEM, it is not usually used for network analysis.
- Instead, it is typically used as an intermediary between log collection and Logstash to standardize data formats.

### Log Storage: Elasticsearch
- Elasticsearch is the search engine and database of The Elastic Stack.
- The Elasticsearch database is stored as serialized JSON documents.
- Each document contains records and metadata for one object.
- Objects with similar properties are grouped in an index to organize the database.
- Elasticsearch uses an API that allows analysts to interact directly with the data using scripts or to deliver data to Kibana.

### Log Visualization
#### Kibana
- Kibana is the web interface for Elasticsearch.
- Kibana has strong visualization capabilities that can be combined into dashboards.
- Dashboards can be customized to display information relevant to analysts to quickly identify anomalies and investigate events.
- Kibana can also be configured to create custom alerts.

#### Splunk
- Splunk is a SIEM designed for analyzing and visualizing log data in real time.
- Splunk offers a powerful search engine that gives analysts the tools to quickly develop visualizations and alerts.
- The Search Processing Language (SPL), Splunk’s query language, uses pipes (|) in a way similar to Bash or PowerShell that allows analysts to quickly and efficiently manipulate data.
- The downside of using Splunk over other SIEMs is that Splunk charges a fee based on the amount of data it processes and can quickly become quite expensive. 

### Sensor Platforms
- Although any OS can be used as a sensor platform, most deployments use Linux server OSs due to their lightweight and easily customizable deployment.
- These sensors can be loaded with any of the capabilities discussed previously to meet mission requirements.
- Some companies have developed ready-to-deploy sensor platforms that can be used to provide a prebuilt solution for most requirements.
- The most common of these platforms are discussed below.

#### Security Onion
- Security Onion is a prebuilt sensor platform that can be configured as a (NIDS), (HIDS), Security Operations Center (SOC) workstation, or static analysis workstation.
- Some of the default tools used by Security Onion are as follows:
  - Elastic Stack
  - Suricata
  - Zeek
  - Stenographer

#### ROCK NSM
- The Response Operation Collection Kit (ROCK) Network Security Monitoring (NSM) platform is an open-source sensor platform.
- The default tools used by Rock NSM are as follows:
  - Stenographer
  - Zeek
  - Suricata
  - Kafka
  - The Elastic Stack
- The ROCK NSM server collects data using Stenographer, Zeek, and Suricata.
- Docket provides a web interface to query Stenographer PCAP.
- Suricata and Zeek logs are compiled using Kafka and forwarded to The Elastic Stack for analysis capabilities.
<img width="1999" height="1101" alt="50ba0fb3-4bf1-4f71-bb0e-0a706f776875" src="https://github.com/user-attachments/assets/4e47674c-3c27-42f1-ab7c-aef199b93b06" />

### Sensor Deployment
#### Standalone Sensors
- Standalone sensors are a sensor deployment scheme that uses one sensor to collect and present data to analysts.
- Under this deployment scheme, one sensor collects network traffic logs and runs all the tools.
- Analysts can use the sensor to directly access tools, or they can use an analysis workstation to interact with the sensor.
- Due to the lack of scalability of this deployment, standalone sensors are not recommended in most situations and should be used primarily for small networks and training environments.

#### Distributed Deployments
- Distributed deployments are much more common in production environments and involve using multiple nodes to collect and manage data.
- Choosing a sensor platform is not necessary for a distributed deployment, but it greatly reduces configuration time and makes the deployment much more scalable.
- The following provides descriptions of Security Onion and ROCK NSM distributed deployments.

#### Security Onion
- The Security Onion distributed deployment requires at least two nodes and an analysis workstation, as shown in Figure 13.2-3.
- The first node is called a Forward Node that acts as a sensor and location for logs to be forwarded.
- These forward nodes run tools like Zeek and Suricata to collect the data and forward to the Manager Node.
- The other node is deployed as a Manager Search Node, which receives and stores logs as well as manages search functionality.
<img width="1936" height="1999" alt="58c751cd-1628-4777-839a-76714118d6e3" src="https://github.com/user-attachments/assets/4d83c2e9-d019-4a13-b204-26947665ee3a" />

- If possible, a third node should be deployed as a Search Node that takes over responsibility for managing the database and returning search results from the Manager Node.
- Using the three-node setup, as illustrated in Figure 13.2-4, enables the deployment to be scalable to meet mission requirements and is the ideal deployment option.

<img width="1656" height="1999" alt="4719cc9f-c10e-4bde-8a31-46de68987b4b" src="https://github.com/user-attachments/assets/646cbb4c-f4f7-4f29-bb1b-412d33eccb1c" />

#### ROCK NSM
- In a ROCK NSM distributed deployment, one sensor is designated as the Deployment Manager (DM).
- Unlike Security Onion, distributed ROCK NSM deployments are typically split into several sensor groups where each node can fall into multiple groups.
- A typical deployment is quite similar to a Security Onion deployment, with the exception being that the Security Onion Manager node also serves as the DM.
- These sensor groups are as follows:
  - **ROCK**: Sensor services, including Zeek, Suricata, and Stenographer.
  - **Web**: Web services, including Kibana and Docket.
  - **Zookeeper**: Kafka cluster manager.
  - **Elasticsearch**
    - es_masters: Elasticsearch master nodes
    - es_data: Elasticsearch data nodes
    - es_ingest: Elasticsearch ingest nodes

### Sensor Placement and Selecting Capabilities
- To determine sensor configuration, three things — **mission requirements**, **capabilities**, and **traffic flow** — should be kept in mind.
- By considering each of these requirements, analysts can identify optimal sensor placement.

#### Mission Requirements
- Understanding the requirements of a mission is the most important factor of sensor placement.
- This includes identification of Key Terrain in Cyberspace (KT-C) and a security gap analysis.
- Identified security gaps can be minimized by installing a sensor to increase visibility and alert on anomalous activity.
- The data that must be collected to meet mission requirements should be identified and used to determine the other two components of sensor placement. 

#### Capabilities
- The second factor to consider in sensor placement is the capabilities of sensors.
- Such capabilities include hardware and software capabilities as well as the limitations of tools.
- Tools should be matched with all previously identified data to be collected to meet mission requirements.
- If gaps in required data and tool capabilities are identified, analysts may look for additional tools and follow the existing process to modify Joint Deployable Mission Support System (JDMSS) capabilities.
- Generic sensors usually have a solution for network metadata collection, PCAP, log forwarding, and some sort of SIEM.

#### Traffic Flow
- With an understanding of the mission requirements and capabilities, analysts can consider the traffic flow to select optimal sensor placement.
- The primary traffic flow concern is that the data that a sensor is intended to detect actually collects that information.
- For example, if the analyst places a sensor on an interface going to the external network, they miss all internally routed traffic, which could be mission essential.
- On the other hand, sensor placement can lead to collection of irrelevant data that can fill sensor storage and cause important data to be rolled over.
- An example of this is placing a sensor before a firewall connecting to an external network.
- In this scenario, the sensor collects traffic that never reaches the network because the firewall blocks it.
- In most situations, this information is irrelevant, and blocked traffic is ignored in analysis.

- Keeping all these factors in mind helps an analyst optimally place network sensors.
- The following scenario is an example of good sensor placement: If a mission partner has a mission-critical web server in their Demilitarized Zone (DMZ) that has a Web Application Firewall (WAF) installed, analysts can identify the web server as KT-C and monitor its activity.
- When selecting capabilities, analysts choose to collect traffic metadata using Zeek and Suricata, forward WAF logs using Filebeats, and consolidate the data using Elasticsearch.
- For the sensor placement, the analysts could choose to mirror the port going to the web server on the switch, or they could choose to install a network tap.
- Using a tap is generally preferred to the port mirroring option because, although both meet the mission requirements, monitoring the entire network segment can provide more context and lead to better visibility of the network.

### Sensor Impact
- A common misconception is that more sensors are always better than fewer sensors.
- Although more sensors generally provide more information to analysts, increasing the number of sensors can also impact the availability of the network.
- This is because modern networking equipment is designed to prioritize speed, whereas security equipment attempts to analyze and index traffic often in a resource-intensive manner.
- Some methods of installing sensors can also cause network outages.
- The following commonly impact network performance and mitigation opportunities.

#### Port Mirroring
- Port mirroring is a technique that uses a Switched Port Analyzer (SPAN) port on switches to mirror traffic to a sensor.
- The amount of traffic that can be mirrored is limited to the physical capabilities of the interface.
- If a SPAN port is configured to mirror both transmitted and received data on one interface, it can mirror up to 50% of the traffic, and the amount of data mirrored decreases as more interfaces are mirrored.
- For these reasons, port mirroring is not usually recommended except on small networks.
- The action of mirroring can also use resources that can slow down switch hardware performance, negatively impacting network traffic.
- The benefit of using a SPAN port is that it can be installed and configured without network downtime.

#### Network Tap
- A network tap is a device that receives traffic and duplicates it, allowing the traffic to pass on two lines.
- The first line continues to where the traffic was intended to be delivered, and the duplicated traffic can be sent to a sensor for monitoring.
- This is an incredibly efficient way to monitor traffic and has no impact after installation.
- However, the installation does require downtime to unplug the existing network cables and plug them into a tap.

#### Bandwidth
If the analysis network and sensors are configured to use existing network infrastructure to communicate, the data can generate excess bandwidth that the network was not designed to handle. 
This can be caused by log forwarding or by analyst queries. 
To minimize the impact of this, analysts can configure Beats to forward logs only during off-hours.

#### NetFlow
- Because NetFlow is collected by forwarding data from networking devices, it uses network resources that could be used to forward packets (if an out-of-band management network is not implemented).
- As a solution to this problem, many networking devices sample the flow and forward only a percentage of the NetFlow.
- Sampling the flow does not record all network connections, which greatly decreases NetFlow’s usefulness for security.

### Traffic Shaping
- Network and host configurations can have an impact on how traffic moves through the network and can provide capabilities to focus network monitoring on certain network segments by creating network choke points.
- This can be done by redesigning the network, installing new devices, or by changing existing configurations.
- If the network is redesigned, analysts can place sensitive devices on network segments that route traffic through a choke point that can be easily monitored.
- To shape traffic by adding devices, analysts can add web proxies or internal DNS servers.
- This creates a deterministic flow of DNS and web traffic through the network and consolidates important data sources in one location.
- Finally, device configurations can be changed to create Virtual Local Area Networks (VLAN) to segment certain devices or route traffic.

- An example of traffic shaping is installation of internal DNS servers and a requirement for all devices to make DNS requests through that server.
- Routing the traffic through an internal DNS server has two advantages.
  - The first advantage is that all DNS logs can be compiled in one location.
  - Another advantage is all traffic not routed to the internal DNS server can be identified as anomalous and firewalls can be configured to block all outgoing DNS queries not originating from the internal DNS server.

<img width="749" height="338" alt="image" src="https://github.com/user-attachments/assets/6f639414-eb39-4b60-90e0-b81e5f2e52de" />

## Sensor Optimization
### Sensor Deficiencies
- Network sensors provide a powerful tool by collecting and indexing network traffic in a way useful to analysts, but limitations exist, including storage and computational requirements.
- Because of these resource limitations, it is necessary to make decisions about sensor location and tool choice, and this can lead to blind spots.
- Generally speaking, the more data a tool generates, the more useful it is to an investigation — but it is also available for less time.
- Although the average time between intrusion and detection is trending down, an average of 12 days is still required to identify a breach in the United States.
- Because data must be stored until a breach is identified for it to be useful, it is important for analysts to take into account the amount of data generated and storage capabilities.

- Full packet capture can offer powerful insights into adversary activity. However, several downsides to unconditional capture exist.
- Modern networks generate a massive amount of traffic. If a Stenographer node is operating at its limit, approximately 10 gigabits per second (Gbps) — more than a petabyte (PB) of data a day — can be collected.
- To offset the high storage requirements of full packet capture, sensors can be configured to extract files from protocols like  (HTTP), (SMTP), (FTP), and (SMB).
- Because these files take far less space than Packet Capture (PCAP), they can be stored over a longer period of time and can provide evidence of the initial attack vector.

- To augment packet capture, many sensors collect network metadata using Zeek, Suricata, or NetFlow.
- Although metadata records are much smaller than PCAP, they still have storage requirements that must be kept in mind to meet mission requirements.
- Of the solutions Zeek, Suricata, and NetFlow, Zeek requires the greatest amount of storage due to the multiple logs it generates.
- Of course, this means that Zeek logs offer far more insight into the traffic and are generally more useful to analysts.
- The format that requires the smallest amount of data is NetFlow, which records only eight fields (Timestamp, Duration, Protocol, Source Socket, Destination Socket, Packets, Bytes, and Flows) by default. 

- Another resource used is the computational overhead associated with sensor functionality.
- Some tools, such as Stenographer, copy data directly to the disk and perform little to no enrichment or indexing of the data.
- Metadata extraction tools, such as Zeek, analyze the data as it reaches the sensor and can be quite Central Processing Unit (CPU) intensive.
- Intrusion Detection Systems (IDS), such as Suricata, also perform tasks that are potentially CPU intensive to examine and alert on traffic.
- On sensors with multiple CPU sockets, analysts can dedicate a processor or CPU thread for a tool or service using process pinning.
- When a process is pinned, it is prioritized by the scheduler for the CPU or thread, which can minimize load times and increase performance by keeping process relevant data in the CPU cache.

### Sensors and Encrypted Traffic
- The number of websites using (HTTPS) has increased from 40% to more than 80% since 2016, and analysts face new challenges when analyzing web traffic or other protocols that use encryption, such as (SSH).
- Although it is possible to decrypt encrypted network traffic, most organizations do not collect session keys, which are required for (SSL) and (TLS) decryption.
- As a result, packet capture of encrypted protocols offers little to no benefit to analysts while writing a large amount of traffic to disk.

- To minimize strain on the disk, it can be advantageous to exclude ports 443 and 22 from full packet capture.
- This can preserve storage that would otherwise be taken up with encrypted PCAP not beneficial to analysts.
- If this is done in conjunction with a Network Security Monitor (NSM) such as Zeek or Suricata, much of the useful information, including records of the network connection, is recorded but the encrypted data is not.
- The downside of using this technique is that no data sent over the excluded ports is collected, regardless of the protocol used. Adversaries can disguise malicious traffic by mismatching ports and protocols to take advantage of this gap in packet capture.

### IDS Evasion Techniques
- Despite the ability of network sensors to detect Malicious Cyberspace Activity (MCA), many techniques to evade detection exist.
- Adversaries can use these evasion techniques to disguise Indicators of Compromise (IOC) and infect a network without detection.
- The following are some common IDS evasion techniques.

#### Fragmentation
- Fragmentation is an IDS evasion technique that involves splitting malicious packets into several smaller packets.
- This makes it necessary for the IDS to perform the CPU-intensive task of reassembling packet streams to detect malicious activity.
- Derivatives of this technique involve inserting another packet in the same stream between the fragmented packets or sending the packets out of order. 

#### Encoding
- Encoding is an IDS evasion technique that uses an encoding algorithm such as Uniform Resource Locator (URL), Base64, or Exclusive Or (XOR).
- If the IDS uses signature-based detection, encoding strings identified as IOCs can be used by the detection engine to sneak.

#### Low-Bandwidth Attacks
- Low-bandwidth attacks involve an adversary spreading an attack over a long period of time or distributing the attack over multiple sources.
- An example of this is a password cracker that tests one password a day, appearing as though a user mistyped their password.
- These attacks are difficult for a normal IDS to distinguish from normal network traffic and often require custom scripts or analysis to identify.

#### CPU Exhaustion
- CPU exhaustion is a Denial of Service (DoS) technique that targets an IDS.
- Because IDS and sensors “fail open,” attackers can flood the network to overwhelm the CPU.
- Fail open is a security concept that means that if a security control fails, it does so in a way that allows all traffic through a device.
- Adversaries can take advantage of the fact that some IDS signatures are computationally expensive and send packets specifically designed to maximize CPU usage.
- Once the CPU buffer is full, the sensor cannot handle more traffic, and the adversary can conduct malicious activity without identification by the IDS.

<img width="679" height="342" alt="image" src="https://github.com/user-attachments/assets/6b07bbfa-5f69-4048-8b45-7be8d7fe78f8" />

### Assess Network
- The first step analysts should take when analyzing the network is identifying critical infrastructure.
- This is usually done by working directly with the mission partner to understand the minimum infrastructure required to meet mission requirements.
- Because the mission partner is not available in this scenario, trainees must infer Key Terrain Cyberspace (KT-C).
- The most likely candidates for KT-C are servers in the OT-Services and Engineering-Department subnets.
- Other candidates for KT-C are the web server and Domain Controllers (DC).

- The next phase of analysis is identification of services running on devices and is confirmed through discussion with the mission partner.
- This is important because it can give analysts insight into traffic flow and can help identify normal traffic patterns
- Some devices are easy to identify, such as web servers that can be expected to have HTTP running on port 80 or HTTPS running on port 443.
- The following table represents expected ports and services on different Operating Systems (OS).
<img width="1381" height="1999" alt="deb6a841-5076-492b-9b6f-471182b33c25" src="https://github.com/user-attachments/assets/2d09e548-b456-4a07-bd4c-49dc6cab81df" />

- Identification of these ports and protocols can help analysts understand what traffic can be expected to the devices.
- This is useful because it can reveal information about the amount of traffic flowing through the network, which can impact sensor optimization.
- For example, if a sensor is installed on a Switch Port Analyzer (SPAN) port of the Services switch identifying that the switch regularly processes up to 3 Gbps but the SPAN port is a 1-Gbps port, that sensor will capture only up to a third of the traffic on the switch. 

- This knowledge can also be leveraged to identify requirements for storage capacity of sensors and analysis infrastructure.
- To calculate the required storage on any sensor, the following formula can be applied: 
<img width="512" height="114" alt="da579748-5fdf-44fd-bd0f-2b4e8433bd6f" src="https://github.com/user-attachments/assets/faad0122-f7a4-411c-b23e-73bcad108612" />

- Average throughput is in bits per second (bps), which the network engineer can help identify.
- Dividing the bits per second by 8 provides the average throughput in bytes per second (Bps), which can be multiplied by 3,600 (60 seconds × 60 minutes) to provide the average amount of data generated in an hour.
- This can then be multiplied by 24 to give the expected traffic in bytes per day.
- Once this information is known, analysts can work with the mission partner to identify retention requirements and determine storage requirements for sensors.

- As an example of identifying the requirements for storage capacity, consider an average throughput of 80,000 bps.
- The requirements in bytes per day is, then, as follows:
  - **(80,000 bits per second ÷ 8) × (60 sec per min × 60 min per hr) × 24 hr =**
    - **10,000 bytes per sec × 3,600 sec per hr × 24 hr per day =**
      - **864,000,000 bytes per day**

- One megabyte (MB) is 1,000,000 bytes, so the requirements in MB is **864,000,000 ÷ 1,000,000 = 864 MB per day.**

- NOTE: To **convert from bps to megabits** per second (Mbps), **divide the throughput by 1,000,000**.
  - To **convert from Mbps to Gbps**, **divide the throughput in Mbps by 1,000**.

<img width="781" height="370" alt="image" src="https://github.com/user-attachments/assets/7bdf17ef-7b29-470a-bbe8-f6d48a509a61" />
  - 250000/8*3600*1=112500000


### Optimal Sensor Location
- Once a mission partner network has been assessed, sensor location can be optimized.
- Doing this can limit the amount of duplicated traffic and optimize collection for the network.
- To optimize the location of sensors, it is necessary to understand the flow of traffic, discussed earlier in this lesson.
- Once the flow is identified, analysts can begin to identify places where collection is redundant and modify collection techniques to minimize the redundant traffic.
- Although minimizing redundant traffic is ideal, resources should be prioritized to maximize visibility of critical infrastructure over efficiency of collection.
- The examples below use the same scenario and network map discussed earlier in the lesson.
- One example of the redundant traffic collection is the Onion-fn1 sensors in this network.
- The Onion-fn1 sensors collect all web traffic from the user subnets, but the traffic is also collected at the Onion-fn2 sensor connected to the core-router.
- After identifying this duplicate data, analysts may choose to filter out the traffic collected on the sensor using Berkeley Packet Filter (BPF) on either set of sensors.

- It is also important for analysts to consider firewall location in relation to sensor placement when considering what traffic will be collected.
- For example, the sensor on the OT-Services subnet would capture all attempts for endpoints on that subnet to reach out to the internet.
- However, the firewall may be configured to block those attempts.
- Capturing that traffic on the sensor provides greater visibility of attempted outbound traffic, but the captured traffic may be blocked.
- Collecting traffic on only one of the sensors can create blind spots in the traffic collection, as the sensor for the OT-Services does not see blocked incoming traffic, and the sensor on the core-router does not see blocked outbound traffic.
- To minimize this impact, it is recommended to capture network metadata on both sensors and compare traffic to network device logs (e.g., syslog from firewalls) in any investigation instead of collecting full packet capture on both sensors.
- Because OT-Services is KT-C, in this situation it is recommended to prioritize available resources to collect full packet capture on that segment. 

- NetFlow reporting can be used to ensure visibility of network connections, regardless of sensor placement.
- In addition to the performance penalties on networking equipment, collecting NetFlow on all networking devices also generates a new flow for every device a connection crosses.
- Although this is useful for network engineers, the duplicate data is redundant from a security perspective, and it is recommended to use NetFlow de-duplication techniques.

### Sensor Tuning
- Once sensors have been placed and configured, analysts can generate a baseline to identify what traffic is normal.
- During the baseline generation, analysts compare actual traffic with expected traffic identified when choosing sensor placement.
- Analysts also work with mission partners to identify anomalous traffic that may require further investigation.
- If it is identified that expected traffic places too much stress on a sensor, the sensor can be tuned using BPF to not capture that traffic.

- If traffic has been identified as anomalous, analysts can implement rulesets to alert on or block the traffic.
- This is done using such tools as Snort® or Suricata.
- Both of these tools use an easy-to-understand rule language that makes it easy for analysts to write and publish rulesets to sensors.
- Because Snort and Suricata can inspect traffic up to the Application layer of the Open Systems Interconnection (OSI) model, implementing these rules can impact sensor performance.

# MOD 14
## Filebeat
### Filebeat Overview and Use Cases
#### Logstash
- Data within a network is generated from multiple sources and contains a wide range of information about systems, connections, traffic, and other network elements.
- This data is often unstructured, being in its own format.
- For example, Apache web server logs are entirely different from NetFlow data generated from a router, and both are unrelated to Twitter Application Programming Interface (API) data.
- Logging and centralizing these examples would be a difficult and time-consuming task. Use of Elastic Logstash can address these needs. 

- Logstash is a data pipeline that operates in three steps: 
1. **Ingest**: Logstash supports a large variety of plug-ins that allow it to receive and read data from various sources. This stage is when data is sent from such sources as logs, applications, and Amazon Web Services (AWS) and then ingested into Logstash. 
2. **Transform**: When the ingested data is received, it is processed by Logstash. This step involves normalizing, structuring, and transforming the data so that it fits into a common format. Additionally, filters may be applied that, for example, anonymize sensitive information or determine location based on IP addresses. 
3. **Ship**: The final step in Logstash’s pipeline is to send the data to an output location. The most common example is sending the data to Elasticsearch, but data may also be sent to AWS, MongoDB, Redis, Slack, and other locations.  

- **Beats** are **data shippers** with **low overhead and processing demands**; they reside on **servers**, **containers**, or other systems that **generate data**.
- Beats are **single-purpose programs** designed to handle a **specific type of data** from a source. 

#### Filebeat Overview 
- Filebeat is designed to ship log files from its host.
- Filebeat has a low memory footprint, can handle large volumes of logs, and can intelligently communicate with Logstash to throttle its output as needed. 
- Filebeat has two components that allow it to perform its tasks:
  - **Harvesters** open and read files. Once each line of the file is read, the contents are sent as output to a destination such as Logstash or Elasticsearch.
  - **Inputs** act as supervisors to the harvesters, telling them where to find files to read.
    - Inputs are specified within a Filebeat configuration file.
    - Essentially, inputs tell harvesters, “Look here, and when you find a certain file, read it and forward it to the destination.” 

<img width="1999" height="963" alt="d1bfff7e-0f78-43b1-9155-f2a77c24751b" src="https://github.com/user-attachments/assets/31370bcf-5dea-406a-a2bd-408f0715ad82" />

#### Filebeat Use Cases 
- Filebeat is quite flexible in its capabilities.
- Nearly any file on its host can be read by harvesters and sent to Logstash for further processing and filtering or sent directly to Elasticsearch for indexing. 
- The use of Filebeat within the scenario demonstrates the advantages of the tool.
- By implementing Beats as a lightweight shipper of data, the collection process is streamlined.  

<img width="638" height="344" alt="image" src="https://github.com/user-attachments/assets/0a705ffc-6d87-4527-a92f-637044d8dcaa" />

<img width="579" height="343" alt="image" src="https://github.com/user-attachments/assets/9b2cb933-7e65-4c1e-9edb-e9a723fc7f5d" />

## Heartbeat
### Heartbeat Overview and Use Cases
#### Overview
- Heartbeat is a member of the Elastic Beats family.
- Similar to Filebeat, which was introduced in the last lesson, Heartbeat is a lightweight shipper that sends information from a host to a destination.
- Unlike Filebeat, which is geared toward handling log files, Heartbeat monitors various services to ensure that they are “alive” and reachable.
- Additionally, Heartbeat can generate response times from its feedback. 

#### Heartbeat Monitors 
- To determine if a service or system is “alive,” Heartbeat defines monitors.
- Much like medical heart rate monitors, Elastic Heartbeat monitors send “pulses” to indicate the state of the target.
- Heartbeat monitors come in three varieties:
  - (**ICMP**): This is the “lowest”-level protocol (Open Systems Interconnection [OSI] model Layer 3) used in a monitor. By using ICMP, commonly known as pings, the monitor can determine if a system is powered and reachable on the network. However, this is mostly the extent of its capabilities, as this monitor does not determine the state of any services.
  - (**TCP**): A TCP monitor runs at a higher-level protocol (OSI Layer 5) than the ICMP monitor. This checks a specific port on the host to determine whether the service is active. For example, the monitor checks if port 1433 is reachable. A successful TCP connection determines that both the system and its service are live.
  - (**HTTP**): The HTTP monitor operates at OSI Layer 7 and validates the connectivity of its host’s web server. This monitor can also be configured to verify certain response codes (for example, 200, 201, or 301), headers, or even content of a page. 
Heartbeat Use Cases 

- Heartbeat is useful in a variety of situations in which an analyst needs to know the status of a system without manually checking it.
- Consider the following scenarios in which Heartbeat is used.

- An organization runs several web servers that host sites for online shopping. To minimize loss of sales in the event of an outage, the network administrator installs Heartbeat on a system within the network. Heartbeat has several HTTP and TCP monitors configured that ensure that the proper ports are open and the webpages are responsive. By checking the uptime statistics within Kibana, the administrator can react more quickly to issues, ensuring that the web store is available for use. 

- Another organization operates a factory with hundreds of robotic assembly arms connected to their Industrial Control System (ICS) network. The uptime and availability of the arms are extremely critical, as the assembly process is halted if they cease functioning. To monitor the availability of the assembly arms, Heartbeat is implemented. The network administrator configures ICMP monitors, which require less processing power from the robot arms than if using TCP monitors. 

<img width="603" height="305" alt="image" src="https://github.com/user-attachments/assets/0931108f-63aa-4caf-87b6-27b2b297fc11" />

<img width="556" height="293" alt="image" src="https://github.com/user-attachments/assets/2d9d7950-c5f4-4252-bf42-8a1bee63bbaf" />


## PacketBeat
### Overview and Use Cases
#### Overview
- Packetbeat is a member of the Elastic Beats family.
- The tool is a lightweight shipper that captures and analyzes network traffic.
- Unlike Filebeat and Heartbeat, which focus on hosts, Packetbeat provides visibility to the communications between systems.
- By parsing and analyzing network traffic in real time, Packetbeat allows analysts to examine trends and anomalies within Elasticsearch. 

##### How Packetbeat Works
- Packetbeat is installed and runs on servers in the network.
- These servers may be the same ones that run applications, such as a web server, or they may be dedicated Packetbeat servers.
- When running on a dedicated server, Packetbeat receives its traffic from a switch’s mirror interface or from a network tap.
- An important advantage of Packetbeat is that it does not interfere with the monitored application, such as causing latency or modifying the traffic data. 

- Once installed and running, Packetbeat captures network traffic and then parses protocols.
- Packetbeat version 8.1 and later support the following protocols:
  - Internet Control Message Protocol (ICMP) (v4 and v6)
  - Dynamic Host Configuration Protocol (DHCP) (v4)
  - Domain Name System (DNS)
  - Hypertext Transfer Protocol (HTTP)
  - Advanced Message Queuing Protocol (AMQP) 0.9.1
  - Cassandra
  - MySQL
  - PostgreSQL
  - Redis
  - Thrift–Remote Procedure Call (RPC)
  - MongoDB
  - Memcache
  - Network File System (NFS)
  - Transport Layer Security (TLS)
  - Session Initiation Protocol/Session Description Protocol (SIP/SDP) (beta)
- mOnce the protocols are parsed, Packetbeat correlates the messages into transactions.
- These correlated transactions are sent to Elasticsearch as JavaScript Object Notation (JSON).
- The JSON data contains various exported fields, which are populated with the information contained in the traffic.
- Finally, Elasticsearch indexes these fields to be viewed by the user. 

<img width="575" height="302" alt="image" src="https://github.com/user-attachments/assets/ce9d8f0a-e853-412c-8d94-fe6dcea7275b" />

#### Packetbeat Use Cases 
- Packetbeat is useful in various situations in which an analyst needs to examine network traffic.
- Consider the following scenario in which Packetbeat is used: An organization’s network operates an e-commerce site.
- The website runs on an Apache web server, and a separate MySQL server contains the databases to track inventory.
- To track network traffic, an analyst installs and configures Packetbeat on a dedicated server, which receives mirrored traffic from a switch connecting to the web server and MySQL server.
- The data shipped by Packetbeat is examined in Kibana by the analyst. 

<img width="598" height="277" alt="image" src="https://github.com/user-attachments/assets/2017006c-f2e4-4c26-a73a-0e3a742b3e10" />


# MOD 15
## Logstash
### Logstash Overview
- Logstash is an open-source data collection engine used to collect, process, and forward events and log messages.
- Logstash is capable of taking data from different sources and normalizing the data into a parsable format to be sent to a destination.
- Although it can be used with any log collection tools, Logstash is primarily intended for The Elastic Stack due to its tight integration and powerful log-processing capabilities.

- The main services that Logstash provides are the following:
  - **Log collection**: Collects logs from different sources and gathers the collected data.
  - **Log processing**: Applies transformations to the collected data, typically to normalize data across multiple datasets or parse incoming data and break it down for searching or indexing.
  - **Log forwarding**: Sends the processed data to a different application for further processing or storing.
- A recurring theme for Cyber Defense Analyst (CDA) efficiency is the ability to filter out information that does not need to be seen.
- Logstash is helpful in that regard because it has the capability to enrich and transform data by the use of plug-ins.
- Filters are prebuilt into Logstash, allowing CDAs to readily transform data types, index them in Elasticsearch, and begin querying without the need to build custom filters.
- More than 200 plug-ins are readily available for use with Logstash. If a required plug-in does not exist, it can easily be created. 

<img width="704" height="268" alt="image" src="https://github.com/user-attachments/assets/5039b3ae-0bd3-4bdc-a7a0-3abfbc86dbd7" />

### Logstash Pipelines
- Logstash pipelines are how data is handled from inception to delivery within a Logstash solution.
- Logstash pipelines include **ingestion or input of data**, **transformation or filtering of data**, and **delivery or output of data**.
- A Logstash instance consists of many different pipelines, installed at various hosts or sensors throughout a network environment.

- The three stages of the Logstash pipeline, from beginning to end, are as follows:
1. Input: Logstash inputs are defined by how data is consumed from the source. Inputs are required to establish a Logstash pipeline. 
2. Filter: Logstash filters are optional. Filters are used to transform the data as specified by specific rules. Filters can help to normalize and organize data so that data is easier to search and data from various sources appears in a similar form.
3. Output: Logstash outputs define how data is sent to the destination. Outputs are also required to establish a Logstash pipeline.

- A Logstash pipeline can be configured in Windows and Linux using the command line.
- A Logstash configuration file needs to be created, and Logstash reads from that file to determine the input, filter, and output.
- The following is an example of the contents of a Logstash configuration file and the changes that can be made to it.

<img width="382" height="206" alt="image" src="https://github.com/user-attachments/assets/c6a5a643-2795-4b50-a079-a15477e97e71" />
- This configuration listens for syslog messages over port 514 and uses log forwarding to store events in Elasticsearch.
- The 'udp' and 'elasticsearch' entries are input and output plug-ins

<img width="683" height="309" alt="image" src="https://github.com/user-attachments/assets/de7bf575-2132-46ed-a12a-5b1500535e09" />
- This section applies filters such as 'grok'
- Grok is a filter plug-in that parses unstructured data into something structured and queryable.
- With a filter section added, the event output has extra fields added to it: client, method, request, bytes, and duration.
- For the input and output, the file plug-in streams events from the file and writes events to files on disk, respectively.
- Once created, a Logstash configuration file can easily be used in the command line with the following entries:
  - Linux: `bin/logstash -f [filename.conf]`
  - Windows: `bin/logstash.bat -f [filename.conf]`

<img width="613" height="482" alt="image" src="https://github.com/user-attachments/assets/87eb76a0-6d6f-4dcc-9975-7937db1c9570" />

<img width="764" height="403" alt="image" src="https://github.com/user-attachments/assets/efb566b8-a178-46ca-a3c8-ea25d335f3ad" />


## Syslog
### Syslog Overview
- Linux has many system-level logging frameworks.
- Syslog (also known as syslogd) and related projects like Syslog Next Generation (syslog-ng) and rocket-fast system for log processing (rsyslog) are commonly configured for use on Linux systems.
- Both the syslog-ng and rsyslog projects were based on syslog and were designed to meet needs that syslog did not.
- Syslog, syslog-ng, and rsyslog all **contain log forwarding capabilities**, allowing machines using these technologies to **forward log entries to a centralized server for collection**.

- Centralized log collection simplifies the process of obtaining and analyzing logs.
- Support for ingesting syslog messages is included in many Security Information and Event Management (**SIEM**) systems and other analysis platforms.

#### Syslog
- Syslog is included by default in many Linux distributions and is relatively simple to configure and maintain.
- Syslog can be configured as a server to **receive log entries from remote machines** and **receives only log entries using User Datagram Protocol (UDP) packets**.
- Because UDP is unreliable, this can lead to dropped log entries if packets are lost due to intermittent network issues.

#### Syslog-ng
- The syslog-ng project was begun to address the shortcomings of syslog and to add enhancements to the platform.
- For instance, syslog-ng supports Transmission Control Protocol (TCP) for its remote logging capabilities (although it can both receive and send logs using UDP for backward compatibility).
- The use of TCP for syslog-ng clients and servers allows for reliable delivery due to the nature of TCP.
- In addition, syslog-ng supports rich filtering and also supports more inputs and outputs than syslog.

- Under the current management of the project, syslog-ng is partially open source and partially proprietary.
- This dual nature allows its maintainers to sell proprietary plug-ins that enhance the platform above what is included in the open-source version.

#### Rsyslog
- Similar to syslog-ng, rsyslog was created to **address shortcomings** and **add enhancements** to syslog.
- In addition to supporting TCP (and UDP backward compatibility), rsyslog supports a wide **variety of inputs and outputs** for processing.
- Data can be filtered and transformed by various plug-ins to allow for highly customizable and rich configurations.
- Rsyslog supports a wide variety of outputs for various platforms (some of which are enabled via plug-ins), such as outputting data to log files, open console sessions, database outputs (such as MySQL or PostgreSQL), and Elasticsearch.

<img width="701" height="331" alt="image" src="https://github.com/user-attachments/assets/9748e4b9-5955-483f-98f8-88b501ebb5ae" />

<img width="680" height="406" alt="image" src="https://github.com/user-attachments/assets/2b180cd4-a1c7-4fe1-bb65-12066dd1975e" />

### Rsyslog Configuration
- The syntax used in rsyslog’s configuration files is complex.
- The configuration file for rsyslog is stored at **/etc/rsyslog.conf** by default.
- Comments can be specified by a hash character (#). Everything past this character for the current line is treated as a comment.

- Rsyslog supports several syntax styles:
  - **sysklogd**: Classic syslog configuration entries, provided for backward compatibility.
  - **legacy rsyslog**: The older format used by rsyslog, now considered outdated and superseded by RainerScript syntax when available. Legacy lines are prefixed with a $.
  - **RainerScript**: The modern syntax, considered by the maintainers of rsyslog to be more concise.

- Each line in RainerScript syntax corresponds to one of three categories:
  - **Rule**: A rule consists of an input, a filter, and an output.
  - **Data manipulation**: This comprises variables that can be used in various ways. These are not covered in depth in this lesson.
  - **Action**: These lines are used to load and configure plug-ins or execute plug-in actions (such as writing an output).
    - For example, an action can load, or modify the configuration of, a plug-in.
    - In addition, some actions can be used as the output of a rule.
    - An action is a keyword followed by parameters in parentheses, such as module(load="imudp"). Actions can span multiple lines, if needed.

- RainerScript rule syntax is illustrated in the following example:
  - `user.*     -/var/log/user.log`

<img width="730" height="327" alt="image" src="https://github.com/user-attachments/assets/f02ea912-23b4-408b-9d45-a81651653dda" />

<img width="768" height="521" alt="image" src="https://github.com/user-attachments/assets/0b130ea8-8670-420a-9fd7-b8baf6dbb076" />


## Splunk Forwarders
### Splunk Forwarders
- Splunk forwarders are the data aggregation tool used by Splunk systems, specifically Splunk Enterprise.
- Splunk forwarders consume data and send it to an indexer, which can be standalone or built into a Splunk Enterprise instance.
- With broad platform support, Splunk forwarders operate on many different platforms and architectures.

#### Splunk Forwarder Types
- There are three different types of Splunk forwarders:
  - **Universal** forwarder: Contains only the components that are necessary to forward data. A universal forwarder has no indexing or data searching capability; however, they have the smallest system resource usage.
  - **Heavy** forwarder: Parses data before it is forwarded and routes data based on criteria like source or event type.
    - A heavy forwarder also indexes data locally and forwards to another Splunk instance.
    - With all these features, a heavy forwarder consumes the most system resources.
  - **Light** forwarder: Sacrifices additional capabilities found in heavy forwarders for the sake of preserving system resources.
    - No indexing or data parsing is performed and is used only to forward data to Splunk instances.
    - A light forwarder is generally used only in legacy applications.
<img width="660" height="357" alt="image" src="https://github.com/user-attachments/assets/7822a2b7-38ab-41e4-9cd6-e4af4b346053" />


#### D﻿ata Types
- The primary use case for these forwarders is in a network environment that feeds data collection to a Splunk Enterprise instance.
- There are three different types of data that these forwarders transmit:
  - **Raw**: The forwarder sends unaltered data over a Transmission Control Protocol (TCP) stream.
    - It does not convert the data into a Splunk communication format.
    - The forwarder simply collects data and sends it, which is useful for sending data to non-Splunk systems.
  - **Unparsed**: No transformations are done to the data; however, the data is tagged with a source, source type, and host information.
    - The data stream is also divided into blocks and timestamped for easier categorization by the receiving indexer.
  - **Parsed**: This data is broken down into individual events, which are then tagged and forwarded.
    - Because the data has been parsed, the forwarder can perform conditional routing based on specific data found in the events.
  
  - Both **universal and heavy forwarders are capable of sending raw data**. However, **unparsed data** is used for **universal forwarders** and **parsed data** is used for **heavy forwarders**.

#### Advanced Features and Configurations
- Many advanced features are used with Splunk forwarders to help increase their capabilities.
- Such features include data parsing, which allows the performance of event and field extractions.
- Searching and alerting allows alert generation and pre-instance searching for heavy forwarders.
- Load balancing forwards data over multiple paths to alleviate any congestion in network traffic that may occur.
- Event filtering analyzes each event and only forwards data that meets filter criteria.

- Once a Splunk forwarder is downloaded from the Splunk website, it can be installed.
- The installation wizard provides opportunities for a username and password to be configured, as well as an endpoint specified to receive forwarded data.
- Additionally, a receiving indexer can be configured.

- There are four key Splunk universal forwarder configuration files:
  - **inputs**.conf controls how the forwarder **collects data**.
  - **outputs**.conf controls how the forwarder **sends data** to an indexer or other forwarder.
  - **server**.conf controls **connection and performance** tuning.
  - **deploymentclient**.conf controls **connecting to a deployment server**.

- The configuration files of the universal forwarder can be edited at the installation location, `C:\Program Files\SplunkUniversalForwarder\etc\apps\SplunkUniversalForwarder\local`

- By editing configuration files, parameters can be set for Splunk. For example, the following can be added for input.conf to set parameters for the Windows Application Event Log:
  <img width="238" height="130" alt="image" src="https://github.com/user-attachments/assets/5393b6c6-83c2-40a5-a4c4-aa436480a14b" />

<img width="729" height="308" alt="image" src="https://github.com/user-attachments/assets/919e4621-0737-47e7-aaf4-05f265bc33e2" />












