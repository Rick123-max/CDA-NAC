# MOD 16
## Inputs
### Logstash Input Plug-Ins
- Architectural inputs are inputs generated by disparate architectural components of a network.
- In many modern networks, these inputs come from a variety of sources, making it difficult for CDAs to assess the inputs.
- To solve this problem, many tools exist to centralize and standardize the architectural inputs.

- **Logstash** solves the input problem for The Elastic Stack by using input plug-ins.
- Logstash uses the plug-ins to accept and ingest data from disparate sources into The Elastic Stack.
- Input plug-ins accept data from numerous sources and enable data to be read and validated by Logstash.
- Each input plug-in is designed for a specific event source, and the plug-ins can be combined to meet the unique needs of any network architecture.
- Table 16.1-1 provides a list of common Logstash input plug-ins.
  <img width="1668" height="1469" alt="70b12c3a-a447-4f8a-8650-99ac260b23ee" src="https://github.com/user-attachments/assets/822b6920-619d-4f79-aca9-4b6e2448ea70" />

- Using such plug-ins as **http**, **http_poller**, **tcp**, and **file**, analysts can **create custom inputs** and forward them using a **custom network connection**, an Application Programming Interface (**API**), or a **simple file**.
- If the existing customizable inputs do not meet mission requirements, developers may create custom input plug-ins to add support for unique sources.
- Developers can specify **data type**, **required data**, and **default values** for fields.
- In these plug-ins, developers and analysts can also **enrich** data by **adding fields** and **performing calculations**.

### Advanced Logstash Inputs
- Analysts may apply Logstash input plug-ins to meet mission requirements on unique networks and to ingest data from legacy systems not designed for use with The Elastic Stack.
- Other unique situations may require analysts to access data from devices not owned by the mission partner or make custom input plug-ins to ingest a unique data format.
- For example, if a mission partner has equipment that operates using Radio Frequency (RF), they may want to collect weather data to correlate with possible outages.
  - Analysts can use the **http_poller** input plug-in to collect data from a weather API in 1-hour intervals.
- In a different network, the mission partner may have a legacy system that records user data in a Structured Query Language (SQL) database.
  - Correlating this data with other events in The Elastic Stack can provide useful information in security investigations.
  - To ingest this database, analysts can implement the **sqlite** input plug-in to ingest the database as events.

### Logstash Input Strategies
- Generally, organizations should apply the highest-level input concept available based on available resources.

#### Ingest Everything
- The Ingest Everything concept uses the **most resources** and generates the **most events**.
- This strategy requires more time to configure but generates the largest amount of events.
- The downside of using this concept is that analysts must take time to parse through the large number of events to filter out noise to generate useful alerts.
- Once the initial configuration is complete, this concept collects only the most critical data and provides the greatest context to investigations.

#### Network Connections and Key Terrain in Cyberspace
- Under the Network Connections and Key Terrain in Cyberspace (KT-C) ingestion concept, **not all events are forwarded** to Logstash inputs.
- Instead, **detailed events** are forwarded **primarily from KT-C** and **critical assets**, whereas connection events from all network segments are used to provide additional context.
- Using this strategy **lowers the overall amount of data collected** relative to the Ingest Everything strategy but **still collects enough events** to give analysts an understanding of what occurs on the network.

#### Network Connections and Minimal System Data
- The Network Connections and Minimal System Data concept ingests the **smallest number of events**.
- This concept is **not recommended** because it can **make identifying incidents difficult** due to **lack of visibility**.
- It should be used only when minimal system resources are available for ingesting. Under this concept, network connections and some system events are ingested.

<img width="678" height="298" alt="image" src="https://github.com/user-attachments/assets/c26cc5f9-2fe2-429f-b4e1-bf7782879d73" />

### Implement Logstash Stdin Input Plug-In
- Run the following to start logstash with a config: `logstash -e  'input { stdin {}} output { stdout {}}'
- The common fields that Logstash generates during processing and output in JSON format are as follows:
  - **hostname**: The logstash node that processed the log.
  - **timestamp**: The time the Logstash received the log.
  - **original**: The contents of the raw log that Logstash received.
  - **message**: The resulting message after Logstash processes the raw log.

## Enriching Data with Filters
### Logstash Filters
- When CDAs receive an event in Logstash, they may find they have limited ways to use the event in an investigation or dashboard.
- For example, most network logs store the source of an event as an Internet Protocol (IP) address, but syslog stores only the hostname by default.
- By enriching syslog data to include a source IP address, CDAs can more easily correlate events.
- Logstash filter plug-ins provide the ability for analysts to enrich events before outputting the data to the Elasticsearch database.
- This process is the second-to-last process in the Logstash pipeline and prepares events for output.
- With Logstash filter plug-ins, analysts can ensure that incoming events are structured correctly and contain normalized fields, which enhances the ability of analysts to correlate events.
- These plug-ins can enrich the data by assessing conditions in the data, accessing external information, and extraction.

#### Conditional Filters
- Conditional Logstash filter plug-ins read data from the ingested data and perform actions based on predefined conditions.
<img width="1668" height="1032" alt="8801078a-b49f-4fc7-a727-b2f66e8d91ea" src="https://github.com/user-attachments/assets/b537217f-5efd-4bcc-8af9-3f6fc5d91ae0" />

#### External Information Filters
- External filters extract information from certain fields ingested and use external resources to enrich the data and populate new fields.
<img width="1668" height="994" alt="60451412-f6c1-48e9-a6e8-0735aa86ce98" src="https://github.com/user-attachments/assets/8c01ce8f-6579-4135-ad27-18ee528b0786" />

#### Extraction Filters
- Extraction filters use existing fields and data to generate new fields. This is especially useful when used to ingest unstructured data.
<img width="1668" height="1515" alt="01748afa-ad6e-4d48-8733-b46d7f9e0a35" src="https://github.com/user-attachments/assets/9248bc31-2e0b-4ab7-8220-9349b39de3ef" />

<img width="740" height="300" alt="image" src="https://github.com/user-attachments/assets/b7caa67e-7e9f-416f-b3b8-9a2bb9e7ec7e" />

### Logstash Performance and Data Normalization
- When data from different sources is ingested into Logstash, analysts may find it difficult to correlate events because the data is non-standard.
- Normalization, the act of organizing data from disparate sources into standardized fields, is the solution to this problem.
- Normalization enables analysts to create shared fields across data from various sources to enhance event correlation.
- Using shared fields, analysts can **create accurate visualizations and metrics** from different sources.
- Useful fields that are commonly included in data normalization are **timestamps**, Unique Identifiers (**UID**), and **source devices**.

### Logstash Performance
#### Filter Performance
- To improve performance of The Elastic Stack, analysts may implement Logstash filters to filter out fields that are not important.
- Filtering out fields that are redundant or that offer little benefit to investigations can improve the performance of Elasticsearch indexing and decrease the amount of time Elasticsearch requires to read and write the event.
- This can also improve performance of Kibana because the event has less information to read when generating visualizations.

- Another way to improve Logstash performance is by using the **Throttle** filter.
- This filter may be used to store a limited number of high-frequency events over a time period.
- Analysts can specify a **time period** and the **lower and upper limits** of the events.
- When the first event comes in, Logstash identifies that the time period has started and begins tracking the event.
- Once the lower-limit threshold is met, the events are stored.
- If the upper limit is met before the time period is complete, the Throttle filter drops the extra events until the process begins again.
<img width="1668" height="1538" alt="f531342f-6466-49fc-b26c-15fb83a72f50" src="https://github.com/user-attachments/assets/cc2c07ed-ed3d-4a85-bdbf-56474f79900f" />

#### Logstash Config Performance
- The final way to improve the performance of the Logstash pipeline is by modifying the Logstash configuration files.
- Although Logstash defaults are designed to prioritize speed and performance, some applications may require tuning, depending on the environment.
- If events are backed up but the Central Processing Unit (**CPU**) is not running at capacity, the `pipeline.workers` setting can be used to **tune the number of threads** used by Logstash.
- It may be beneficial to increase the number of workers past the number of threads in the CPU, as some threads can maintain a “waiting on event” status.
- The `pipeline.batch.size` setting **defines the maximum number of events processed by an individual worker thread**.
- A higher-number batch size is usually more efficient but may have high memory requirements to function properly.
- If the batch size is too high relative to the resources available, pipelines may crash and out-of-memory exceptions may occur, resulting in lost log events and blind spots for analysts.
- Finally, the **latency of the Logstash pipeline** can be tuned using the `pipeline.batch.delay` setting.
- The latency defines the amount of time a Logstash worker thread waits for additional messages after receiving an initial event.
- Once the time period expires, filters are executed against the event.

### Implementing Logstash Filters
- Although Logstash filters can greatly improve the quality of event collection, analysts may maximize the benefit by combining multiple filters.
- The filters that should be combined are dependent on the effect analysts would like to have on event data.
- If data is unstructured, it is usually preferable to first structure the data to simplify use of other filters.
- After structuring data, analysts should apply filters that remove fields (usually conditional filters) first to improve overall performance.
- Applying fields that can normalize data and add new fields — usually extraction filters — can then be used.
- Finally, analysts can improve and enrich ingested data using external information filters. 

### Dissect, Drop, and Mutate Filters
- The **Dissect** filter plug-in is excellent for **parsing unstructured data based on delimiters**.
- Common examples of datasets that use delimiters are Comma-Separated Values (**CSV**) or Tab-Separated Values (**TSV**).
- The Dissect filter is best used when the **delimited data reliably follows a repeatable pattern line by line**. 

- As an example, Zeek can log data in a TSV format, and each line in the log follows the same pattern of available fields, such as timestamp, UID, and source IP.
- Other log types may change entirely line by line, such as the information saved to /var/log/messages, where log formats change based on the process that generates the log or based on event severity.

- The **Drop** filter may be used to **ignore unwanted logs** so they are not forwarded for additional processing.
- Proper implementation of Drop filters can **aid performance for Logstash and prune data before it reaches a final destination**, such as a SIEM.









