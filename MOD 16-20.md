# MOD 16
## Inputs
### Logstash Input Plug-Ins
- Architectural inputs are inputs generated by disparate architectural components of a network.
- In many modern networks, these inputs come from a variety of sources, making it difficult for CDAs to assess the inputs.
- To solve this problem, many tools exist to centralize and standardize the architectural inputs.

- **Logstash** solves the input problem for The Elastic Stack by using input plug-ins.
- Logstash uses the plug-ins to accept and ingest data from disparate sources into The Elastic Stack.
- Input plug-ins accept data from numerous sources and enable data to be read and validated by Logstash.
- Each input plug-in is designed for a specific event source, and the plug-ins can be combined to meet the unique needs of any network architecture.
- Table 16.1-1 provides a list of common Logstash input plug-ins.
  <img width="1668" height="1469" alt="70b12c3a-a447-4f8a-8650-99ac260b23ee" src="https://github.com/user-attachments/assets/822b6920-619d-4f79-aca9-4b6e2448ea70" />

- Using such plug-ins as **http**, **http_poller**, **tcp**, and **file**, analysts can **create custom inputs** and forward them using a **custom network connection**, an Application Programming Interface (**API**), or a **simple file**.
- If the existing customizable inputs do not meet mission requirements, developers may create custom input plug-ins to add support for unique sources.
- Developers can specify **data type**, **required data**, and **default values** for fields.
- In these plug-ins, developers and analysts can also **enrich** data by **adding fields** and **performing calculations**.

### Advanced Logstash Inputs
- Analysts may apply Logstash input plug-ins to meet mission requirements on unique networks and to ingest data from legacy systems not designed for use with The Elastic Stack.
- Other unique situations may require analysts to access data from devices not owned by the mission partner or make custom input plug-ins to ingest a unique data format.
- For example, if a mission partner has equipment that operates using Radio Frequency (RF), they may want to collect weather data to correlate with possible outages.
  - Analysts can use the **http_poller** input plug-in to collect data from a weather API in 1-hour intervals.
- In a different network, the mission partner may have a legacy system that records user data in a Structured Query Language (SQL) database.
  - Correlating this data with other events in The Elastic Stack can provide useful information in security investigations.
  - To ingest this database, analysts can implement the **sqlite** input plug-in to ingest the database as events.

### Logstash Input Strategies
- Generally, organizations should apply the highest-level input concept available based on available resources.

#### Ingest Everything
- The Ingest Everything concept uses the **most resources** and generates the **most events**.
- This strategy requires more time to configure but generates the largest amount of events.
- The downside of using this concept is that analysts must take time to parse through the large number of events to filter out noise to generate useful alerts.
- Once the initial configuration is complete, this concept collects only the most critical data and provides the greatest context to investigations.

#### Network Connections and Key Terrain in Cyberspace
- Under the Network Connections and Key Terrain in Cyberspace (KT-C) ingestion concept, **not all events are forwarded** to Logstash inputs.
- Instead, **detailed events** are forwarded **primarily from KT-C** and **critical assets**, whereas connection events from all network segments are used to provide additional context.
- Using this strategy **lowers the overall amount of data collected** relative to the Ingest Everything strategy but **still collects enough events** to give analysts an understanding of what occurs on the network.

#### Network Connections and Minimal System Data
- The Network Connections and Minimal System Data concept ingests the **smallest number of events**.
- This concept is **not recommended** because it can **make identifying incidents difficult** due to **lack of visibility**.
- It should be used only when minimal system resources are available for ingesting. Under this concept, network connections and some system events are ingested.

<img width="678" height="298" alt="image" src="https://github.com/user-attachments/assets/c26cc5f9-2fe2-429f-b4e1-bf7782879d73" />

### Implement Logstash Stdin Input Plug-In
- Run the following to start logstash with a config: `logstash -e  'input { stdin {}} output { stdout {}}'
- The common fields that Logstash generates during processing and output in JSON format are as follows:
  - **hostname**: The logstash node that processed the log.
  - **timestamp**: The time the Logstash received the log.
  - **original**: The contents of the raw log that Logstash received.
  - **message**: The resulting message after Logstash processes the raw log.

## Enriching Data with Filters
### Logstash Filters
- When CDAs receive an event in Logstash, they may find they have limited ways to use the event in an investigation or dashboard.
- For example, most network logs store the source of an event as an Internet Protocol (IP) address, but syslog stores only the hostname by default.
- By enriching syslog data to include a source IP address, CDAs can more easily correlate events.
- Logstash filter plug-ins provide the ability for analysts to enrich events before outputting the data to the Elasticsearch database.
- This process is the second-to-last process in the Logstash pipeline and prepares events for output.
- With Logstash filter plug-ins, analysts can ensure that incoming events are structured correctly and contain normalized fields, which enhances the ability of analysts to correlate events.
- These plug-ins can enrich the data by assessing conditions in the data, accessing external information, and extraction.

#### Conditional Filters
- Conditional Logstash filter plug-ins read data from the ingested data and perform actions based on predefined conditions.
<img width="1668" height="1032" alt="8801078a-b49f-4fc7-a727-b2f66e8d91ea" src="https://github.com/user-attachments/assets/b537217f-5efd-4bcc-8af9-3f6fc5d91ae0" />

#### External Information Filters
- External filters extract information from certain fields ingested and use external resources to enrich the data and populate new fields.
<img width="1668" height="994" alt="60451412-f6c1-48e9-a6e8-0735aa86ce98" src="https://github.com/user-attachments/assets/8c01ce8f-6579-4135-ad27-18ee528b0786" />

#### Extraction Filters
- Extraction filters use existing fields and data to generate new fields. This is especially useful when used to ingest unstructured data.
<img width="1668" height="1515" alt="01748afa-ad6e-4d48-8733-b46d7f9e0a35" src="https://github.com/user-attachments/assets/9248bc31-2e0b-4ab7-8220-9349b39de3ef" />

<img width="740" height="300" alt="image" src="https://github.com/user-attachments/assets/b7caa67e-7e9f-416f-b3b8-9a2bb9e7ec7e" />

### Logstash Performance and Data Normalization
- When data from different sources is ingested into Logstash, analysts may find it difficult to correlate events because the data is non-standard.
- Normalization, the act of organizing data from disparate sources into standardized fields, is the solution to this problem.
- Normalization enables analysts to create shared fields across data from various sources to enhance event correlation.
- Using shared fields, analysts can **create accurate visualizations and metrics** from different sources.
- Useful fields that are commonly included in data normalization are **timestamps**, Unique Identifiers (**UID**), and **source devices**.

### Logstash Performance
#### Filter Performance
- To improve performance of The Elastic Stack, analysts may implement Logstash filters to filter out fields that are not important.
- Filtering out fields that are redundant or that offer little benefit to investigations can improve the performance of Elasticsearch indexing and decrease the amount of time Elasticsearch requires to read and write the event.
- This can also improve performance of Kibana because the event has less information to read when generating visualizations.

- Another way to improve Logstash performance is by using the **Throttle** filter.
- This filter may be used to store a limited number of high-frequency events over a time period.
- Analysts can specify a **time period** and the **lower and upper limits** of the events.
- When the first event comes in, Logstash identifies that the time period has started and begins tracking the event.
- Once the lower-limit threshold is met, the events are stored.
- If the upper limit is met before the time period is complete, the Throttle filter drops the extra events until the process begins again.
<img width="1668" height="1538" alt="f531342f-6466-49fc-b26c-15fb83a72f50" src="https://github.com/user-attachments/assets/cc2c07ed-ed3d-4a85-bdbf-56474f79900f" />

#### Logstash Config Performance
- The final way to improve the performance of the Logstash pipeline is by modifying the Logstash configuration files.
- Although Logstash defaults are designed to prioritize speed and performance, some applications may require tuning, depending on the environment.
- If events are backed up but the Central Processing Unit (**CPU**) is not running at capacity, the `pipeline.workers` setting can be used to **tune the number of threads** used by Logstash.
- It may be beneficial to increase the number of workers past the number of threads in the CPU, as some threads can maintain a “waiting on event” status.
- The `pipeline.batch.size` setting **defines the maximum number of events processed by an individual worker thread**.
- A higher-number batch size is usually more efficient but may have high memory requirements to function properly.
- If the batch size is too high relative to the resources available, pipelines may crash and out-of-memory exceptions may occur, resulting in lost log events and blind spots for analysts.
- Finally, the **latency of the Logstash pipeline** can be tuned using the `pipeline.batch.delay` setting.
- The latency defines the amount of time a Logstash worker thread waits for additional messages after receiving an initial event.
- Once the time period expires, filters are executed against the event.

### Implementing Logstash Filters
- Although Logstash filters can greatly improve the quality of event collection, analysts may maximize the benefit by combining multiple filters.
- The filters that should be combined are dependent on the effect analysts would like to have on event data.
- If data is unstructured, it is usually preferable to first structure the data to simplify use of other filters.
- After structuring data, analysts should apply filters that remove fields (usually conditional filters) first to improve overall performance.
- Applying fields that can normalize data and add new fields — usually extraction filters — can then be used.
- Finally, analysts can improve and enrich ingested data using external information filters. 

### Dissect, Drop, and Mutate Filters
- The **Dissect** filter plug-in is excellent for **parsing unstructured data based on delimiters**.
- Common examples of datasets that use delimiters are Comma-Separated Values (**CSV**) or Tab-Separated Values (**TSV**).
- The Dissect filter is best used when the **delimited data reliably follows a repeatable pattern line by line**. 

- As an example, Zeek can log data in a TSV format, and each line in the log follows the same pattern of available fields, such as timestamp, UID, and source IP.
- Other log types may change entirely line by line, such as the information saved to /var/log/messages, where log formats change based on the process that generates the log or based on event severity.

- The **Drop** filter may be used to **ignore unwanted logs** so they are not forwarded for additional processing.
- Proper implementation of Drop filters can **aid performance for Logstash and prune data before it reaches a final destination**, such as a SIEM.


## Pipelines
### Logstash Pipeline Flow
- The Logstash pipeline has three stages: inputs, filters, and outputs.
- Inputs generate events that can be modified by filters and forwarded to another location by outputs.
- Inputs and filters were discussed in detail in previous lessons; they give analysts the ability to ingest a wide variety of event sources and normalize or enrich that data.
- At the end of the pipeline, output plug-ins can store the data in a variety of formats, which are discussed later in this lesson.
- After events have been processed by the Logstash pipeline, they can be accessed by analysts to identify anomalous activity and investigate Malicious Cyber Activity (MCA).

#### Inputs
- Logstash plug-ins accept and ingest data from disparate sources into The Elastic Stack and generate events.
- Each input plug-in is designed for a specific event source and can be combined to meet the unique needs of any network architecture, regardless of the systems and network segments.
- Table 16.3-1 provides a list of some common Logstash input plug-ins.
<img width="1668" height="1469" alt="f8fd6f25-dff0-4e35-ae34-f09bc3f4cf8c" src="https://github.com/user-attachments/assets/88d076a2-76b8-4251-b4fb-88ed87c243f5" />

#### Filters
- With Logstash filter plug-ins, analysts can ensure that incoming events are structured correctly and contain normalized fields, which enhances the ability of analysts to correlate events.
- These plug-ins can enrich the data by assessing conditions in the data, accessing external information, and extraction.
- Filters can also be used to increase performance of the Logstash pipeline. Table 16.3-2 provides a list of some common Logstash filter plug-ins.
<img width="1999" height="1453" alt="a55bb3d7-e705-4332-a62f-bed937c4cc6b" src="https://github.com/user-attachments/assets/2fc53f2c-c757-43f8-b37e-d40b712b039b" />

#### Outputs
- Once events are ingested and normalized using inputs and filters, they are forwarded to a log repository.
- Events can be forwarded to a variety of outputs, which enables analysts to analyze data over several tools.
- Analysts can also choose to configure multiple outputs for the same dataset to enable analysis by multiple inspection platforms, which can help analysts identify activity that may have been missed in a single environment.
- If multiple inspection platforms are used, it is important to normalize and categorize the data to make the events readable by the multiple platforms.
- The most common output plug-in is Elasticsearch, which analysts can use to forward logs for use in Kibana. Table 16.3-3 provides a list of common Logstash output plug-ins. 
<img width="1999" height="807" alt="413c95a4-551c-4d51-992d-5d215fb924fe" src="https://github.com/user-attachments/assets/0a1d33f7-b72d-4f24-9302-24b6d8f5134f" />

### Output to Elastic Indexes
- Data in Elasticsearch is organized using indexes that organize events into a common database.
- Data should be normalized based on the destination index, and different indexes can have different types of events.
- Data is loaded into Kibana on an index-by-index basis, which leads to dashboards and visualizations being limited to one index.
- When similar data is collected from different tools, it is common practice to ingest the data into different indexes.
- An example of this is to ingest data from Zeek into a Zeek index and data from Suricata into a Suricata index, despite the similarities of the data.

- The easiest way to forward logs to different indexes is to use multiple pipelines for each input to be separated.
- However, this is not always possible, due to mission requirements or resources available.
- To forward events to different indexes on the same pipeline, analysts can configure tags in the pipeline configuration file, which can then be output to different indexes.
- Using this technique, analysts can have granular control over which index to output events to in order to enhance analysis.

### Data Retention
- Data retention requirements can be a complex issue. Some data may have to be retained for a certain length of time by legal, regulatory, or mission requirements.
- This can conflict with storage resources available, as data can quickly pile up when ingesting much event data.
- At the same time, analysts want to retain data for as long as possible in order to enhance investigations and identify anomalous activity.
- Balancing these considerations requires compromise where available and may often require separate data retention solutions. These decisions should be documented in a data retention policy.

#### Data Retention Solutions
##### Hot Storage
- Hot storage solutions are designed for **data that must be quickly processed and searched**.
- Typically, a hot storage solution stores the most recent and relevant data with the time frame dictated by the mission partner and system resources.
- Server nodes hosting data in the hot storage tier generally require additional compute resources such as increased Central Processing Unit (CPU), Random Access Memory (RAM), and fast disk arrays to meet performance needs.
- An example of a common policy is to store 1 month of events in a hot storage solution.


##### Warm Storage
- Warm storage solutions are used to **store data that is queried less frequently** than recently indexed data kept in hot storage.
- Nodes hosting data in the warm storage tier have lower performance requirements than hot storage nodes. Therefore, this hardware is generally less expensive.
- This solution provides organizations with an opportunity to store data over longer periods and reduce cost associated with high-performance compute nodes. 

##### Cold Storage
- Under the cold storage solution, **older data is retained** in an **inspection platform** that is still **searchable** but is considerably **slower than hot or warm tiers**.
- This solution prioritizes bulk storage of data over search performance.
- The data stored in cold storage generally includes log and event data that is not needed for operations but must be maintained to meet legal or regulatory requirements.
- The mission partner or organization should make a determination for the length of time that different sources should be stored in cold storage.

##### Data Archiving
- When the mission partner or organization has determined that data is no longer useful but must still be kept available to meet legal or regulatory requirements, the organization can use a data-archiving solution.
- This data may be stored in various ways, ranging from cold storage to physical records, and can be stored on or off premises.
- The data retention time should be defined by regulatory and legal requirements

##### Rolling Data
- The way the data retention policy is followed is dependent on the platform on which logs are stored.
- Some log platforms can be configured to automatically roll over data once it reaches a certain age, but other platforms may require a custom solution.
- To roll over data in Elasticsearch, analysts can configure Rollup, which consolidates older data into summaries that can be stored in cold storage.
- Custom scripts can also be scheduled to query Elasticsearch for events older than a specified age to be sent to a new index stored in a different location.

<img width="770" height="334" alt="image" src="https://github.com/user-attachments/assets/bfe573a2-c604-4092-b30a-6d839972d4f5" />

### Using Logstash Pipelines
1. Configure the input plug-in to read data from multiple log sources (Zeek conn.log, Zeek dns.log, and Suricata eve.json).
2. Remove duplicate data, such as the original event and message fields.
3. Normalize Internet Protocol (IP) and port fields from each log source to match the destination field name. Use if statements to normalize data based on log source.
  - Zeek
    - `id.orig_h => SrcIP`
    - `id.orig_p => SrcPort`
    - `id.resp_h => DstIP`
    - `id.resp_p => DstPort`
  - Suricata
    - `src_ip => SrcIP`
    - `src_port => SrcPort`
    - `dst_ip => DstIP`
    - `dst_port => DstPort`
4. Output the log data to an Elasticsearch instance.
5. Ensure that the original log timestamps are preserved and used by Elasticsearch.
6. Modify the default Elasticsearch retention policy for logs such that they are moved from hot to warm storage after 30 days and from warm to cold storage after 60 days.


# MOD 17
## Data Transformation
### Data Streams and Transformations
- A data stream is defined as the flow of data between intercommunicating systems.
- Data streams are therefore the way in which data flows into various (SIEM) applications, regardless of how that data is ingested.
- As an example, The Elastic Stack uses Logstash pipelines as its solution to create data streams to forward data to Elastic instances.
- The location of data streams must be applied such that inspection tools can gather a clear understanding of the network environment and the communications that occur.
- Each SIEM application generally has its own format of creating data streams, and each application requires its own configurations to modify the data stream to support data parsing.

### Data Streams for SIEM Platforms
- Data streams must be adjusted to meet the needs of the SIEM platforms in use.
- If Splunk forwarders are in use for a Splunk instance but an Elastic instance also exists, data may be forwarded to both locations and used within both SIEM platforms.
- In this lesson, Splunk and Kafka solutions are illustrated to generate data streams.
- However, creating a data stream of raw data is not very beneficial to the SIEM systems.
- Data within the data streams may be unusable, irrelevant, or difficult to read for analysis purposes.
- Transformations must be applied to alter the data for use within the SIEM platform.
- The following discusses how to apply data transformations within Splunk forwarder and Kafka Stream solutions.

### Spunk Transformations
- Configuring the data stream is as simple as configuring any Splunk forwarder, as covered in previous lessons.
- However, to perform transformations, filters must be applied to the data coming from the source.
- The following focuses on the details of adding a transformation within different Splunk forwarder instances.
- The first part shows the commands that apply only to Splunk instances that use the Splunk heavy forwarder.
- The Splunk universal forwarder is severely restricted in its capabilities to transform data, but some simple commands, briefly covered in the second part, allow for basic transformations as well.

#### Splunk Heavy Forwarder Transformation
- The `props.conf` file within the Splunk heavy forwarder is responsible for **configuring data properties**.
- The file exists in the `…/etc/system/local/` folder of the Splunk instance where the **heavy** forwarder is implemented.
  - This file is responsible for **deploying the commands programmed within** the `transforms.conf` file.
![61ce3729-3dc5-488f-b575-5197126c54c1](https://github.com/user-attachments/assets/0831eee8-a923-4b90-b9db-b74e4355dfcf)

- Within the` props.conf` file, code is used to specify the commands that transform the data and the order of operations of those commands.
- The following code is responsible for the order of operations for the transformations, specifically stating that a command will be used to drop events (**setnull**) and a command will be used to parse events (**setparsing**).
- The **parsing** of events here may also be referred to as **transforming** the events.
  - `[source::/var/log/messages]`
  - `TRANSFORMS-set= setnull,setparsing`
- The order of the commands in the `props.conf` file is extremely important.
- The order of operations is **reversed** in how the actions are executed.
- For example, in the syntax above, the **setnull** command **runs after** the **setparsing** command even though the setnull command is first in the list.
- If the order of the syntax were reversed, as shown below, setnull would drop all events before the setparsing command ran.
  - `[source::/var/log/messages]`
  - `TRANSFORMS-set= setparsing,setnull`
- Once additions have been made to the props.conf file, the file must be saved and closed.
- For any newly added commands in the props.conf file to take effect, the `transforms.conf` file must be edited to define the actions within the commands specified in the props.conf file.

![6bfe2e07-2302-4fc4-b026-83f5ed8ceedf](https://github.com/user-attachments/assets/64688f15-db5e-4bd6-8d14-fd93156c2adc)

- The `transforms.conf` file allows for code that specifies what actions to take to transform the data.
- The transforms.conf file also exists within the `…/etc/system/local/` subfolder of the Splunk heavy forwarder deployment.
- In following with the previous code in the `props.conf` file, the following code blocks illustrate the setnull and setparsing commands.
- The code for setnull is responsible for matching all data with a simple Regular Expression (RegEx) and sending the data to the nullQueue, which drops it.
- The code for setparsing is shown to use RegEx to match only events that contain the data that has Internet Protocol v4 (IPv4) addresses within the content. 
<img width="503" height="195" alt="image" src="https://github.com/user-attachments/assets/1d7553dd-fb6e-4c98-b4ac-9a556561fa1f" />

- Once the code has been added, the file must be saved and closed.
- The Splunk Enterprise instance must be restarted for any changes in the props.conf and transforms.conf files to take effect.
- All changes must be confirmed by validating that the new transformation is applied to the data visible through the Splunk User Interface (UI).
- After confirmation within the Splunk UI, the process of implementing a simple transformation within the Splunk heavy forwarder is complete.
- In most environments, hundreds of transformations are used to transform data from all available sources and provide the most relevant information.
- However, this task is typically performed by the network technician or administrator in charge of administering the Splunk system.

#### Splunk Universal Forwarder Transformations
- Transformations within the Splunk universal forwarder are limited.
- The Splunk universal forwarder is not capable of performing in-depth transformations and **does not read** the `transforms.conf` and `props.conf` files.
- However, the capabilities of the outputs and inputs allow for filtering of data based on the explicit commands **blacklist** and **whitelist**.
- The following illustrates a simple transformation to filter data as it is processed by the Splunk universal forwarder.
  - NOTE: The terms whitelist and blacklist are considered obsolete, and, in fact, the Splunk Splexicon has replaced the terms.
  - However, the Splunk commands whitelist and blacklist are still in use.
  - For consistency, this lesson continues these terms rather than the replacement terms **allowlist** and **blocklist**.

- The `inputs.conf` file exists within the Splunk universal forwarder deployment in the `…etc/system/local/` subfolder.
  - The inputs file contains all the controls for the Splunk universal forwarder that dictate which data should be processed and monitored on the host system where the universal forwarder is installed.
![06b1a46c-2dbb-408e-bbad-6b4d4696f604](https://github.com/user-attachments/assets/0dfab337-720f-4347-8617-e2b483cec3c3)

- The `outputs.conf` file also exists within the Splunk universal forwarder deployment in the `…etc/system/local subfolder.`
  - The outputs file contains all the controls for the Splunk universal forwarder regarding where data should be sent and which data should be forwarded.
![7c34b7d9-952c-479c-8b77-ca9c58e3c224](https://github.com/user-attachments/assets/8c6f79aa-30e8-4351-bb1a-7b5f8fc6577f)

- A **filter** is applied to specifically filter out which content is **monitored** or **forwarded**.
- The `inputs.conf` file is responsible for **dictating which data is monitored** on the device.
- The `outputs.conf` file is responsible for **stating which data is forwarded** to the Splunk instance.
- The **whitelist** or **blacklist** command is added to either file and functions by either not monitoring for data within the inputs file or not forwarding the data within the outputs file.
- The commands allow for data to be filtered from various sources by explicitly allowing only the stated data from a source (whitelist) or explicitly denying data from a source (blacklist).
- The following command is intended to be added to the inputs.conf file.
- The command illustrates a blacklist that uses a RegEx expression to exclude the monitoring of any files that have either a .txt extension or .gz extension when monitoring logs on the host. 
  - `[monitor:///mnt/logs]`
    - `blacklist = \.(?:txt|gz)$`

- After adding any code to either of the configuration files, the files must be saved and closed.
- The Splunk service for the universal forwarder must be restarted for the changes to take effect.
- The change examples above would prevent file monitoring for .txt and .gz files by using blacklist within the inputs.conf file. 

### Kafka Stream Transformations
- Kafka Streams are used to create streams of data within Kafka instances.
- Transformations can be applied to those streams to help normalize data as it is forwarded to the Kafka brokers, which process the data and send it to whichever UI is used to interface with the data.
- Kafka’s primary UI is called **Offset Explorer** (previously Kafka Tool), but it can be used to interface with most SIEM UIs.
- Kafka uses Java as the code for implementing Kafka and Kafka Streams.
- All Kafka Streams are defined within a Java application.
- Many different Java applications can be created and are used to perform various different data streams within the Kafka deployment.
- To add transformations to a Kafka Stream, the code for the Java file must be edited.
- Below is a small sample of the Java code for a Kafka Stream called FilterStream.
![10b38d97-738a-4ccf-a94f-2d65eaae51fe](https://github.com/user-attachments/assets/34be8677-c1f4-42e4-9765-ae254911f3ce)

- To perform transformations, code must be applied to filter the data.
- Many different transformations exist within Kafka. Kafka transformations are of two types:
  - **stateless**
    - Stateless transformations occur without knowledge of previous events in the stream.
    - These transformations are applied to every event in the stream as it passes through the application.
  - **stateful**.
    - Stateful transformations require knowledge of previous states of the data within the stream and perform actions that correlate data from previous events.
    - An example of a stateful transformation is a summation of values coming in through the data stream.
- The value of each message in the data stream must be known in order to compute the sum; therefore, knowledge of previous states of data must be known.
- Table 17.1-1 shows a breakdown of stateless transformations possible within Kafka Streams. 
<img width="1999" height="1495" alt="4d5b4b37-66e8-49d7-a4c2-a1e68160e33a" src="https://github.com/user-attachments/assets/b12178cc-f56a-4917-935b-fb47986bd954" />

## Data Normalization
### Data Normalization an dthe Common Information Model
- As discussed in previous lessons, **data normalization** is the _process of cleansing and organizing data to add value for analysis_.
  - Data normalization consists of modifying data from different sources to appear in a common format.
  - This eases readability and allows for shared data fields between events from disparate sources.
  - Additionally, normalization makes data useful, removing or cleansing unnecessary data sources and adjusting the presentation of data to provide only relevant information.
  - An open Information Technology (IT) standard was developed to define the need for normalization within IT environments: the Common Information Model (**CIM**).

#### Common Information Model
- CIM **illustrates the interrelationships of all managed objects within an IT environment** and **defines the need for the shared management** of these objects and relationships.
- The main purpose of CIM is to apply shared management to all objects within an IT environment and to all the interrelationships of these objects.
- CIM is commonly applied as a concept for management software for administering objects within an IT organization, but **managing information of the relationships of objects** is a necessity for both administrators and cybersecurity personnel.
- Figure 17.2-1 represents the implementation of CIM and illustrates the management of IT objects through IT management software and the management of data flow between the relationships of those objects accomplished by the SIEM.
<img width="1999" height="1031" alt="681ffabc-bc59-4ca7-b4f2-55b0b537fff6" src="https://github.com/user-attachments/assets/db410e71-a028-4a1c-8e13-9b69dbcf300a" />

- Managing the data of the relationships between objects leads to the use of SIEM platforms, which can be used to ingest data and apply the methodologies of CIM.
  - Applying the CIM to a SIEM platform means the data from all objects in an environment needs to appear in a common form despite being from different sources (normalization).
  - When the CIM standard is applied across SIEMs, all datasets are altered so that there is a common normalized view of events within SIEM platforms.

- Splunk has a solution to apply CIM standards: the Splunk CIM add-on, which contains a collection of data models and tools to normalize data.
  - The Splunk CIM configures multiple data models to sort data and shared field extractions for application of similar data fields across all data.
  - Applying these tools to all data sources within a Splunk instance helps to structure data for maximum efficiency.
  - Once configured, data from disparate sources should appear in a common form within the Splunk User Interface (UI).
  - Configuring Splunk CIM is as simple as running an executable. The Splunk CIM add-on is installed on the Splunk Enterprise system, and all data sources may be ingested into the normalization.

- The Elastic Stack also has a solution for normalizing data.
- Elastic Common Schema (ECS) in The Elastic Stack allows for consistent formats of data.
- ECS configuration is applied by defining ECS to function across a specific data source.
- For data to be normalized, each source must have a Command-Line Interface (CLI) statement defining the use of ECS.
- Applying ECS to an entire Elastic instance meets the standards defined by CIM.

- Other platforms do not have an easy implementation of CIM and normalizing data.
- Data transformations must be applied in a common manner to all data as it is ingested to facilitate data normalization.
- Each environment has its own needs based on the systems and applications in use, and the data from each system/application must be filtered to output similar data fields.
- Using these common transformations to normalize data from all sources facilitates the standard defined by the CIM. This applies to most SIEM tools, including Kafka.

<img width="630" height="270" alt="image" src="https://github.com/user-attachments/assets/774fac25-6086-4599-b3e3-99618c927554" />

<img width="726" height="273" alt="image" src="https://github.com/user-attachments/assets/819d4d6f-d848-41e0-b19a-d401d0eb5784" />

### Splunk Normalization with CIM
1. Go to SplunkBase, a Splunk site comprising add-on applications, and download the Splunk CIM add-on file to a workstation that can access the Splunk web interface.
2. In the Splunk web interface, select the gear icon next to the Apps menu: 
<img width="267" height="210" alt="b45b1477-b398-46d9-bd29-f6ba891a64da" src="https://github.com/user-attachments/assets/8a9b10b6-f9d1-49db-80d7-b3d5bb987d3e" />
  - An Install app from file option appears; select the option:
<img width="417" height="46" alt="4ef968aa-0fa6-44eb-b0d5-d9a598a37cfa" src="https://github.com/user-attachments/assets/a7f1ff3d-7bcb-491f-9ee6-4c12b5d5d01b" />

3. Select the Splunk CIM add-on file that was downloaded from SplunkBase, and install the application. 
  - Upon restarting Splunk, the Splunk CIM add-on is installed.
  - The application appears in the list of apps and add-ons called Splunk Common Information Model on the Splunk server at `$SPLUNK_HOME/etc/apps/Splunk_SA_CIM.`
  - After installation, configurations must be applied for Splunk CIM to have an effect on the data.
  - CIM add-on configuration pages are located in the Splunk web interface; go to `Select Apps > Manage Apps`, and then select Set Up in the Splunk Common Information Model add-on row. 

- Numerous data models are located on the CIM setup page.
- The data models are part of the Splunk CIM application and contain a large series of common events and shared field extractions to apply to various data sources.
- In application, any data models intended for use must be modified. Each environment requires different data models to be configured.
- For example, if multiple Intrusion Detection Systems (IDS) exist in an environment, the various signatures may be configured to report in the Intrusion Detection data model and create shared fields among all IDSs in use.
<img width="361" height="598" alt="41082bf4-f7b1-4e0f-a311-440b5ba4c78f" src="https://github.com/user-attachments/assets/2b4d77a5-5803-41a7-bff6-8a38efe6790a" />

- By default, the data models search for events within all indexes listed.
- However, if the needs of the environment dictate that only certain indexes should be used in certain CIM data models, the Indexes whitelist can be used. (The Indexes whitelist may also be called the Indexes allowlist.)
  - The Indexes whitelist allows indexes to be specified to apply per CIM data model.
  - Leaving this as the default is common practice. After data models are catered to a network environment, the CIM setup may be saved.
<img width="560" height="71" alt="0753c18c-4278-46b2-891c-5f5ecceb0da8" src="https://github.com/user-attachments/assets/a4d05744-1a78-4a54-850b-1a65a2ba1b36" />

- For data models to work, event types must be created and tagged to configure the source type to populate within configured CIM data models.
- The setting for event types is found by selecting Event Types on the Splunk Settings drop-down menu: 
<img width="206" height="176" alt="8d6f5032-eef0-4b7e-bafe-e57026c1c0a4" src="https://github.com/user-attachments/assets/2f23ef95-11a5-4752-86d1-4a9795a59fbe" />

- The New button allows for creation of a new event type.
- In the same menu, a destination application, name (arbitrary), source type (search string), and tag must be identified.
- Each data model has its own tag requirements.
- Entering search strings that match the data events to be returned and applying a tag that matches a specific data model must be done and saved to create a pivot to normalize the data.
<img width="1649" height="763" alt="f07bb695-f709-4eaa-bd26-ea05e12fe2fb" src="https://github.com/user-attachments/assets/5c641597-aa29-4339-b0bf-8049906b1fc4" />

- Finally, a **field alias** must be generated for the fields from the source type that need to match correlating fields within the CIM data model.
- The Fields link is located in the Settings drop-down menu:
<img width="197" height="219" alt="8206be17-7c8c-4a90-b5b2-ffb8c74a0b97" src="https://github.com/user-attachments/assets/d418193e-559c-4ba7-bc0e-5ef871bf2744" />

- Field aliases are specified within the Fields menu.
- To add a field alias, select the Add New button.
- Within any field alias, a destination application, name (arbitrary), source type, and field-to-alias statement need to be specified.
- The alias is intended to assign a coexisting name to a field.
- When applying this concept to the CIM, the aliases specified should match fields within the CIM data model.
- After specifying all details of the aliases required, select Save.
<img width="1777" height="857" alt="9453cb87-e4eb-42f4-8fe1-2094cc8b7f50" src="https://github.com/user-attachments/assets/a7cfee0b-0cd9-47ef-bc8e-ab12aea89d5c" />

- To validate that the data has been added to the CIM data model, find the data model edited through Data models in the Splunk Settings drop-down menu.
- Selecting the Pivot button on the edited data model displays the internal objects of the data model.
- Selecting any object displays all the events associated with that object, and fields may be searched.
- If the field alias assigned in the environment is functional, the events from the edited source type are visible. 
<img width="2001" height="448" alt="6a57b613-ffcc-4c92-9b8d-14962dce511c" src="https://github.com/user-attachments/assets/c4adb1e5-2d11-4ed4-a777-0dde354c9843" />

- The above steps facilitate a walk-through of all the requirements to install and configure Splunk CIM.
- Data from the sources specified are processed through the CIM add-on, and normalization techniques are automatically applied.

### Kafka Stream Normalization with CIM
- Kafka is a more complex method of applying normalization than Splunk.
- Transformations for data sources must be applied to reach the CIM standards.
- In the last lesson, Kafka Stream transformations were applied to modify the data being ingested.
- The same process must be used to achieve normalization, but transformations must be applied in a meaningful way that adds value to the data and creates a common display of the events from all data sources.
- The steps that follow illustrate a simple normalization achieved in Kafka by modifying multiple sources to have a shared theme among the data sent to the analysis platform.

1. Edit the Java application for the Kafka Stream. Figure 17.2-11 shows the FilterStream application from the prior lesson Data Transformation: 
![f350da3d-9b9b-45f4-889d-81297e28d6fc](https://github.com/user-attachments/assets/1b299e4b-76dd-40d6-a9fd-14c48bb9a724)

2. Add a transformation to a specific data source, using the same transformation from the lesson Data Transformation:
   <img width="690" height="75" alt="image" src="https://github.com/user-attachments/assets/f6864ca9-93c6-40dc-857b-c4749664f6e9" />

3. Repeat this transformation for all data sources to normalize messages from each source. Each data source may have different requirements and may need subtle tuning to meet the desired outcome of the filter. For example, the transformation in Step 2 limits messages to only those greater than 1,000 characters. If all messages from a specific data source were under that limit, the transformation would be useless. The transformation should be altered for each data source to find the minimum length of the message at default and only allow greater values. 
4. Save and compile all Kafka Stream files that have the transformation applied.
5. Apply the above steps to all Kafka Streams within the environments to encompass all data sources. This ensures that normalization is achieved from all sources. 
6. Verify that the data is being transformed by reviewing data in the analysis platform in use.
  - This process achieves normalization within the Kafka Stream. In the example above, only one application was running to initiate the Kafka Stream.
  - However, this process can become intensive when a large number of data sources is present and multiple Kafka Stream applications are in an environment.
  - To achieve normalization, a multitude of Kafka Stream transformations must be implemented.
  - Table 17.2-1 describes the Kafka Stream transformations mentioned in the last lesson, focusing on how to implement these transformations for normalization and improvement of data efficiency.
<img width="1999" height="1495" alt="d1dff2fb-885f-41c6-ac69-cd8148fe52d9" src="https://github.com/user-attachments/assets/cc872c5a-3cc0-40c2-8350-1f3cb5c07088" />

#### Branch
- The Branch Kafka Stream transformation is capable of **dividing a data stream** into multiple streams of data.
- The results of applying this transformation is a split of the data as it is ingested. Then data is forwarded to two or more inspection platforms.
- This can be used to create elasticity and redundancy within a Kafka implementation in which multiple brokers are in use.
- For example, if the data is being sent to only one destination and that destination is offline, all data from the source becomes unreachable.
- However, if two or more destinations are in use, the architecture allows for a system to be offline and data to still be accessible.
- Another solution is to use the Branch transformation to send data to multiple platforms in a range.
- If the architecture of the environment has Splunk, The Elastic Stack, and ROCK NSM, Kafka Streams can be branched to forward the data stream to all SIEM instances.

#### Filter
- The Filter Kafka Stream transformation provides the capability to restrict data that is sent to inspection platforms.
- Applying this transformation results in a variety of different outcomes, depending on the code used to filter the data.
- In its simplest form, Filter searches for a specific value within the data stream and, when a match is found, performs an operation to either block that data from the stream or allow only the specified data through.
- Filter is essential for increasing the performance and bandwidth usage of Kafka Streams.

#### flatMap
- The flatMap Kafka Stream transformation provides the capability to alter records as they are ingested.
- Applying this transformation results in searching for a specific value within the data stream and performing a function over that data.
- For example, if the only important portion of the data is a list of addresses, the data can be transformed into just a list of the addresses, and the new record replaces the original message to be forwarded to the inspection platforms.
- flatMap is capable of dividing single records into multiple new records or dropping the original record if no match is found.
- flatMap is essential to normalizing data from the data stream in that it can alter the messages so only relevant data is found, and all data appears in a common format. 

#### Map
- The Map Kafka Stream transformation is similar to flatMap in that Map also provides the capability to alter records as they are ingested.
- However, Map cannot divide single records into multiple new records.
- Map is capable of performing only a one-to-one record manipulation.
- This means that each record present within the data stream always produces one new record to be forwarded to the inspection platforms.
- Map, like flatMap, is essential to normalizing data from the data stream.
- Whether applying various transformations in Kafka or configuring the CIM in Splunk, normalization should always be implemented in the network environment.
- The methods above illustrate some of the tools available to help achieve normalization.

<img width="781" height="299" alt="image" src="https://github.com/user-attachments/assets/f3499214-a757-4fe3-b9cd-46485c6401b5" />
<img width="727" height="323" alt="image" src="https://github.com/user-attachments/assets/4f12b1c5-97d2-47c0-a835-e711ee3c71de" />

<img width="767" height="319" alt="image" src="https://github.com/user-attachments/assets/3d199da5-bb8f-49a1-aa38-a6061e4c4fdc" />








