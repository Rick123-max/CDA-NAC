# MOD 16
## Inputs
### Logstash Input Plug-Ins
- Architectural inputs are inputs generated by disparate architectural components of a network.
- In many modern networks, these inputs come from a variety of sources, making it difficult for CDAs to assess the inputs.
- To solve this problem, many tools exist to centralize and standardize the architectural inputs.

- **Logstash** solves the input problem for The Elastic Stack by using input plug-ins.
- Logstash uses the plug-ins to accept and ingest data from disparate sources into The Elastic Stack.
- Input plug-ins accept data from numerous sources and enable data to be read and validated by Logstash.
- Each input plug-in is designed for a specific event source, and the plug-ins can be combined to meet the unique needs of any network architecture.
- Table 16.1-1 provides a list of common Logstash input plug-ins.
  <img width="1668" height="1469" alt="70b12c3a-a447-4f8a-8650-99ac260b23ee" src="https://github.com/user-attachments/assets/822b6920-619d-4f79-aca9-4b6e2448ea70" />

- Using such plug-ins as **http**, **http_poller**, **tcp**, and **file**, analysts can **create custom inputs** and forward them using a **custom network connection**, an Application Programming Interface (**API**), or a **simple file**.
- If the existing customizable inputs do not meet mission requirements, developers may create custom input plug-ins to add support for unique sources.
- Developers can specify **data type**, **required data**, and **default values** for fields.
- In these plug-ins, developers and analysts can also **enrich** data by **adding fields** and **performing calculations**.

### Advanced Logstash Inputs
- Analysts may apply Logstash input plug-ins to meet mission requirements on unique networks and to ingest data from legacy systems not designed for use with The Elastic Stack.
- Other unique situations may require analysts to access data from devices not owned by the mission partner or make custom input plug-ins to ingest a unique data format.
- For example, if a mission partner has equipment that operates using Radio Frequency (RF), they may want to collect weather data to correlate with possible outages.
  - Analysts can use the **http_poller** input plug-in to collect data from a weather API in 1-hour intervals.
- In a different network, the mission partner may have a legacy system that records user data in a Structured Query Language (SQL) database.
  - Correlating this data with other events in The Elastic Stack can provide useful information in security investigations.
  - To ingest this database, analysts can implement the **sqlite** input plug-in to ingest the database as events.

### Logstash Input Strategies
- Generally, organizations should apply the highest-level input concept available based on available resources.

#### Ingest Everything
- The Ingest Everything concept uses the **most resources** and generates the **most events**.
- This strategy requires more time to configure but generates the largest amount of events.
- The downside of using this concept is that analysts must take time to parse through the large number of events to filter out noise to generate useful alerts.
- Once the initial configuration is complete, this concept collects only the most critical data and provides the greatest context to investigations.

#### Network Connections and Key Terrain in Cyberspace
- Under the Network Connections and Key Terrain in Cyberspace (KT-C) ingestion concept, **not all events are forwarded** to Logstash inputs.
- Instead, **detailed events** are forwarded **primarily from KT-C** and **critical assets**, whereas connection events from all network segments are used to provide additional context.
- Using this strategy **lowers the overall amount of data collected** relative to the Ingest Everything strategy but **still collects enough events** to give analysts an understanding of what occurs on the network.

#### Network Connections and Minimal System Data
- The Network Connections and Minimal System Data concept ingests the **smallest number of events**.
- This concept is **not recommended** because it can **make identifying incidents difficult** due to **lack of visibility**.
- It should be used only when minimal system resources are available for ingesting. Under this concept, network connections and some system events are ingested.

<img width="678" height="298" alt="image" src="https://github.com/user-attachments/assets/c26cc5f9-2fe2-429f-b4e1-bf7782879d73" />

### Implement Logstash Stdin Input Plug-In
- Run the following to start logstash with a config: `logstash -e  'input { stdin {}} output { stdout {}}'
- The common fields that Logstash generates during processing and output in JSON format are as follows:
  - **hostname**: The logstash node that processed the log.
  - **timestamp**: The time the Logstash received the log.
  - **original**: The contents of the raw log that Logstash received.
  - **message**: The resulting message after Logstash processes the raw log.

## Enriching Data with Filters
### Logstash Filters
- When CDAs receive an event in Logstash, they may find they have limited ways to use the event in an investigation or dashboard.
- For example, most network logs store the source of an event as an Internet Protocol (IP) address, but syslog stores only the hostname by default.
- By enriching syslog data to include a source IP address, CDAs can more easily correlate events.
- Logstash filter plug-ins provide the ability for analysts to enrich events before outputting the data to the Elasticsearch database.
- This process is the second-to-last process in the Logstash pipeline and prepares events for output.
- With Logstash filter plug-ins, analysts can ensure that incoming events are structured correctly and contain normalized fields, which enhances the ability of analysts to correlate events.
- These plug-ins can enrich the data by assessing conditions in the data, accessing external information, and extraction.

#### Conditional Filters
- Conditional Logstash filter plug-ins read data from the ingested data and perform actions based on predefined conditions.
<img width="1668" height="1032" alt="8801078a-b49f-4fc7-a727-b2f66e8d91ea" src="https://github.com/user-attachments/assets/b537217f-5efd-4bcc-8af9-3f6fc5d91ae0" />

#### External Information Filters
- External filters extract information from certain fields ingested and use external resources to enrich the data and populate new fields.
<img width="1668" height="994" alt="60451412-f6c1-48e9-a6e8-0735aa86ce98" src="https://github.com/user-attachments/assets/8c01ce8f-6579-4135-ad27-18ee528b0786" />

#### Extraction Filters
- Extraction filters use existing fields and data to generate new fields. This is especially useful when used to ingest unstructured data.
<img width="1668" height="1515" alt="01748afa-ad6e-4d48-8733-b46d7f9e0a35" src="https://github.com/user-attachments/assets/9248bc31-2e0b-4ab7-8220-9349b39de3ef" />

<img width="740" height="300" alt="image" src="https://github.com/user-attachments/assets/b7caa67e-7e9f-416f-b3b8-9a2bb9e7ec7e" />

### Logstash Performance and Data Normalization
- When data from different sources is ingested into Logstash, analysts may find it difficult to correlate events because the data is non-standard.
- Normalization, the act of organizing data from disparate sources into standardized fields, is the solution to this problem.
- Normalization enables analysts to create shared fields across data from various sources to enhance event correlation.
- Using shared fields, analysts can **create accurate visualizations and metrics** from different sources.
- Useful fields that are commonly included in data normalization are **timestamps**, Unique Identifiers (**UID**), and **source devices**.

### Logstash Performance
#### Filter Performance
- To improve performance of The Elastic Stack, analysts may implement Logstash filters to filter out fields that are not important.
- Filtering out fields that are redundant or that offer little benefit to investigations can improve the performance of Elasticsearch indexing and decrease the amount of time Elasticsearch requires to read and write the event.
- This can also improve performance of Kibana because the event has less information to read when generating visualizations.

- Another way to improve Logstash performance is by using the **Throttle** filter.
- This filter may be used to store a limited number of high-frequency events over a time period.
- Analysts can specify a **time period** and the **lower and upper limits** of the events.
- When the first event comes in, Logstash identifies that the time period has started and begins tracking the event.
- Once the lower-limit threshold is met, the events are stored.
- If the upper limit is met before the time period is complete, the Throttle filter drops the extra events until the process begins again.
<img width="1668" height="1538" alt="f531342f-6466-49fc-b26c-15fb83a72f50" src="https://github.com/user-attachments/assets/cc2c07ed-ed3d-4a85-bdbf-56474f79900f" />

#### Logstash Config Performance
- The final way to improve the performance of the Logstash pipeline is by modifying the Logstash configuration files.
- Although Logstash defaults are designed to prioritize speed and performance, some applications may require tuning, depending on the environment.
- If events are backed up but the Central Processing Unit (**CPU**) is not running at capacity, the `pipeline.workers` setting can be used to **tune the number of threads** used by Logstash.
- It may be beneficial to increase the number of workers past the number of threads in the CPU, as some threads can maintain a “waiting on event” status.
- The `pipeline.batch.size` setting **defines the maximum number of events processed by an individual worker thread**.
- A higher-number batch size is usually more efficient but may have high memory requirements to function properly.
- If the batch size is too high relative to the resources available, pipelines may crash and out-of-memory exceptions may occur, resulting in lost log events and blind spots for analysts.
- Finally, the **latency of the Logstash pipeline** can be tuned using the `pipeline.batch.delay` setting.
- The latency defines the amount of time a Logstash worker thread waits for additional messages after receiving an initial event.
- Once the time period expires, filters are executed against the event.

### Implementing Logstash Filters
- Although Logstash filters can greatly improve the quality of event collection, analysts may maximize the benefit by combining multiple filters.
- The filters that should be combined are dependent on the effect analysts would like to have on event data.
- If data is unstructured, it is usually preferable to first structure the data to simplify use of other filters.
- After structuring data, analysts should apply filters that remove fields (usually conditional filters) first to improve overall performance.
- Applying fields that can normalize data and add new fields — usually extraction filters — can then be used.
- Finally, analysts can improve and enrich ingested data using external information filters. 

### Dissect, Drop, and Mutate Filters
- The **Dissect** filter plug-in is excellent for **parsing unstructured data based on delimiters**.
- Common examples of datasets that use delimiters are Comma-Separated Values (**CSV**) or Tab-Separated Values (**TSV**).
- The Dissect filter is best used when the **delimited data reliably follows a repeatable pattern line by line**. 

- As an example, Zeek can log data in a TSV format, and each line in the log follows the same pattern of available fields, such as timestamp, UID, and source IP.
- Other log types may change entirely line by line, such as the information saved to /var/log/messages, where log formats change based on the process that generates the log or based on event severity.

- The **Drop** filter may be used to **ignore unwanted logs** so they are not forwarded for additional processing.
- Proper implementation of Drop filters can **aid performance for Logstash and prune data before it reaches a final destination**, such as a SIEM.


## Pipelines
### Logstash Pipeline Flow
- The Logstash pipeline has three stages: inputs, filters, and outputs.
- Inputs generate events that can be modified by filters and forwarded to another location by outputs.
- Inputs and filters were discussed in detail in previous lessons; they give analysts the ability to ingest a wide variety of event sources and normalize or enrich that data.
- At the end of the pipeline, output plug-ins can store the data in a variety of formats, which are discussed later in this lesson.
- After events have been processed by the Logstash pipeline, they can be accessed by analysts to identify anomalous activity and investigate Malicious Cyber Activity (MCA).

#### Inputs
- Logstash plug-ins accept and ingest data from disparate sources into The Elastic Stack and generate events.
- Each input plug-in is designed for a specific event source and can be combined to meet the unique needs of any network architecture, regardless of the systems and network segments.
- Table 16.3-1 provides a list of some common Logstash input plug-ins.
<img width="1668" height="1469" alt="f8fd6f25-dff0-4e35-ae34-f09bc3f4cf8c" src="https://github.com/user-attachments/assets/88d076a2-76b8-4251-b4fb-88ed87c243f5" />

#### Filters
- With Logstash filter plug-ins, analysts can ensure that incoming events are structured correctly and contain normalized fields, which enhances the ability of analysts to correlate events.
- These plug-ins can enrich the data by assessing conditions in the data, accessing external information, and extraction.
- Filters can also be used to increase performance of the Logstash pipeline. Table 16.3-2 provides a list of some common Logstash filter plug-ins.
<img width="1999" height="1453" alt="a55bb3d7-e705-4332-a62f-bed937c4cc6b" src="https://github.com/user-attachments/assets/2fc53f2c-c757-43f8-b37e-d40b712b039b" />

#### Outputs
- Once events are ingested and normalized using inputs and filters, they are forwarded to a log repository.
- Events can be forwarded to a variety of outputs, which enables analysts to analyze data over several tools.
- Analysts can also choose to configure multiple outputs for the same dataset to enable analysis by multiple inspection platforms, which can help analysts identify activity that may have been missed in a single environment.
- If multiple inspection platforms are used, it is important to normalize and categorize the data to make the events readable by the multiple platforms.
- The most common output plug-in is Elasticsearch, which analysts can use to forward logs for use in Kibana. Table 16.3-3 provides a list of common Logstash output plug-ins. 
<img width="1999" height="807" alt="413c95a4-551c-4d51-992d-5d215fb924fe" src="https://github.com/user-attachments/assets/0a1d33f7-b72d-4f24-9302-24b6d8f5134f" />

### Output to Elastic Indexes
- Data in Elasticsearch is organized using indexes that organize events into a common database.
- Data should be normalized based on the destination index, and different indexes can have different types of events.
- Data is loaded into Kibana on an index-by-index basis, which leads to dashboards and visualizations being limited to one index.
- When similar data is collected from different tools, it is common practice to ingest the data into different indexes.
- An example of this is to ingest data from Zeek into a Zeek index and data from Suricata into a Suricata index, despite the similarities of the data.

- The easiest way to forward logs to different indexes is to use multiple pipelines for each input to be separated.
- However, this is not always possible, due to mission requirements or resources available.
- To forward events to different indexes on the same pipeline, analysts can configure tags in the pipeline configuration file, which can then be output to different indexes.
- Using this technique, analysts can have granular control over which index to output events to in order to enhance analysis.

### Data Retention
- Data retention requirements can be a complex issue. Some data may have to be retained for a certain length of time by legal, regulatory, or mission requirements.
- This can conflict with storage resources available, as data can quickly pile up when ingesting much event data.
- At the same time, analysts want to retain data for as long as possible in order to enhance investigations and identify anomalous activity.
- Balancing these considerations requires compromise where available and may often require separate data retention solutions. These decisions should be documented in a data retention policy.

#### Data Retention Solutions
##### Hot Storage
- Hot storage solutions are designed for **data that must be quickly processed and searched**.
- Typically, a hot storage solution stores the most recent and relevant data with the time frame dictated by the mission partner and system resources.
- Server nodes hosting data in the hot storage tier generally require additional compute resources such as increased Central Processing Unit (CPU), Random Access Memory (RAM), and fast disk arrays to meet performance needs.
- An example of a common policy is to store 1 month of events in a hot storage solution.


##### Warm Storage
- Warm storage solutions are used to **store data that is queried less frequently** than recently indexed data kept in hot storage.
- Nodes hosting data in the warm storage tier have lower performance requirements than hot storage nodes. Therefore, this hardware is generally less expensive.
- This solution provides organizations with an opportunity to store data over longer periods and reduce cost associated with high-performance compute nodes. 

##### Cold Storage
- Under the cold storage solution, **older data is retained** in an **inspection platform** that is still **searchable** but is considerably **slower than hot or warm tiers**.
- This solution prioritizes bulk storage of data over search performance.
- The data stored in cold storage generally includes log and event data that is not needed for operations but must be maintained to meet legal or regulatory requirements.
- The mission partner or organization should make a determination for the length of time that different sources should be stored in cold storage.

##### Data Archiving
- When the mission partner or organization has determined that data is no longer useful but must still be kept available to meet legal or regulatory requirements, the organization can use a data-archiving solution.
- This data may be stored in various ways, ranging from cold storage to physical records, and can be stored on or off premises.
- The data retention time should be defined by regulatory and legal requirements

##### Rolling Data
- The way the data retention policy is followed is dependent on the platform on which logs are stored.
- Some log platforms can be configured to automatically roll over data once it reaches a certain age, but other platforms may require a custom solution.
- To roll over data in Elasticsearch, analysts can configure Rollup, which consolidates older data into summaries that can be stored in cold storage.
- Custom scripts can also be scheduled to query Elasticsearch for events older than a specified age to be sent to a new index stored in a different location.

<img width="770" height="334" alt="image" src="https://github.com/user-attachments/assets/bfe573a2-c604-4092-b30a-6d839972d4f5" />

### Using Logstash Pipelines
1. Configure the input plug-in to read data from multiple log sources (Zeek conn.log, Zeek dns.log, and Suricata eve.json).
2. Remove duplicate data, such as the original event and message fields.
3. Normalize Internet Protocol (IP) and port fields from each log source to match the destination field name. Use if statements to normalize data based on log source.
  - Zeek
    - `id.orig_h => SrcIP`
    - `id.orig_p => SrcPort`
    - `id.resp_h => DstIP`
    - `id.resp_p => DstPort`
  - Suricata
    - `src_ip => SrcIP`
    - `src_port => SrcPort`
    - `dst_ip => DstIP`
    - `dst_port => DstPort`
4. Output the log data to an Elasticsearch instance.
5. Ensure that the original log timestamps are preserved and used by Elasticsearch.
6. Modify the default Elasticsearch retention policy for logs such that they are moved from hot to warm storage after 30 days and from warm to cold storage after 60 days.








