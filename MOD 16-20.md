# MOD 16
## Inputs
### Logstash Input Plug-Ins
- Architectural inputs are inputs generated by disparate architectural components of a network.
- In many modern networks, these inputs come from a variety of sources, making it difficult for CDAs to assess the inputs.
- To solve this problem, many tools exist to centralize and standardize the architectural inputs.

- **Logstash** solves the input problem for The Elastic Stack by using input plug-ins.
- Logstash uses the plug-ins to accept and ingest data from disparate sources into The Elastic Stack.
- Input plug-ins accept data from numerous sources and enable data to be read and validated by Logstash.
- Each input plug-in is designed for a specific event source, and the plug-ins can be combined to meet the unique needs of any network architecture.
- Table 16.1-1 provides a list of common Logstash input plug-ins.
  <img width="1668" height="1469" alt="70b12c3a-a447-4f8a-8650-99ac260b23ee" src="https://github.com/user-attachments/assets/822b6920-619d-4f79-aca9-4b6e2448ea70" />

- Using such plug-ins as **http**, **http_poller**, **tcp**, and **file**, analysts can **create custom inputs** and forward them using a **custom network connection**, an Application Programming Interface (**API**), or a **simple file**.
- If the existing customizable inputs do not meet mission requirements, developers may create custom input plug-ins to add support for unique sources.
- Developers can specify **data type**, **required data**, and **default values** for fields.
- In these plug-ins, developers and analysts can also **enrich** data by **adding fields** and **performing calculations**.

### Advanced Logstash Inputs
- Analysts may apply Logstash input plug-ins to meet mission requirements on unique networks and to ingest data from legacy systems not designed for use with The Elastic Stack.
- Other unique situations may require analysts to access data from devices not owned by the mission partner or make custom input plug-ins to ingest a unique data format.
- For example, if a mission partner has equipment that operates using Radio Frequency (RF), they may want to collect weather data to correlate with possible outages.
  - Analysts can use the **http_poller** input plug-in to collect data from a weather API in 1-hour intervals.
- In a different network, the mission partner may have a legacy system that records user data in a Structured Query Language (SQL) database.
  - Correlating this data with other events in The Elastic Stack can provide useful information in security investigations.
  - To ingest this database, analysts can implement the **sqlite** input plug-in to ingest the database as events.

### Logstash Input Strategies
- Generally, organizations should apply the highest-level input concept available based on available resources.

#### Ingest Everything
- The Ingest Everything concept uses the **most resources** and generates the **most events**.
- This strategy requires more time to configure but generates the largest amount of events.
- The downside of using this concept is that analysts must take time to parse through the large number of events to filter out noise to generate useful alerts.
- Once the initial configuration is complete, this concept collects only the most critical data and provides the greatest context to investigations.

#### Network Connections and Key Terrain in Cyberspace
- Under the Network Connections and Key Terrain in Cyberspace (KT-C) ingestion concept, **not all events are forwarded** to Logstash inputs.
- Instead, **detailed events** are forwarded **primarily from KT-C** and **critical assets**, whereas connection events from all network segments are used to provide additional context.
- Using this strategy **lowers the overall amount of data collected** relative to the Ingest Everything strategy but **still collects enough events** to give analysts an understanding of what occurs on the network.

#### Network Connections and Minimal System Data
- The Network Connections and Minimal System Data concept ingests the **smallest number of events**.
- This concept is **not recommended** because it can **make identifying incidents difficult** due to **lack of visibility**.
- It should be used only when minimal system resources are available for ingesting. Under this concept, network connections and some system events are ingested.

<img width="678" height="298" alt="image" src="https://github.com/user-attachments/assets/c26cc5f9-2fe2-429f-b4e1-bf7782879d73" />

### Implement Logstash Stdin Input Plug-In
- Run the following to start logstash with a config: `logstash -e  'input { stdin {}} output { stdout {}}'
- The common fields that Logstash generates during processing and output in JSON format are as follows:
  - **hostname**: The logstash node that processed the log.
  - **timestamp**: The time the Logstash received the log.
  - **original**: The contents of the raw log that Logstash received.
  - **message**: The resulting message after Logstash processes the raw log.

## Enriching Data with Filters
### Logstash Filters
- When CDAs receive an event in Logstash, they may find they have limited ways to use the event in an investigation or dashboard.
- For example, most network logs store the source of an event as an Internet Protocol (IP) address, but syslog stores only the hostname by default.
- By enriching syslog data to include a source IP address, CDAs can more easily correlate events.
- Logstash filter plug-ins provide the ability for analysts to enrich events before outputting the data to the Elasticsearch database.
- This process is the second-to-last process in the Logstash pipeline and prepares events for output.
- With Logstash filter plug-ins, analysts can ensure that incoming events are structured correctly and contain normalized fields, which enhances the ability of analysts to correlate events.
- These plug-ins can enrich the data by assessing conditions in the data, accessing external information, and extraction.

#### Conditional Filters
- Conditional Logstash filter plug-ins read data from the ingested data and perform actions based on predefined conditions.
<img width="1668" height="1032" alt="8801078a-b49f-4fc7-a727-b2f66e8d91ea" src="https://github.com/user-attachments/assets/b537217f-5efd-4bcc-8af9-3f6fc5d91ae0" />

#### External Information Filters
- External filters extract information from certain fields ingested and use external resources to enrich the data and populate new fields.
<img width="1668" height="994" alt="60451412-f6c1-48e9-a6e8-0735aa86ce98" src="https://github.com/user-attachments/assets/8c01ce8f-6579-4135-ad27-18ee528b0786" />

#### Extraction Filters
- Extraction filters use existing fields and data to generate new fields. This is especially useful when used to ingest unstructured data.
<img width="1668" height="1515" alt="01748afa-ad6e-4d48-8733-b46d7f9e0a35" src="https://github.com/user-attachments/assets/9248bc31-2e0b-4ab7-8220-9349b39de3ef" />

<img width="740" height="300" alt="image" src="https://github.com/user-attachments/assets/b7caa67e-7e9f-416f-b3b8-9a2bb9e7ec7e" />

### Logstash Performance and Data Normalization
- When data from different sources is ingested into Logstash, analysts may find it difficult to correlate events because the data is non-standard.
- Normalization, the act of organizing data from disparate sources into standardized fields, is the solution to this problem.
- Normalization enables analysts to create shared fields across data from various sources to enhance event correlation.
- Using shared fields, analysts can **create accurate visualizations and metrics** from different sources.
- Useful fields that are commonly included in data normalization are **timestamps**, Unique Identifiers (**UID**), and **source devices**.

### Logstash Performance
#### Filter Performance
- To improve performance of The Elastic Stack, analysts may implement Logstash filters to filter out fields that are not important.
- Filtering out fields that are redundant or that offer little benefit to investigations can improve the performance of Elasticsearch indexing and decrease the amount of time Elasticsearch requires to read and write the event.
- This can also improve performance of Kibana because the event has less information to read when generating visualizations.

- Another way to improve Logstash performance is by using the **Throttle** filter.
- This filter may be used to store a limited number of high-frequency events over a time period.
- Analysts can specify a **time period** and the **lower and upper limits** of the events.
- When the first event comes in, Logstash identifies that the time period has started and begins tracking the event.
- Once the lower-limit threshold is met, the events are stored.
- If the upper limit is met before the time period is complete, the Throttle filter drops the extra events until the process begins again.
<img width="1668" height="1538" alt="f531342f-6466-49fc-b26c-15fb83a72f50" src="https://github.com/user-attachments/assets/cc2c07ed-ed3d-4a85-bdbf-56474f79900f" />

#### Logstash Config Performance
- The final way to improve the performance of the Logstash pipeline is by modifying the Logstash configuration files.
- Although Logstash defaults are designed to prioritize speed and performance, some applications may require tuning, depending on the environment.
- If events are backed up but the Central Processing Unit (**CPU**) is not running at capacity, the `pipeline.workers` setting can be used to **tune the number of threads** used by Logstash.
- It may be beneficial to increase the number of workers past the number of threads in the CPU, as some threads can maintain a “waiting on event” status.
- The `pipeline.batch.size` setting **defines the maximum number of events processed by an individual worker thread**.
- A higher-number batch size is usually more efficient but may have high memory requirements to function properly.
- If the batch size is too high relative to the resources available, pipelines may crash and out-of-memory exceptions may occur, resulting in lost log events and blind spots for analysts.
- Finally, the **latency of the Logstash pipeline** can be tuned using the `pipeline.batch.delay` setting.
- The latency defines the amount of time a Logstash worker thread waits for additional messages after receiving an initial event.
- Once the time period expires, filters are executed against the event.

### Implementing Logstash Filters
- Although Logstash filters can greatly improve the quality of event collection, analysts may maximize the benefit by combining multiple filters.
- The filters that should be combined are dependent on the effect analysts would like to have on event data.
- If data is unstructured, it is usually preferable to first structure the data to simplify use of other filters.
- After structuring data, analysts should apply filters that remove fields (usually conditional filters) first to improve overall performance.
- Applying fields that can normalize data and add new fields — usually extraction filters — can then be used.
- Finally, analysts can improve and enrich ingested data using external information filters. 

### Dissect, Drop, and Mutate Filters
- The **Dissect** filter plug-in is excellent for **parsing unstructured data based on delimiters**.
- Common examples of datasets that use delimiters are Comma-Separated Values (**CSV**) or Tab-Separated Values (**TSV**).
- The Dissect filter is best used when the **delimited data reliably follows a repeatable pattern line by line**. 

- As an example, Zeek can log data in a TSV format, and each line in the log follows the same pattern of available fields, such as timestamp, UID, and source IP.
- Other log types may change entirely line by line, such as the information saved to /var/log/messages, where log formats change based on the process that generates the log or based on event severity.

- The **Drop** filter may be used to **ignore unwanted logs** so they are not forwarded for additional processing.
- Proper implementation of Drop filters can **aid performance for Logstash and prune data before it reaches a final destination**, such as a SIEM.


## Pipelines
### Logstash Pipeline Flow
- The Logstash pipeline has three stages: inputs, filters, and outputs.
- Inputs generate events that can be modified by filters and forwarded to another location by outputs.
- Inputs and filters were discussed in detail in previous lessons; they give analysts the ability to ingest a wide variety of event sources and normalize or enrich that data.
- At the end of the pipeline, output plug-ins can store the data in a variety of formats, which are discussed later in this lesson.
- After events have been processed by the Logstash pipeline, they can be accessed by analysts to identify anomalous activity and investigate Malicious Cyber Activity (MCA).

#### Inputs
- Logstash plug-ins accept and ingest data from disparate sources into The Elastic Stack and generate events.
- Each input plug-in is designed for a specific event source and can be combined to meet the unique needs of any network architecture, regardless of the systems and network segments.
- Table 16.3-1 provides a list of some common Logstash input plug-ins.
<img width="1668" height="1469" alt="f8fd6f25-dff0-4e35-ae34-f09bc3f4cf8c" src="https://github.com/user-attachments/assets/88d076a2-76b8-4251-b4fb-88ed87c243f5" />

#### Filters
- With Logstash filter plug-ins, analysts can ensure that incoming events are structured correctly and contain normalized fields, which enhances the ability of analysts to correlate events.
- These plug-ins can enrich the data by assessing conditions in the data, accessing external information, and extraction.
- Filters can also be used to increase performance of the Logstash pipeline. Table 16.3-2 provides a list of some common Logstash filter plug-ins.
<img width="1999" height="1453" alt="a55bb3d7-e705-4332-a62f-bed937c4cc6b" src="https://github.com/user-attachments/assets/2fc53f2c-c757-43f8-b37e-d40b712b039b" />

#### Outputs
- Once events are ingested and normalized using inputs and filters, they are forwarded to a log repository.
- Events can be forwarded to a variety of outputs, which enables analysts to analyze data over several tools.
- Analysts can also choose to configure multiple outputs for the same dataset to enable analysis by multiple inspection platforms, which can help analysts identify activity that may have been missed in a single environment.
- If multiple inspection platforms are used, it is important to normalize and categorize the data to make the events readable by the multiple platforms.
- The most common output plug-in is Elasticsearch, which analysts can use to forward logs for use in Kibana. Table 16.3-3 provides a list of common Logstash output plug-ins. 
<img width="1999" height="807" alt="413c95a4-551c-4d51-992d-5d215fb924fe" src="https://github.com/user-attachments/assets/0a1d33f7-b72d-4f24-9302-24b6d8f5134f" />

### Output to Elastic Indexes
- Data in Elasticsearch is organized using indexes that organize events into a common database.
- Data should be normalized based on the destination index, and different indexes can have different types of events.
- Data is loaded into Kibana on an index-by-index basis, which leads to dashboards and visualizations being limited to one index.
- When similar data is collected from different tools, it is common practice to ingest the data into different indexes.
- An example of this is to ingest data from Zeek into a Zeek index and data from Suricata into a Suricata index, despite the similarities of the data.

- The easiest way to forward logs to different indexes is to use multiple pipelines for each input to be separated.
- However, this is not always possible, due to mission requirements or resources available.
- To forward events to different indexes on the same pipeline, analysts can configure tags in the pipeline configuration file, which can then be output to different indexes.
- Using this technique, analysts can have granular control over which index to output events to in order to enhance analysis.

### Data Retention
- Data retention requirements can be a complex issue. Some data may have to be retained for a certain length of time by legal, regulatory, or mission requirements.
- This can conflict with storage resources available, as data can quickly pile up when ingesting much event data.
- At the same time, analysts want to retain data for as long as possible in order to enhance investigations and identify anomalous activity.
- Balancing these considerations requires compromise where available and may often require separate data retention solutions. These decisions should be documented in a data retention policy.

#### Data Retention Solutions
##### Hot Storage
- Hot storage solutions are designed for **data that must be quickly processed and searched**.
- Typically, a hot storage solution stores the most recent and relevant data with the time frame dictated by the mission partner and system resources.
- Server nodes hosting data in the hot storage tier generally require additional compute resources such as increased Central Processing Unit (CPU), Random Access Memory (RAM), and fast disk arrays to meet performance needs.
- An example of a common policy is to store 1 month of events in a hot storage solution.


##### Warm Storage
- Warm storage solutions are used to **store data that is queried less frequently** than recently indexed data kept in hot storage.
- Nodes hosting data in the warm storage tier have lower performance requirements than hot storage nodes. Therefore, this hardware is generally less expensive.
- This solution provides organizations with an opportunity to store data over longer periods and reduce cost associated with high-performance compute nodes. 

##### Cold Storage
- Under the cold storage solution, **older data is retained** in an **inspection platform** that is still **searchable** but is considerably **slower than hot or warm tiers**.
- This solution prioritizes bulk storage of data over search performance.
- The data stored in cold storage generally includes log and event data that is not needed for operations but must be maintained to meet legal or regulatory requirements.
- The mission partner or organization should make a determination for the length of time that different sources should be stored in cold storage.

##### Data Archiving
- When the mission partner or organization has determined that data is no longer useful but must still be kept available to meet legal or regulatory requirements, the organization can use a data-archiving solution.
- This data may be stored in various ways, ranging from cold storage to physical records, and can be stored on or off premises.
- The data retention time should be defined by regulatory and legal requirements

##### Rolling Data
- The way the data retention policy is followed is dependent on the platform on which logs are stored.
- Some log platforms can be configured to automatically roll over data once it reaches a certain age, but other platforms may require a custom solution.
- To roll over data in Elasticsearch, analysts can configure Rollup, which consolidates older data into summaries that can be stored in cold storage.
- Custom scripts can also be scheduled to query Elasticsearch for events older than a specified age to be sent to a new index stored in a different location.

<img width="770" height="334" alt="image" src="https://github.com/user-attachments/assets/bfe573a2-c604-4092-b30a-6d839972d4f5" />

### Using Logstash Pipelines
1. Configure the input plug-in to read data from multiple log sources (Zeek conn.log, Zeek dns.log, and Suricata eve.json).
2. Remove duplicate data, such as the original event and message fields.
3. Normalize Internet Protocol (IP) and port fields from each log source to match the destination field name. Use if statements to normalize data based on log source.
  - Zeek
    - `id.orig_h => SrcIP`
    - `id.orig_p => SrcPort`
    - `id.resp_h => DstIP`
    - `id.resp_p => DstPort`
  - Suricata
    - `src_ip => SrcIP`
    - `src_port => SrcPort`
    - `dst_ip => DstIP`
    - `dst_port => DstPort`
4. Output the log data to an Elasticsearch instance.
5. Ensure that the original log timestamps are preserved and used by Elasticsearch.
6. Modify the default Elasticsearch retention policy for logs such that they are moved from hot to warm storage after 30 days and from warm to cold storage after 60 days.


# MOD 17
## Data Transformation
### Data Streams and Transformations
- A data stream is defined as the flow of data between intercommunicating systems.
- Data streams are therefore the way in which data flows into various (SIEM) applications, regardless of how that data is ingested.
- As an example, The Elastic Stack uses Logstash pipelines as its solution to create data streams to forward data to Elastic instances.
- The location of data streams must be applied such that inspection tools can gather a clear understanding of the network environment and the communications that occur.
- Each SIEM application generally has its own format of creating data streams, and each application requires its own configurations to modify the data stream to support data parsing.

### Data Streams for SIEM Platforms
- Data streams must be adjusted to meet the needs of the SIEM platforms in use.
- If Splunk forwarders are in use for a Splunk instance but an Elastic instance also exists, data may be forwarded to both locations and used within both SIEM platforms.
- In this lesson, Splunk and Kafka solutions are illustrated to generate data streams.
- However, creating a data stream of raw data is not very beneficial to the SIEM systems.
- Data within the data streams may be unusable, irrelevant, or difficult to read for analysis purposes.
- Transformations must be applied to alter the data for use within the SIEM platform.
- The following discusses how to apply data transformations within Splunk forwarder and Kafka Stream solutions.

### Spunk Transformations
- Configuring the data stream is as simple as configuring any Splunk forwarder, as covered in previous lessons.
- However, to perform transformations, filters must be applied to the data coming from the source.
- The following focuses on the details of adding a transformation within different Splunk forwarder instances.
- The first part shows the commands that apply only to Splunk instances that use the Splunk heavy forwarder.
- The Splunk universal forwarder is severely restricted in its capabilities to transform data, but some simple commands, briefly covered in the second part, allow for basic transformations as well.

#### Splunk Heavy Forwarder Transformation
- The `props.conf` file within the Splunk heavy forwarder is responsible for **configuring data properties**.
- The file exists in the `…/etc/system/local/` folder of the Splunk instance where the **heavy** forwarder is implemented.
  - This file is responsible for **deploying the commands programmed within** the `transforms.conf` file.
![61ce3729-3dc5-488f-b575-5197126c54c1](https://github.com/user-attachments/assets/0831eee8-a923-4b90-b9db-b74e4355dfcf)

- Within the` props.conf` file, code is used to specify the commands that transform the data and the order of operations of those commands.
- The following code is responsible for the order of operations for the transformations, specifically stating that a command will be used to drop events (**setnull**) and a command will be used to parse events (**setparsing**).
- The **parsing** of events here may also be referred to as **transforming** the events.
  - `[source::/var/log/messages]`
  - `TRANSFORMS-set= setnull,setparsing`
- The order of the commands in the `props.conf` file is extremely important.
- The order of operations is **reversed** in how the actions are executed.
- For example, in the syntax above, the **setnull** command **runs after** the **setparsing** command even though the setnull command is first in the list.
- If the order of the syntax were reversed, as shown below, setnull would drop all events before the setparsing command ran.
  - `[source::/var/log/messages]`
  - `TRANSFORMS-set= setparsing,setnull`
- Once additions have been made to the props.conf file, the file must be saved and closed.
- For any newly added commands in the props.conf file to take effect, the `transforms.conf` file must be edited to define the actions within the commands specified in the props.conf file.

![6bfe2e07-2302-4fc4-b026-83f5ed8ceedf](https://github.com/user-attachments/assets/64688f15-db5e-4bd6-8d14-fd93156c2adc)

- The `transforms.conf` file allows for code that specifies what actions to take to transform the data.
- The transforms.conf file also exists within the `…/etc/system/local/` subfolder of the Splunk heavy forwarder deployment.
- In following with the previous code in the `props.conf` file, the following code blocks illustrate the setnull and setparsing commands.
- The code for setnull is responsible for matching all data with a simple Regular Expression (RegEx) and sending the data to the nullQueue, which drops it.
- The code for setparsing is shown to use RegEx to match only events that contain the data that has Internet Protocol v4 (IPv4) addresses within the content. 
<img width="503" height="195" alt="image" src="https://github.com/user-attachments/assets/1d7553dd-fb6e-4c98-b4ac-9a556561fa1f" />

- Once the code has been added, the file must be saved and closed.
- The Splunk Enterprise instance must be restarted for any changes in the props.conf and transforms.conf files to take effect.
- All changes must be confirmed by validating that the new transformation is applied to the data visible through the Splunk User Interface (UI).
- After confirmation within the Splunk UI, the process of implementing a simple transformation within the Splunk heavy forwarder is complete.
- In most environments, hundreds of transformations are used to transform data from all available sources and provide the most relevant information.
- However, this task is typically performed by the network technician or administrator in charge of administering the Splunk system.

#### Splunk Universal Forwarder Transformations
- Transformations within the Splunk universal forwarder are limited.
- The Splunk universal forwarder is not capable of performing in-depth transformations and **does not read** the `transforms.conf` and `props.conf` files.
- However, the capabilities of the outputs and inputs allow for filtering of data based on the explicit commands **blacklist** and **whitelist**.
- The following illustrates a simple transformation to filter data as it is processed by the Splunk universal forwarder.
  - NOTE: The terms whitelist and blacklist are considered obsolete, and, in fact, the Splunk Splexicon has replaced the terms.
  - However, the Splunk commands whitelist and blacklist are still in use.
  - For consistency, this lesson continues these terms rather than the replacement terms **allowlist** and **blocklist**.

- The `inputs.conf` file exists within the Splunk universal forwarder deployment in the `…etc/system/local/` subfolder.
  - The inputs file contains all the controls for the Splunk universal forwarder that dictate which data should be processed and monitored on the host system where the universal forwarder is installed.
![06b1a46c-2dbb-408e-bbad-6b4d4696f604](https://github.com/user-attachments/assets/0dfab337-720f-4347-8617-e2b483cec3c3)

- The `outputs.conf` file also exists within the Splunk universal forwarder deployment in the `…etc/system/local subfolder.`
  - The outputs file contains all the controls for the Splunk universal forwarder regarding where data should be sent and which data should be forwarded.
![7c34b7d9-952c-479c-8b77-ca9c58e3c224](https://github.com/user-attachments/assets/8c6f79aa-30e8-4351-bb1a-7b5f8fc6577f)

- A **filter** is applied to specifically filter out which content is **monitored** or **forwarded**.
- The `inputs.conf` file is responsible for **dictating which data is monitored** on the device.
- The `outputs.conf` file is responsible for **stating which data is forwarded** to the Splunk instance.
- The **whitelist** or **blacklist** command is added to either file and functions by either not monitoring for data within the inputs file or not forwarding the data within the outputs file.
- The commands allow for data to be filtered from various sources by explicitly allowing only the stated data from a source (whitelist) or explicitly denying data from a source (blacklist).
- The following command is intended to be added to the inputs.conf file.
- The command illustrates a blacklist that uses a RegEx expression to exclude the monitoring of any files that have either a .txt extension or .gz extension when monitoring logs on the host. 
  - `[monitor:///mnt/logs]`
    - `blacklist = \.(?:txt|gz)$`

- After adding any code to either of the configuration files, the files must be saved and closed.
- The Splunk service for the universal forwarder must be restarted for the changes to take effect.
- The change examples above would prevent file monitoring for .txt and .gz files by using blacklist within the inputs.conf file. 

### Kafka Stream Transformations
- Kafka Streams are used to create streams of data within Kafka instances.
- Transformations can be applied to those streams to help normalize data as it is forwarded to the Kafka brokers, which process the data and send it to whichever UI is used to interface with the data.
- Kafka’s primary UI is called **Offset Explorer** (previously Kafka Tool), but it can be used to interface with most SIEM UIs.
- Kafka uses Java as the code for implementing Kafka and Kafka Streams.
- All Kafka Streams are defined within a Java application.
- Many different Java applications can be created and are used to perform various different data streams within the Kafka deployment.
- To add transformations to a Kafka Stream, the code for the Java file must be edited.
- Below is a small sample of the Java code for a Kafka Stream called FilterStream.
![10b38d97-738a-4ccf-a94f-2d65eaae51fe](https://github.com/user-attachments/assets/34be8677-c1f4-42e4-9765-ae254911f3ce)

- To perform transformations, code must be applied to filter the data.
- Many different transformations exist within Kafka. Kafka transformations are of two types:
  - **stateless**
    - Stateless transformations occur without knowledge of previous events in the stream.
    - These transformations are applied to every event in the stream as it passes through the application.
  - **stateful**.
    - Stateful transformations require knowledge of previous states of the data within the stream and perform actions that correlate data from previous events.
    - An example of a stateful transformation is a summation of values coming in through the data stream.
- The value of each message in the data stream must be known in order to compute the sum; therefore, knowledge of previous states of data must be known.
- Table 17.1-1 shows a breakdown of stateless transformations possible within Kafka Streams. 
<img width="1999" height="1495" alt="4d5b4b37-66e8-49d7-a4c2-a1e68160e33a" src="https://github.com/user-attachments/assets/b12178cc-f56a-4917-935b-fb47986bd954" />

## Data Normalization
### Data Normalization an dthe Common Information Model
- As discussed in previous lessons, **data normalization** is the _process of cleansing and organizing data to add value for analysis_.
  - Data normalization consists of modifying data from different sources to appear in a common format.
  - This eases readability and allows for shared data fields between events from disparate sources.
  - Additionally, normalization makes data useful, removing or cleansing unnecessary data sources and adjusting the presentation of data to provide only relevant information.
  - An open Information Technology (IT) standard was developed to define the need for normalization within IT environments: the Common Information Model (**CIM**).

#### Common Information Model
- CIM **illustrates the interrelationships of all managed objects within an IT environment** and **defines the need for the shared management** of these objects and relationships.
- The main purpose of CIM is to apply shared management to all objects within an IT environment and to all the interrelationships of these objects.
- CIM is commonly applied as a concept for management software for administering objects within an IT organization, but **managing information of the relationships of objects** is a necessity for both administrators and cybersecurity personnel.
- Figure 17.2-1 represents the implementation of CIM and illustrates the management of IT objects through IT management software and the management of data flow between the relationships of those objects accomplished by the SIEM.
<img width="1999" height="1031" alt="681ffabc-bc59-4ca7-b4f2-55b0b537fff6" src="https://github.com/user-attachments/assets/db410e71-a028-4a1c-8e13-9b69dbcf300a" />

- Managing the data of the relationships between objects leads to the use of SIEM platforms, which can be used to ingest data and apply the methodologies of CIM.
  - Applying the CIM to a SIEM platform means the data from all objects in an environment needs to appear in a common form despite being from different sources (normalization).
  - When the CIM standard is applied across SIEMs, all datasets are altered so that there is a common normalized view of events within SIEM platforms.

- Splunk has a solution to apply CIM standards: the Splunk CIM add-on, which contains a collection of data models and tools to normalize data.
  - The Splunk CIM configures multiple data models to sort data and shared field extractions for application of similar data fields across all data.
  - Applying these tools to all data sources within a Splunk instance helps to structure data for maximum efficiency.
  - Once configured, data from disparate sources should appear in a common form within the Splunk User Interface (UI).
  - Configuring Splunk CIM is as simple as running an executable. The Splunk CIM add-on is installed on the Splunk Enterprise system, and all data sources may be ingested into the normalization.

- The Elastic Stack also has a solution for normalizing data.
- Elastic Common Schema (ECS) in The Elastic Stack allows for consistent formats of data.
- ECS configuration is applied by defining ECS to function across a specific data source.
- For data to be normalized, each source must have a Command-Line Interface (CLI) statement defining the use of ECS.
- Applying ECS to an entire Elastic instance meets the standards defined by CIM.

- Other platforms do not have an easy implementation of CIM and normalizing data.
- Data transformations must be applied in a common manner to all data as it is ingested to facilitate data normalization.
- Each environment has its own needs based on the systems and applications in use, and the data from each system/application must be filtered to output similar data fields.
- Using these common transformations to normalize data from all sources facilitates the standard defined by the CIM. This applies to most SIEM tools, including Kafka.

<img width="630" height="270" alt="image" src="https://github.com/user-attachments/assets/774fac25-6086-4599-b3e3-99618c927554" />

<img width="726" height="273" alt="image" src="https://github.com/user-attachments/assets/819d4d6f-d848-41e0-b19a-d401d0eb5784" />

### Splunk Normalization with CIM
1. Go to SplunkBase, a Splunk site comprising add-on applications, and download the Splunk CIM add-on file to a workstation that can access the Splunk web interface.
2. In the Splunk web interface, select the gear icon next to the Apps menu: 
<img width="267" height="210" alt="b45b1477-b398-46d9-bd29-f6ba891a64da" src="https://github.com/user-attachments/assets/8a9b10b6-f9d1-49db-80d7-b3d5bb987d3e" />
  - An Install app from file option appears; select the option:
<img width="417" height="46" alt="4ef968aa-0fa6-44eb-b0d5-d9a598a37cfa" src="https://github.com/user-attachments/assets/a7f1ff3d-7bcb-491f-9ee6-4c12b5d5d01b" />

3. Select the Splunk CIM add-on file that was downloaded from SplunkBase, and install the application. 
  - Upon restarting Splunk, the Splunk CIM add-on is installed.
  - The application appears in the list of apps and add-ons called Splunk Common Information Model on the Splunk server at `$SPLUNK_HOME/etc/apps/Splunk_SA_CIM.`
  - After installation, configurations must be applied for Splunk CIM to have an effect on the data.
  - CIM add-on configuration pages are located in the Splunk web interface; go to `Select Apps > Manage Apps`, and then select Set Up in the Splunk Common Information Model add-on row. 

- Numerous data models are located on the CIM setup page.
- The data models are part of the Splunk CIM application and contain a large series of common events and shared field extractions to apply to various data sources.
- In application, any data models intended for use must be modified. Each environment requires different data models to be configured.
- For example, if multiple Intrusion Detection Systems (IDS) exist in an environment, the various signatures may be configured to report in the Intrusion Detection data model and create shared fields among all IDSs in use.
<img width="361" height="598" alt="41082bf4-f7b1-4e0f-a311-440b5ba4c78f" src="https://github.com/user-attachments/assets/2b4d77a5-5803-41a7-bff6-8a38efe6790a" />

- By default, the data models search for events within all indexes listed.
- However, if the needs of the environment dictate that only certain indexes should be used in certain CIM data models, the Indexes whitelist can be used. (The Indexes whitelist may also be called the Indexes allowlist.)
  - The Indexes whitelist allows indexes to be specified to apply per CIM data model.
  - Leaving this as the default is common practice. After data models are catered to a network environment, the CIM setup may be saved.
<img width="560" height="71" alt="0753c18c-4278-46b2-891c-5f5ecceb0da8" src="https://github.com/user-attachments/assets/a4d05744-1a78-4a54-850b-1a65a2ba1b36" />

- For data models to work, event types must be created and tagged to configure the source type to populate within configured CIM data models.
- The setting for event types is found by selecting Event Types on the Splunk Settings drop-down menu: 
<img width="206" height="176" alt="8d6f5032-eef0-4b7e-bafe-e57026c1c0a4" src="https://github.com/user-attachments/assets/2f23ef95-11a5-4752-86d1-4a9795a59fbe" />

- The New button allows for creation of a new event type.
- In the same menu, a destination application, name (arbitrary), source type (search string), and tag must be identified.
- Each data model has its own tag requirements.
- Entering search strings that match the data events to be returned and applying a tag that matches a specific data model must be done and saved to create a pivot to normalize the data.
<img width="1649" height="763" alt="f07bb695-f709-4eaa-bd26-ea05e12fe2fb" src="https://github.com/user-attachments/assets/5c641597-aa29-4339-b0bf-8049906b1fc4" />

- Finally, a **field alias** must be generated for the fields from the source type that need to match correlating fields within the CIM data model.
- The Fields link is located in the Settings drop-down menu:
<img width="197" height="219" alt="8206be17-7c8c-4a90-b5b2-ffb8c74a0b97" src="https://github.com/user-attachments/assets/d418193e-559c-4ba7-bc0e-5ef871bf2744" />

- Field aliases are specified within the Fields menu.
- To add a field alias, select the Add New button.
- Within any field alias, a destination application, name (arbitrary), source type, and field-to-alias statement need to be specified.
- The alias is intended to assign a coexisting name to a field.
- When applying this concept to the CIM, the aliases specified should match fields within the CIM data model.
- After specifying all details of the aliases required, select Save.
<img width="1777" height="857" alt="9453cb87-e4eb-42f4-8fe1-2094cc8b7f50" src="https://github.com/user-attachments/assets/a7cfee0b-0cd9-47ef-bc8e-ab12aea89d5c" />

- To validate that the data has been added to the CIM data model, find the data model edited through Data models in the Splunk Settings drop-down menu.
- Selecting the Pivot button on the edited data model displays the internal objects of the data model.
- Selecting any object displays all the events associated with that object, and fields may be searched.
- If the field alias assigned in the environment is functional, the events from the edited source type are visible. 
<img width="2001" height="448" alt="6a57b613-ffcc-4c92-9b8d-14962dce511c" src="https://github.com/user-attachments/assets/c4adb1e5-2d11-4ed4-a777-0dde354c9843" />

- The above steps facilitate a walk-through of all the requirements to install and configure Splunk CIM.
- Data from the sources specified are processed through the CIM add-on, and normalization techniques are automatically applied.

### Kafka Stream Normalization with CIM
- Kafka is a more complex method of applying normalization than Splunk.
- Transformations for data sources must be applied to reach the CIM standards.
- In the last lesson, Kafka Stream transformations were applied to modify the data being ingested.
- The same process must be used to achieve normalization, but transformations must be applied in a meaningful way that adds value to the data and creates a common display of the events from all data sources.
- The steps that follow illustrate a simple normalization achieved in Kafka by modifying multiple sources to have a shared theme among the data sent to the analysis platform.

1. Edit the Java application for the Kafka Stream. Figure 17.2-11 shows the FilterStream application from the prior lesson Data Transformation: 
![f350da3d-9b9b-45f4-889d-81297e28d6fc](https://github.com/user-attachments/assets/1b299e4b-76dd-40d6-a9fd-14c48bb9a724)

2. Add a transformation to a specific data source, using the same transformation from the lesson Data Transformation:
   <img width="690" height="75" alt="image" src="https://github.com/user-attachments/assets/f6864ca9-93c6-40dc-857b-c4749664f6e9" />

3. Repeat this transformation for all data sources to normalize messages from each source. Each data source may have different requirements and may need subtle tuning to meet the desired outcome of the filter. For example, the transformation in Step 2 limits messages to only those greater than 1,000 characters. If all messages from a specific data source were under that limit, the transformation would be useless. The transformation should be altered for each data source to find the minimum length of the message at default and only allow greater values. 
4. Save and compile all Kafka Stream files that have the transformation applied.
5. Apply the above steps to all Kafka Streams within the environments to encompass all data sources. This ensures that normalization is achieved from all sources. 
6. Verify that the data is being transformed by reviewing data in the analysis platform in use.
  - This process achieves normalization within the Kafka Stream. In the example above, only one application was running to initiate the Kafka Stream.
  - However, this process can become intensive when a large number of data sources is present and multiple Kafka Stream applications are in an environment.
  - To achieve normalization, a multitude of Kafka Stream transformations must be implemented.
  - Table 17.2-1 describes the Kafka Stream transformations mentioned in the last lesson, focusing on how to implement these transformations for normalization and improvement of data efficiency.
<img width="1999" height="1495" alt="d1dff2fb-885f-41c6-ac69-cd8148fe52d9" src="https://github.com/user-attachments/assets/cc872c5a-3cc0-40c2-8350-1f3cb5c07088" />

#### Branch
- The Branch Kafka Stream transformation is capable of **dividing a data stream** into multiple streams of data.
- The results of applying this transformation is a split of the data as it is ingested. Then data is forwarded to two or more inspection platforms.
- This can be used to create elasticity and redundancy within a Kafka implementation in which multiple brokers are in use.
- For example, if the data is being sent to only one destination and that destination is offline, all data from the source becomes unreachable.
- However, if two or more destinations are in use, the architecture allows for a system to be offline and data to still be accessible.
- Another solution is to use the Branch transformation to send data to multiple platforms in a range.
- If the architecture of the environment has Splunk, The Elastic Stack, and ROCK NSM, Kafka Streams can be branched to forward the data stream to all SIEM instances.

#### Filter
- The Filter Kafka Stream transformation provides the capability to restrict data that is sent to inspection platforms.
- Applying this transformation results in a variety of different outcomes, depending on the code used to filter the data.
- In its simplest form, Filter searches for a specific value within the data stream and, when a match is found, performs an operation to either block that data from the stream or allow only the specified data through.
- Filter is essential for increasing the performance and bandwidth usage of Kafka Streams.

#### flatMap
- The flatMap Kafka Stream transformation provides the capability to alter records as they are ingested.
- Applying this transformation results in searching for a specific value within the data stream and performing a function over that data.
- For example, if the only important portion of the data is a list of addresses, the data can be transformed into just a list of the addresses, and the new record replaces the original message to be forwarded to the inspection platforms.
- flatMap is capable of dividing single records into multiple new records or dropping the original record if no match is found.
- flatMap is essential to normalizing data from the data stream in that it can alter the messages so only relevant data is found, and all data appears in a common format. 

#### Map
- The Map Kafka Stream transformation is similar to flatMap in that Map also provides the capability to alter records as they are ingested.
- However, Map cannot divide single records into multiple new records.
- Map is capable of performing only a one-to-one record manipulation.
- This means that each record present within the data stream always produces one new record to be forwarded to the inspection platforms.
- Map, like flatMap, is essential to normalizing data from the data stream.
- Whether applying various transformations in Kafka or configuring the CIM in Splunk, normalization should always be implemented in the network environment.
- The methods above illustrate some of the tools available to help achieve normalization.

<img width="781" height="299" alt="image" src="https://github.com/user-attachments/assets/f3499214-a757-4fe3-b9cd-46485c6401b5" />
<img width="727" height="323" alt="image" src="https://github.com/user-attachments/assets/4f12b1c5-97d2-47c0-a835-e711ee3c71de" />

<img width="767" height="319" alt="image" src="https://github.com/user-attachments/assets/3d199da5-bb8f-49a1-aa38-a6061e4c4fdc" />

## SIEM Architectures
### What Makes a SIEM?
- SIEMS are **systems**, **software**, or a **collection of services** that **incorporate security information management** and **security event management**.
- The goal of a SIEM is to provide the ability to analyze an organization’s data to understand and respond to security-related occurrences in the organization’s Information Technology (IT) environment.
- All SIEMs share a common set of functions that are integral to the SIEM architecture.
- The following is a breakdown of the components of a SIEM:
  - Forwarding/Sensors: Data ingestion for an environment.
    - These are responsible for gathering and sending data to the SIEM aggregator or indexer.
  - Indexing/Parsing: Organization of data for use in the SIEM platform.
    - This can be a dedicated system, integrated with the SIEM engine or integrated with the aggregation system.
  - Aggregation: Repository of data for review in the SIEM tools.
    - Data is stored and organized in this system, based on source, age, and other sorting rules dictated by the architect.
    - This may be a dedicated device or integrated with the primary SIEM engine.
  - SIEM Engine/Searching: Presentation of data to the interface.
    - The SIEM engine provides all the capability to perform searches on the indexed and aggregated data.
  - User Interface (UI): Interaction with the data for transformation and visualization.
    - This is done using a dedicated processing language (for example, Splunk Processing Language [SPL] or Kibana Query Language [KQL]).
  - Additional Services: Many SIEM tools include additional services, such as user-based analytics, machine learning/(AI), Integrated (CTI), and alerting.
    - These systems are installed on top of the SIEM engine and enhance analysis capabilities.

#### Splunk Architecture
- Each SIEM platform has its own way of labeling its architecture and how data flows in the environment.
- Figure 17.3-1 provides a breakdown of the architecture for Splunk Enterprise:
<img width="1999" height="988" alt="ee9769a5-fd7c-45be-b338-c6e3f0a0a699" src="https://github.com/user-attachments/assets/85fbd286-229d-42f0-8e69-ff898f593566" />

- Each portion of the Splunk architecture in Figure 17.3-1 is described below:
  - The Splunk **forwarder** is responsible for collecting and forwarding data to the Splunk indexer.
    - The forwarder is also capable of performing transformations on the data.
  - The Splunk **indexer** is responsible for sorting, processing, and balancing data as it is ingested.
    - The indexer can also perform transformations on the data.
  - The Splunk **engine** contains the search head and all Splunk services.
    - The Splunk engine is responsible for executing the query commands and retrieving the data with the search head as well as creating alerts and reports from data.
    - Other services may also run on the Splunk Engine, as dictated by operations.
  - The Splunk **UI** provides access to the Splunk engine.
    - The UI is primarily accessed through the Splunk web interface.

<img width="1999" height="1000" alt="698a927d-d171-453d-8431-a9a345ca221e" src="https://github.com/user-attachments/assets/f2c89a63-e8d5-4038-9a9f-86be36387ab2" />

- Each portion of The Elastic Stack architecture in Figure 17.3-2 is described below:
  - Multiple log collection solutions may be used in The Elastic Stack, but a primary tool is Beats.
    - This portion of the architecture dictates which data is ingested into the Logstash pipeline and is responsible for collecting and forwarding data to the Logstash solution.
    - Data may also be sent directly to The Elastic Stack engine if no transformations are necessary.
  - Logstash is responsible for ingesting and processing data from the log collection sources.
    - Data processing and transformations are handled by the Logstash solution, and the data is forwarded to The Elastic Stack engine.
    - Logstash may also be used to collect logs from a source.
  - The Elastic Stack engine performs the storage and organization of data as it is received.
    - The stored data is made accessible to querying applications like Kibana and also provides additional services within The Elastic Stack, as dictated by operations.
  - Kibana provides access to the data within The Elastic Stack engine. The UI is primarily accessed through the web interface.

### Configuring SIEMs
- Configuring SIEMs is dependent on the platform in use. The installers and licensing for the particular SIEM intended to be used are required for any setup and configuration, but the tasks to implement each SIEM platform share a common theme. The tasks, in order from first to last, are as follows:
1. Install the primary SIEM instance (for example, The Elastic Stack or Splunk Enterprise).
2. Configure the primary SIEM instance. Multiple instances or separate log repositories intended for the architecture should be installed and configured after this step.
3. Install data ingestion methods.
4. Configure data ingestion methods.
5. Validate data ingestion.
6. Configure data through the SIEM interface.
7. Validate data and visibility for all intended data sources.
8. Configure normalization.

#### Data Collection and Ingestion
<img width="1999" height="264" alt="88bafbc2-10e7-4698-b820-bdf9a30f3220" src="https://github.com/user-attachments/assets/2d2e341e-b953-456c-928b-34cb7e0acdca" />

- SIEMs need data to operate, so data collection and forwarding solutions must be installed throughout the network environment to send data to the SIEM.
- As stated before, Splunk forwarders must be placed in such a manner to gather a clear picture of network environment activity from all network segments within a Splunk environment.
- Installing forwarders has been covered thoroughly in prior Cyber Defense Analyst – Network lessons.
- The configurations learned throughout the previous lessons fulfill the requirements of configuring the data collection and forwarding portion of Splunk architecture.

#### Data Aggregation
- Data storage must be decided for a SIEM environment.
- The dedicated device for a SIEM instance, or another dedicated device for data storage, may be used as the storage device.
- This system is often a repository of logs and other information.
- In a default installation of Splunk Enterprise, the repository is assigned to the same device in which Splunk Enterprise was installed.
- Configuring the data repository is as simple as installing Splunk Enterprise.

#### Data Organization
- At multiple points in the pipeline, data can be organized. Data may be filtered and transformed with forwarders, as previously noted in lessons, but data is also sorted and organized in the indexing solution.
  - In Splunk environments, the Splunk indexer performs these operations to sort data into buckets and performs any preconfigured transformations to the data.
  - According to the Splunk Splexicon, a bucket is “a file system directory containing a portion of a Splunk Enterprise index,” and a Splunk Enterprise index “typically consists of many buckets, organized by age.”

- Various events may occur in the Splunk indexer.
  - One event that must occur is the assignment of data to hot, warm, cold, or frozen buckets.
    - These buckets are responsible for sorting data based on ease of access.
    - Data that is new and important to operations is placed in **hot** buckets, which are the fastest to return search results and process commands.
    - **Warm** buckets are also fast but are generally used for older data that is less relevant.
    - **Cold** buckets are used for data that is much older and not used for operations as frequently.
    - **Frozen** buckets are for data that no longer needs access for operations and by default is deleted (although this can be configured to send data to an archive).
    - The indexer makes these decisions based on configurations that are set to meet the needs of operations.
    - Additionally, the indexer is capable of sorting data and performing transformations to the data to help normalize data before reaching the Splunk search head.

- In the default installation of Splunk Enterprise, the indexer is installed at the same time as the other Splunk services and exists on the same device as the Splunk search head.
- Configuring the default indexer is as simple as installing Splunk Enterprise.
- However, adding transformations and other functions to the indexer requires additional steps.
- By default, all data is sent to one index called main.
- This can be difficult to search through when data from many sources is ingested.
- Adding indexes is done by logging in to the Splunk web interface and selecting Indexes in the menu, selecting the Add New button, and changing data from various sources to the new index.

#### SIEM Instance
- Splunk can have a dedicated server or be installed onto pre-existing servers.
- In most instances, a dedicated device (or multiple dedicated devices) is used for deploying the primary environment of Splunk.
- The dedicated device is usually a server and is generally referred to as the Splunk server or Splunk Enterprise server.
- This server should host all the SIEM software that manages the UI and facilitates the handling of data.
- In the Splunk server, the primary service for interfacing is called the search head.
- The search head is responsible for taking any SPL commands sent from the web interface, processing the commands, and returning any data that matches the command criteria.
- When accessing the Splunk instance from the web interface, the actual connection is to the search head.
- The search head parses through all indexed data available within the Splunk Indexer.
- The default installation of Splunk Enterprise is all that is needed for using the Splunk search head and Splunk web interface.
- Some variables may need to be changed, such as the port and Uniform Resource Location (URL) of the web interface.
- These can all be configured through the settings of the Splunk web interface.
- Other configurations that should be set up are the permissions and user accounts.
- Administrator privileges should be restricted, and most users interfacing with data should be assigned to User or Power User accounts.
- Outside the various architecture component installations, additional work is needed to configure, validate, and normalize data.
- This may be conducted in the same manner as in the previous two lessons. 

<img width="699" height="480" alt="image" src="https://github.com/user-attachments/assets/f36793be-1c94-4645-8117-3324e26178db" />


### Troubleshooting SIEM Environments
- Identifying faults in SIEM environments can help to diagnose issues that may impact data analysis.
- Many SIEMs provide alerts for egregious faults within the system functionality.
- These are usually sent as system messages and indicate that a portion of the SIEM architecture is not functioning.
- An example is a Splunk indexer that has crashed.
- The Splunk web interface provides an alert at the top of the screen next to the account in use, as shown below:
![f56cba06-ce7f-4438-ae0a-e552e980ae52](https://github.com/user-attachments/assets/1f61e371-4f36-4bff-9d61-ae5ab3a07274)

- Hovering over the caution icon displays the alert, which, in this example, would say Splunk Indexer is offline.
- For Splunk to function again, the indexer must be brought back online. Outside this example, many different errors can be found through this system.
- Other platforms have similar notification systems for errors. However, these systems do not identify all faults within SIEM platforms.
- The following are other areas where less easily detected faults can be identified within SIEM platforms.

#### Health Checks
- Each SIEM platform has its own requirements for checking the health of the system.
- Many SIEM platforms have dashboards to help alert users of issues with the health of their system.
- Splunk has a system that can be used to check the health of Splunk Enterprise systems:
  - The Splunk Health Check system allows for the performance of health checks of the various components and services of the Splunk Enterprise system.
  - Running the health check illustrates any known faults or issues present within the environment.
  - This may also include recommendations to improve the efficiency of the Splunk Enterprise services.
  - This can be conducted on demand and is an effective tool for identifying faults within the Splunk environment.

#### Data Loss or Redundancy
- Data loss and redundancy are also important to monitor for the state of a system.
- Such monitoring can be conducted by reviewing the sources of data through the SIEM interface.
- One method to identify data loss or redundancy in Splunk Enterprise systems is to review data sources for repeat instances.
- The Data Summary, which includes all data sources, is linked from the Splunk Search and Reporting app page.
<img width="800" height="257" alt="efc8109a-7cb4-4ba0-a4e5-b8cb39848df6" src="https://github.com/user-attachments/assets/989a45c9-075f-4912-a132-3c69aea4cbe8" />

- The Data Summary in Figure 17.3-9 shows the issue of multiple data sources.
- The issue occurs due to errors or misconfigurations with the Splunk forwarders.
- Restarting the Splunk forwarder may fix the issue. However, sometimes transformations or ingestion declarations occur multiple times in the inputs.conf file.
- These errors must be rectified and saved within the Splunk forwarder configuration, resulting in removing the redundant data source.

#### Data Quality and System erformance
- Identifying problems in data quality or system performance is a complex issue.
- This can be the result of poor configurations throughout the architecture.
- Splunk features a dashboard to review the quality of data from all sources.
<img width="2001" height="419" alt="21064402-8e7d-4d36-8881-334c26915fda" src="https://github.com/user-attachments/assets/2556f5fc-f690-4155-9c06-1d25674e52a6" />

- The Data Quality dashboard allows analysts to select which indexers and instances to review as well as a time range.
- Reviewing all indexes can illustrate which indexers are properly sending data to the system.
- The Data Quality dashboard also provides insight into possible indexer issues or data sources that are providing irrelevant or redundant information.
- The insight from the Data Quality dashboard can be used to create new filters and transformations on the Splunk forwarder configurations or to create additional filtering and transformations on the Splunk indexer.
- System performance is another major factor in capturing and analyzing data effectively and efficiently.
- If the system is performing inadequately, analysis can be hindered, or even impossible, as queries on the data may never complete.
- Splunk includes dashboards for monitoring the performance of the Splunk system under Resource Usage.
  - The Resource Usage: Instance dashboard contains a series of selections for identifying which sections of the system to inspect, including instance, group, and role.
  - The options available depend on the configuration and architecture of the Splunk environment.
  - With the All role, All group, and splunk instance selected, as illustrated in Figure 17.3-11, all components services of the Splunk system are reviewed.
  - The result of these filters is a series of metrics about data storage usage, memory usage, and Central Processing Unit (CPU) performance and usage.
  - All the metrics available on the Resource Usage: Instance dashboard can be used to identify bottlenecks in performance and resource usage.
  - In some instances, hardware requirements are the limiting factor, and remediating efforts require new or improved hardware.
<img width="2000" height="180" alt="c1d1a505-2d7e-4148-8386-9565db415224" src="https://github.com/user-attachments/assets/909d1edf-cbdf-44c4-8c14-5c2484039bac" />

<img width="754" height="305" alt="image" src="https://github.com/user-attachments/assets/fe3b57af-e9fe-486c-984c-7786ef99455d" />


## SIEM Optimization
### SIEM Data Lifecycle
- The SIEM data lifecycle shows all events that occur to the flow of data within a SIEM environment.
- Each stage of the lifecycle represents a specific function of data as it applies to the SIEM platform. 
<img width="1999" height="1472" alt="146506cc-96a0-4263-8f19-c5bfb157b8f0" src="https://github.com/user-attachments/assets/bbe8878f-4f5a-4ed1-bf6c-8159bd3e74f8" />

#### Planning
- Planning is the first phase in the SIEM data lifecycle.
- This is when a scheme is developed about where to collect data in order to gather a clear picture of the network environment.
- Planning also includes gathering and preparing the SIEM architecture.
- Additionally, after all stages of the data lifecycle, planning is reassessed. More data may be needed regularly as dictated by operations.
- The planning step is also used to implement schemes for improving the performance of the data lifecycle.

#### Collecting
- Data collection should capture all relevant information from a host or network segment to provide a clear picture of events for analysis.
- Additionally, simple exclusion transformations can be created to prevent the ingestion of specific data that may not be necessary for SIEM operations.
- For example, the Splunk forwarder configuration is responsible for identifying data to be collected through the inputs.conf file, but it is also responsible for identifying log forwarding destinations through the outputs.conf file.

#### Forwarding
- Data that is collected must be sent to a destination.
- A destination for data is selected, and the forwarding solution is responsible for shipping the data.
- Additionally, at this step in the data lifecycle, advanced normalization transformations can be implemented to alter data.
- Forwarding solutions are capable of high-level transformations to improve performance, reduce redundancy, and create a common shared format for events.

#### Aggregating 
- After being received from forwarding, data needs a place to be stored.
- The Aggregating phase of the lifecycle accomplishes the ingestion and storage of all data from the forwarding stage.
- Aggregating may occur in three forms:
  - Have data stored in a log repository that runs no services other than collecting and storing logs, which then requires another robust forwarding solution to send logs to the SIEM instance. 
  solution, such as a Splunk indexer, on the data repository so organizing occurs simultaneously with the Aggregating stage and is accessed remotely by the SIEM interface.
Build the aggregating solution into the primary SIEM instance, and host all the aggregating, organizing, analyzing, and archiving solutions within the same system.

#### Organizing
- As noted above, the Organizing phase of the data lifecycle may occur either simultaneously with aggregating or separately.
- However, organizing typically occurs on the device that hosts the primary SIEM engine for simplicity and performance.
- Organizing is the phase in which data is sorted into locations for ease of use and access.
- Data is placed into various locations within the organizing system, and priority is assigned to the data so that important data is quickest and easiest to access.
- In some instances, the Organizing phase can also be used to perform transformations on data based on the configuration of the system.
- These transformations mean that additional normalization can be achieved at this stage. 

#### Analyzing
- During the Analyzing phase of the lifecycle, data is presented to users for analysis.
- The organized data is made accessible to the user through an interface that performs queries and visualizations of the data.
- This stage is where data sits the longest for operational use.
- As data ages, it may move locations within the organizing solution as the older data becomes less prioritized for operations. 

#### Archiving
- The Archiving phase occurs once analysis no longer has use of the data.
- Data required by regulations is maintained, and all data that is no longer necessary is deleted to make room for newly captured data.
- Some organizations may dictate that no data should be deleted and all data should be archived, and some organizations may want almost no data archived and choose to delete most of the data.
- The timeline for this is also dictated by the organization and resource requirements. 

- Together, all the above phases represent the SIEM data lifecycle.
- These phases illustrate how data flows through the SIEM architecture from start to finish and then the cycle begins anew.

### Optimal SIEM Performance
#### Using Best Practices
- Best practices in optimizing SIEM performance include conducting optimal searches and limiting data requested from disks.
- System resources on the server can be consumed quickly if no time limits or field attributes are specified.
- The result can be queries that require more than 2 hours to complete in complex SIEM architectures.
- Using optimal searching techniques is one of the best methods to limit system resource use.
- As an example, in Splunk, a search using the syntax `index=main` and a time selection of `All Time` can be improved dramatically by changing the syntax to `index=main sourcetype=WinEventLog host=cda-win-hunt` and changing the time selection to `Last 24 hours`.

- A common searching technique is to include wildcards, usually indicated by an asterisk (*).
- Wildcards search for any data within a specified parameter. However, open wildcard searches are not recommended, as all events must be read to see if the field exists.
- This returns more information than is usually necessary and consumes significant system resources.
- As an example using Splunk, consider a search using the syntax `index=main source_IP=*.`
  - This search returns any fields with a value for source_IP.
  - A more efficient search might be generated using such syntax as `index=main source_IP=192.168.1.*.`

#### Performing Tuning Operations
- The following tuning operations can help optimize SIEM performance. 

##### Sorting Hot, Warm, and Cold Data
- Data must be sorted and stored in SIEM environments, usually according to age.
- Ensuring data is in proper storage solutions for hot, warm, and cold access eases system resources and allows for the most relevant data to be quickly searched.
- Each SIEM has its own system for sorting and handling the data stored and requires unique configurations.

- By default in Splunk, data is stored into one of 10 hot buckets in the main index; the buckets are automatically created through the Splunk Enterprise installation.
- Once a hot bucket reaches 750 megabytes (MB) in size, it rolls over its oldest data into a warm bucket.
- Warm buckets roll over their oldest data into cold buckets once 300 warm buckets are created.
- Cold buckets roll over to frozen buckets once the cold bucket reaches 500,000 MB in size or after 6 years.
- The default setting for frozen buckets is to delete all data.
- These limits are set by default, and all the limits can be increased or lowered to increase performance or limit resource use.
- Altering this data can result in dramatic increases in performance.
- For example, if the size of the hot buckets in a Splunk environment is being exceeded in 2 minutes, most data is stored in warm and cold buckets.
  - This means that unless data is being queried for fewer than 2 minutes, all searches must access data from the slower warm or cold buckets.
  - This can result in major delays with queries on data. Increasing the size and quantity of the hot buckets provides a larger portion of data to be searchable from the fastest storage solution.

- Additionally, data should always be stored by age.
- If the size of hot buckets is too great, older data clutters the fastest method of accessing data, resulting in slowed performance.
- A general rule for meeting ideal storage solutions is for hot buckets to store 24 hours’ worth of data, warm buckets to store 1 week’s worth of data, and cold buckets to store 1 year’s worth of data.

##### Using Built-in Tools to Optimize Data-Processing Performance
- SIEMs are constantly ingesting and storing data from multiple sources.
- Generally, data is sorted by time of indexing and then left as is.
- This can lead to population of redundant data or even misuse of data storage space.

- To remediate this, SIEMs may have built-in commands that help to optimize data storage and performance.
- Splunk’s command for this is splunk-optimize.
- This command optimizes how data is stored and improves the performance of Splunk indexers.
- This works much like defragmenting a hard drive but supports a much larger scale and higher level of efficiency.
- By default, all hot buckets in Splunk run splunk-optimize every 24 hours. The splunk-optimize command must be run manually for all other buckets.

##### Creating Custom Data Stores
- In default SIEM instances, data is stored directly to the data store solution generated by the installation.
- All data populates in the default data store, and the data store must be searched fully when calling any source of data.
- This can be an inefficient process, and creating new data stores can help to alleviate resources used to fetch data from indexes.

- In Splunk, the default storage device is an index.
- The default index is titled main.
- If left in the default state, all data from data sources populates only into the main index, causing the data to fill up the hot and warm buckets more quickly.
- This would eventually result in a large series of cold buckets and slower performance when running queries.

- To combat the performance issues with indexes, new custom indexes may be created.
- Custom indexes are generated through the Splunk web interface and generally are used to sort data from different sources.
- For example, if a group of Linux hosts and a group of Windows hosts exist, indexes can be created to store data from each and can be titled appropriately (for instance, win_events and lin_events).
- The data sources for each index must be specified after they are created.
- Once this is completed, the new index should alleviate performance issues associated with having only a single index.

##### Removing Irrelevant or Unnecessary Data
- In default SIEM implementations, log collection is meant to ingest everything that the log collection device is pointed at.
- However, this can lead to a large amount of unnecessary data reaching the SIEM engine and waste processing power to sort and organize.
- Performing transformations from the source to prevent unnecessary or irrelevant data can greatly improve the speed of processing logs from all sources and conserves resources for relevant data.

- As discussed in the Cyber Defense Analyst – Network (CDA-N) lessons Data Transformation and Data Normalization, tuning the Splunk forwarder with transformations is a great way to normalize and reduce redundant data.
- The Splunk forwarders can limit which data is allowed in from the source, limiting the processing power and data storage consumption on the Splunk Indexer.
- This simple task can significantly improve performance of SIEM solutions.

##### Monitoring SIEM Performance
- Most performance issues in the SIEM environment can be identified through the health checks available through the SIEM interface.
- For instance, Splunk’s Health Check dashboard was discussed in the CDA-N lesson SIEM Architectures.
- Identifying and troubleshooting the issues correlated within the SIEM can help to eliminate performance issues.

- For example, a health check in Splunk can be run to build a report of the system status.
  - If the system reports that the “Splunk service does not have sufficient resources,” action should be taken to improve system resources.
  - This can be as simple as logging directly into the Splunk system and checking which processes are running and taking up system resources.
  - Any services running that do not directly support Splunk can be killed and should help free up system resources for Splunk operations.
  - If the insufficient-resources message persists, the administrators of the device may have to revise the hardware of the system to adequately support the software.

<img width="690" height="425" alt="image" src="https://github.com/user-attachments/assets/6aba5892-d55a-4a1c-9185-fed72e619aca" />

<img width="759" height="448" alt="image" src="https://github.com/user-attachments/assets/a76a4fef-21c6-4efa-ac4e-305ba0904646" />


# MOD 18
## Data Collection
### Syslog
- Syslog is a standards-based logging protocol that converts system events into a standardized text-based log message format.
- The logs are then displayed directly to a console, held temporarily in a memory buffer, stored to a local file, sent to a remote log collector for centralized storage, or dealt with using a combination of all these options. 

- The syslog protocol was originally developed in the 1980s and over the years has experienced widespread adoption.
- In fact, syslog is the default logging protocol for virtually every modern Linux/UNIX Operating System (OS), network device, and storage appliance.
- Syslog is also available for Microsoft Windows through open-source projects and third-party vendors such as rsyslog, SolarWinds, ManageEngine, and Humio. 

#### Syslog Use Cases
- CDAs leverage syslog to increase visibility of mission partner networks because it is already present on a majority of (IT) platforms.
- This means that analysts do not need to install additional software or agents on most existing assets to generate relevant log data.
- Additionally, due to its popularity, most log collection platforms support syslog natively.

- Traditionally, syslog has been used primarily by IT professionals to monitor the status or performance of devices and applications.
- Local logging of system events provides an excellent mechanism for systems administrators to troubleshoot configuration issues.
- Shipping of logs to a remote collection platform provides a way to receive alerts about such critical events as network link failures or application crashes.
- Centralized log collection also provides additional security against malicious actors who may attempt to clear local log files to evade detection.

- Similarly, syslog can be configured to provide remote monitoring of security events.
- The following are examples of common security applications for syslog:
  - Monitoring of remote access attempts via such protocols as Secure Shell (SSH).
  - Policy violations, such as access control list blocks on network devices and host-based firewalls.
  - Configuration changes on network devices.
  - Proxy server access logs.
  - Remote shipping of almost any text-based log file.

- Syslog collectors can also be configured to detect when logs are no longer being received from critical systems.
- This feature can be useful to detect such activity as a malicious actor blocking logs from being shipped from an endpoint, or the failure of a critical system. 

#### Syslog Facilities
- Syslog facilities are essentially the name of the service or process that is generating log data.
- By default, syslog monitors only well-known system processes. However, additional configuration can be applied to monitor third-party or custom services.
- The syslog process monitors the logs generated by these services and categorizes them with a facility code based on their origin.
- The facility codes are used in tandem with a severity code to calculate log priority.

- Standard syslog facilities are described in Table 18.1-1.
- Understanding the details of each logging facility is not necessary to implement syslog.
- However, it is useful to understand the application of the facility codes in order to dissect information from the priority code, which is demonstrated shortly.
<img width="2206" height="2500" alt="da10adb8-7057-4f08-82f5-34911f962197" src="https://github.com/user-attachments/assets/7a3ebc6c-b695-4ca1-8e44-9bac3c98c011" />

- Not all installed services natively use syslog as their logging facility.
- An example of this is an Apache server hosting a website over Hypertext Transfer Protocol (HTTP).
- Apache uses its own internal configuration to write logs to disk.
- This means that syslog may be able to natively log some Apache events such as service restart (because it is tracked through the service manager) but syslog is not natively aware of the contents of Apache’s access_log.
- However, syslog can be configured to use built-in input modules to monitor nonstandard files. The lesson’s lab exercise demonstrates this. 

#### Syslog Severity
- Each log generated by a facility is accompanied by a severity code.
- Severity is a measure of **how important the log is** regarding the system or processes stability.
- Table 18.1-2 lists syslog severity codes and their corresponding level, where severity code 0 is highest and 7 is lowest.
<img width="2500" height="1484" alt="3164787d-2de1-4922-842b-2b72d64ee003" src="https://github.com/user-attachments/assets/4f683b52-5212-4ed4-b7b3-b15976fb1346" />

#### Syslog Message Format
- The traditional syslog message format consists of six fields, described as follows and illustrated in Figure 18.1-1:\
  - **Priority**: Calculated by multiplying the facility code by 8 and adding the severity level to the product.
  - **Timestamp**: The time that the log was generated.
  - **Host**: The hostname of the device that sent the message.
  - **Process Name**: The name of the process related to the message.
  - **Process ID**: The Process Identifier (PID) associated with the process that created the message.
  - **Message**: The relevant log information; this message is unstructured and varies, depending on the process that generates it.
<img width="663" height="41" alt="1a52fbc3-a1d9-479a-8e7c-d1a4a403ab53" src="https://github.com/user-attachments/assets/5ad63f10-b96d-4e42-aea8-db630d87491e" />

- The format above is based on the traditional syslog format specified in Request for Comments (RFC) 3164.

- Despite the existence of these standards, many vendors may choose to format their syslog output differently.
- For instance, Cisco® includes facility and severity codes in message output, which is useful, but by default syslog does not.
- Other vendors have completely custom log output formats that do not resemble the official syslog specification at all. In such cases, syslog has become a generic term that refers to any logging.  

- The facility code and severity code from a traditional syslog message may be determined by extracting them from the priority field shipped with the log.
- As stated above, the priority is calculated by multiplying the facility code by 8 and then adding the severity code.
- Using the example priority of 14 from Figure 18.1-1, the steps to calculate the facility code and severity code are as follows:

1. Divide the priority, 14, by 8:
  - 14/8  = 1.75

2. Round the solution, 1.75, down to the first whole number to obtain the facility code; the facility code is, thus, 1. 

3. Multiply the facility code by 8:
  - 1 ✕ 8 = 8

4. Subtract the result from Step 3 from the priority to determine the severity code:
  - 14 − 8 = 6

- As a result, the facility code in this example is 1, which is a user-level message with a severity code of 6, which is informational.

<img width="481" height="290" alt="image" src="https://github.com/user-attachments/assets/0bac0560-b2b0-4f92-84ec-a4428b078df9" />

### RSYSLOG Config
- Rsyslog is a common service available for modern OSs to store and ship syslog events.
- The primary rsyslog configuration file is `/etc/rsyslog.conf`.
- The main section of the `/etc/rsyslog.conf` configuration file is described below.

- Each of the uncommented lines of the configuration file describes logging a specific combination of a log facility with a log severity to a file.
- The general syntax for the rsyslog rule is facility.severity followed by the destination.
<img width="662" height="407" alt="3f643e1a-2ebe-46f7-a804-a7286a8f32e3" src="https://github.com/user-attachments/assets/f6e1d0cf-7ed0-4b7b-bdcd-34a46afa33b5" />


<img width="761" height="458" alt="image" src="https://github.com/user-attachments/assets/21ef8a84-b81e-4339-b501-61c6a2d171b8" />

### Splunk inputs.conf
- Like Logstash and Elasticsearch, Splunk can be configured to accept a wide array of inputs.
- This behavior is controlled by a collection of files, all named `inputs.conf`, that can be located in several directories.
- Splunk uses apps to ingest data and index it appropriately, and each app has its own `inputs.conf` file.
- The `inputs.conf` file defines how an app should process ingested data based on the port, protocol, and format that it receives.  

- Consider the example `inputs.conf` file located at `/opt/splunk/etc/apps/search/local/inputs.conf` and used by the default Splunk search app.
- The configuration stanza in Figure 18.1-11 instructs the Splunk search app to listen on UDP port 514, identify the sending host based on its IP address, send the collected data to an index named vyatta, and use the syslog format to dissect the data. 
<img width="662" height="81" alt="98c09d3a-7a1b-441c-8529-29c5bf01335e" src="https://github.com/user-attachments/assets/83499b7b-48f9-4670-a7e2-ef88d62bec0e" />

<img width="725" height="387" alt="image" src="https://github.com/user-attachments/assets/09725042-8eb0-4ba6-9876-71e414ef4bf3" />


## Event Consolidtion and Categorization
### Splunk Source Types
- Operating Systems (OS), applications, network devices, and appliances all generate log data that can be useful to CDAs in an investigation.
- However, the logs generated from these sources do not always use the same format when describing the event data.
- This disparity between log formats presents a challenge when attempting to collect and parse event logs from several sources in a centralized location.

- Splunk addresses this challenge by using the concept of source types.
- In Splunk, a source type is a **predefined mapping of field assignments** based on **known log source formats**.
- Source types are applied to incoming log data and used to extract relevant fields during the ingestion and indexing process.
- The result is that the extracted fields are searchable across all indexes, and the application of source types can be used to categorize data. 
- Splunk ships with more than 40 predefined source types that are capable of extracting field data from many widely adopted log sources.
- Table 18.2-1 provides a small sample of available default source types: 
<img width="1956" height="2500" alt="bb71be76-63ef-4725-be44-d7fd0ace0b07" src="https://github.com/user-attachments/assets/c53466ae-d515-4cef-b9a5-3a1cca4df36f" />

- Sometimes a CDA is required to ingest log data from a nonstandard log source, such as logs from a custom application.
- Splunk provides the ability to create custom source types that can parse this data based on delimiters, regular expressions, or a combination of both.
- This capability can be quite useful when it is necessary to ingest data from Comma Separated Values (CSV)–formatted logs, for instance. 

- Splunk is capable of automatically detecting the source type of most logs based on their format when they are ingested.
- This is an extremely useful feature that can simplify the task of log collection for an analyst. However, the autodetection function is not always reliable.
  - To overcome this, Splunk provides the ability to manually assign source types on the log forwarder or on the indexer. The upcoming lab demonstrates this behavior.

<img width="751" height="367" alt="image" src="https://github.com/user-attachments/assets/db116858-a9c0-4ab9-bed4-05b0426a28a3" />


### Elastic Log Forwarding
- The Elastic Stack uses a similar approach as Splunk to extract relevant field data from log sources.
- Whereas Splunk uses source types to parse logs, Elastic uses Logstash plug-ins.
- Elastic currently provides more than 50 Logstash input plug-ins to parse common log formats.
- Table 18.2-2 provides a sample of these plug-ins:
<img width="1668" height="1469" alt="cd417214-493e-4c4c-80c7-9b6f8042cfd8" src="https://github.com/user-attachments/assets/03bc1d6c-f617-438b-8ce7-4384b25dd105" />

- Some Elastic Logstash input plug-ins are designed to accept logs based on specific log formats, such as syslog, whereas others can accept raw log data over a listening port that can be tailored based on specific use cases, such as Transmission Control Protocol (TCP), User Datagram Protocol (UDP), and Hypertext Transfer Protocol (HTTP).

- Elastic Beats is one of the most popular choices for shipping logs from hosts or endpoints to an Elastic Stack instance.
- Elastic Beats is quite similar to the Splunk universal forwarder, where files or processes are monitored and the log data is shipped to an indexer.
- However, Beats is not always an appropriate solution for shipping data.
- For instance, some mission partners may not be willing to allow the installation of the Beats agent on their assets.
- In other instances, it may not be possible to install the agent on the target endpoint, such as network equipment, printers, and incompatible OSs.
- In such cases, Elastic can be configured to accept the log data shipped directly from the devices using standard or even custom log formats; syslog is the most common example.

<img width="708" height="300" alt="image" src="https://github.com/user-attachments/assets/7a854b95-cd58-47e1-85a6-387b23052380" />


## Event Correlation
### Searching for MCA
- CDAs tasked with performing threat hunt operations on mission partner networks are often required to develop an attack hypothesis before a threat hunt begins.
- An attack hypothesis helps to guide the hunt and ensures that CDAs are searching for meaningful artifacts that either prove or disprove the existence of MCA within a mission partner network.
- CDAs rely on previous experience and external data sources to form attack hypotheses.

#### Public Intelligence Feeds
- Most public intelligence feeds are based on previously observed Indicators of Compromise (IOC).
- Typically, these intelligence feeds consist of such information as event signatures, known malicious Internet Protocol (IP) addresses, domain names, file names, and file hashes.

#### Known Tactics, Techniques, and Procedures
- The MITRE ATT&CK® framework is composed of numerous TTPs, such as reconnaissance, initial access, persistence, privilege escalation, and lateral movement, that adversaries can use throughout all phases of the attack lifecycle.

#### Cyber Threat Intelligence
- CTI provides additional insights into how identified adversaries may behave, such as their motivations and specific TTPs they are known to use.
- This information is usually tailored to reflect known threats against a specific mission partner. 

### Intelligence Sources
![9a75293b-b4eb-4373-a00c-38d4a84ed031](https://github.com/user-attachments/assets/c6c345ac-3972-4ad8-9633-a91e73014bb7)

#### IOC's
- IOCs fall into the lower half of the Pyramid of Pain. Once made public, IOCs generally become less relevant.
- Such identifiers as file names, file hashes, IP addresses, sender email addresses, and domain names are easily modified by more advanced adversaries and typically change between attack campaigns.
- However, these public IOCs can still be useful to identify less-skilled adversaries or script kiddies who use readily available malware without modification.
- On the other hand, IOCs that are identified within a mission partner network during an active hunt can be extremely useful.

#### Tactics, Techniques, and Procedures
- Tools and TTPs fall into the upper half of the Pyramid of Pain.
- Implementation of detections based on known TTPs is arguably one of the most effective ways to catch MCA.
- However, attempting to quickly build detections of all known TTPs in a tailored manner on a mission partner network is not feasible.
- There are too many known TTPs, and target networks work in different ways.

#### Cyber Threat Intelligence
- CTI provides CDAs with a targeted set of TTPs and objectives that a known adversary attempts to use within a mission partner network.
- The use of CTI can be considered a best-of-both-worlds approach, as CDAs can focus on specific tools, TTPs, and their related artifacts to detect MCA.

### Using CTI
- When presented with CTI, a CDA must identify the relevance of available log sources and how they can be used to identify MCA.
- For instance, some TTPs and their artifacts are more likely to be detected through host-based logging than through network capture and network metadata extraction. 

#### Log Sources
- Consider a scenario in which a client workstation grants initial access to an attacker by downloading a malicious executable file or executing a macro-enabled Microsoft Excel file attached to an email.
- It may be easier to detect this occurrence via host-based sysmon logs that track process creation and network connections.
- However, from a network perspective, a CDA may see only an encrypted Hypertext Transfer Protocol Secure (HTTPS) connection to a web page that is actually being used as a Command-and-Control (C2) channel.
- Due to encryption, the C2 traffic may go undetected because it is intended to blend in with normal internet usage.

- CDAs need to be familiar with the log sources currently available to them.
- Knowledge of these log sources helps them identify which fields can be searched to identify evidence to help prove or disprove their hypothesis.
- This process also allows CDAs to identify gaps in collection where necessary log sources are not currently being ingested. 

#### Log Source Location
- Consider again the location of the log sources.
- For instance, a network sensor placed at the perimeter would not be capable of detecting lateral movement within the internal network.
- Alternatively, an internal sensor may not be able to detect MCA in other network segments or outbound C2 channels.
- Log source location is important, as it directly affects a CDA’s visibility of network segments and boundaries.
- A CDA’s understanding of where logs are being collected from plays a significant role in planning detections and identifying visibility gaps.

#### Data Normalization
- CDAs rely heavily on search queries to find IOCs and other evidence of TTP execution in Security Information and Event Management (SIEM)–collected log data.
- To make such search queries more efficient, it is necessary to ensure that common fields found in log data from different sources are normalized.
- An example of data normalization is searching for known malicious domain names in two different types of Domain Name System (DNS) logs, such as those generated by Zeek and Microsoft Windows DNS servers.
- In a Zeek DNS log, the field that contains the requested domain name is called query, whereas Microsoft DNS server logs store this value in a field called domain. 

- If a CDA had a list containing 100 interesting domain names, they would need to combine two different queries to search both logs at the same time.
- This means that they would have to search for a match in all the fields named query and then perform the same search through all the fields named domain.
- In Elastic, this might look like the following:
    - `query: is one of evil1.com, , evil100.com or domain: is one of evil1.com, …, evil100.com`

- A more efficient option is to modify the SIEM ingest pipeline so that both the query and domain fields in the Zeek and Windows DNS logs are mutated to share the same name, such as dns.query, prior to SIEM indexing.
- The result is that a CDA can search both log sources by querying a single field — in this example, the dns.query field.
- The resulting query in Elastic may look similar to the following:
  - `dns.query: is one of evil1.com, ..., evil100.com`

<img width="702" height="269" alt="image" src="https://github.com/user-attachments/assets/185b6744-857c-4295-a62f-5566913a064a" />

## Event Risk and Timelines
### Risk-Based MCA Response
- When planning hunt forward operations on a mission partner network, CDAs create a collection plan.
-  When CDAs detect MCA on a mission partner network, they have a responsibility to recommend appropriate response actions to the mission partner or system owner.
-  Similar to crafting the collection plan, a CDA should use assessed risk when developing these response actions.
-  Two key aspects that can assist CDAs in developing response actions are **determination of the severity** of the MCA and **determination of the MCA’s operational risk** to specific assets.

### Threat Severity
- CDAs may encounter a wide array of MCA while on mission.
- This activity includes, but is not limited to, **simple reconnaissance**, **exploitation attempts at the network boundary**, **phishing emails**, **data exfiltration**, **internal lateral movement**, **credential harvesting**, and **service degradation**.
- All this activity may be malicious, but each type does not warrant the same type of response.

- For instance, it is not appropriate to block an entire subnet’s connection to the internet due to one host performing an outbound port scan.
  - However, it may be desirable to terminate external access entirely if evidence exists that extremely sensitive data is being exfiltrated. 

- Additionally, different types of MCA may provide insight into the scope of an attacker’s access in the network.
- Consider a situation in which a CDA detects evidence that a user has executed a payload contained in a phishing email.
- It is possible that the adversary now has remote access to this workstation, but they may be confined to that workstation while they attempt to escalate privileges and expand access.
- In this case, simply isolating the affected workstations and clearing the payload may be sufficient. 

- On the other hand, detection of lateral movement via payload uploads to remote Server Message Block (SMB) administrative shares indicates that the attacker has already gained some level of administrative access in the environment.
- The lateral movement activity may require a more robust response than the phishing activity, such as launching a full investigation to determine the full scope of the compromise. 

### Operational Risks
- CDAs frequently need to tailor their response to MCA based on risks to the mission partner’s operations.
- Essentially, a **CDA’s planned response** to MCA should **not have a greater negative impact** on the mission partner’s operation **than the MCA itself**. 

- Consider an example of a detected call management system compromise within a 911 call center.
- From a technical perspective, it may be rational to take the system offline and restore its baseline.
- However, the system owner may choose to leave the system online despite the known compromise because downtime prevents the organization from performing its primary function, which is receiving emergency calls and coordinating assistance.
- That downtime could result in grave outcomes to those who rely on the service.
- In cases like this, CDAs must be more creative in their response plan and focus on alternate containment strategies, such as blocking specific network communications with access control lists.

### Additional Considerations
- CDAs should consider the impact their response actions may have from an intelligence perspective.
- In some cases, it may be desirable to closely monitor an adversary’s behavior after MCA is detected rather than immediately clearing that activity.
- Monitoring adversary behavior within the network can provide CDAs with additional information, such as (IOC) like malicious file hashes; (C2) (IP) addresses/domain names; or the adversary’s (TTP).
- This approach may better assist the CDA with properly scoping the compromise to identify all compromised hosts in the network.
- The decision to gather intelligence versus clear the activity is made by the system owner and is typically more palatable when the observed activity does not endanger critical systems.

- However, just as adversaries’ actions can provide CDAs with intelligence, a CDA’s response actions can provide the attacker with information.
- In general, adversaries believe that their presence is undetected until they are given a reason to believe otherwise.
- If a CDA initiates clearing actions on compromised hosts, the attacker notices that they have lost C2 of those hosts.
- This may prompt the attacker to become more aggressive in accomplishing their objectives.
- They may start mass data exfiltration from systems where they still have access, attempt to destruct or degrade systems in the network, or rapidly expand their foothold in the network by compromising additional hosts.
- Alternatively, they may temporarily halt their activities in an attempt to remain silent and maintain some access on the network, only to return later and continue their campaign.
- This scenario can result in “Whac-a-Mole,” in which CDAs are constantly clearing new compromised hosts but never completely remove the adversaries’ access to the network. 

<img width="750" height="462" alt="image" src="https://github.com/user-attachments/assets/f74f96ae-6bf1-482b-8cc8-dc11c5e23fc2" />

### Compilation of Evidence Sources
- CDAs may be alerted to the potential existence of MCA through such means as (IDS) alerts, log anomalies, user-detected behaviors, or flagging on known malicious IOCs.
- In general, these individual events are not sufficient to declare an incident outright and must be investigated further.
- Once MCA is suspected within a mission partner network, CDAs must identify all relevant and available log sources that can be used to aid in an investigation. 

- The prior lesson Event Correlation included a lateral movement example.
- The potentially malicious behavior used remote access of administrative SMB shares, remote service manager commands, and connections to named pipes on Microsoft Windows systems.
- Detection of these events required three different Zeek logs.  

- The combination of the three events within a relative time frame is indicative of a known TTP used by adversaries for lateral movement.
- However, if those events were observed individually or were separated by a long period of time, they may have been attributed to legitimate behavior. 


## Event Tuning
### False Positives and False Negatives
- IDSs are used extensively in network security deployments.
- Such systems have the capability to alert analysts to the presence of malicious behavior on a network.
- These alerts are generated when network traffic observed by the IDS sensor matches a predefined set of criteria, commonly referred to as a signature. 

- A common issue with IDS deployments is that they are often implemented with generic signature rule sets that are not tuned for a specific environment, or signatures are not maintained to keep up with a changing environment.
- The result of such a deployment can be that the IDS floods the CDA with meaningless alerts while simultaneously failing to alert during actual events. 

- Poorly written IDS rules can generate so many IDS alerts that analysts are overwhelmed, with the analyst stuck in a loop constantly investigating and clearing alerts.
- Eventually, this leads to alert fatigue, in which analysts start to ignore high volume alerts and clear them without investigation. 

- The concepts used to describe the alert accuracy of an IDS are **true positives**, **true negatives**, **false positives**, and **false negatives**:

<img width="2500" height="575" alt="c3b57fd3-8bd2-4a24-ba87-793bfb8d3e0c" src="https://github.com/user-attachments/assets/139c4584-4c8f-49fb-ab0b-765f333ecf18" />

#### True Positive
- A true positive occurs when malicious traffic traverses an IDS sensor interface and an IDS rule appropriately identifies the traffic as malicious, generating an alert.
- This is a desired result, because malicious traffic is indeed present and the IDS detects the malicious traffic and provides an alert that prompts action from an analyst. 
- However, not all true positive IDS alerts are actionable.
- For instance, an IDS alert that detects external Secure Shell (SSH) attempts against an external firewall may be a true positive, but this does not prompt action from an analyst if the firewall is configured to block external SSH traffic.
- In cases in which IDS alerts are not actionable, it may be beneficial to disable the alert.

#### True Negative
- A true negative is a situation in which legitimate traffic is observed by an IDS sensor, no IDS rules match, and, therefore, no alerts are generated.
- This is also desired behavior, as the IDS correctly identifies the observed traffic as benign.

#### False Positive
- False positives occur when an IDS alert is triggered by non-malicious activity.
- False positives are typically caused by poorly tuned IDS rules that cannot discern the difference between the targeted malicious activity and legitimate traffic. 

#### False Negative
- False negatives occur when malicious activity is present but the IDS sensor does not properly alert on the activity.
- Like false positives, false negatives can also be caused by poorly written IDS rules. 

- A unique characteristic of false negatives is that they can also occur when the malicious behavior is completely unknown and, therefore, no IDS signature currently exists to detect the traffic.
- Because some malicious behavior is unknown at the time of attack, it is impossible to prove that a false negative condition exists. 

### Alert Classification
- An IDS alert alone is not enough to determine if an attack has occurred or is currently underway.
- The ability to contextualize events and discriminate between true positives and false positives is where an analyst’s work truly begins.
- In some cases, the event may clearly display evidence or malicious behavior, such as Structured Query Language (SQL) injection attempts triggered by traffic containing 1=1;.
- Other events may be more obscure, such as an event named potential C2 detected.
- When investigating potential malicious traffic, relying on such common frameworks as Lockheed Martin Corporation’s Cyber Kill Chain® (Figure 18.5-5) or MITRE ATT&CK® can be useful.

- Applying alert activity to steps in either of these frameworks assists the analyst in a more targeted investigation.
- Consider key steps in the Cyber Kill Chain, such as delivery, installation, and Command and Control (C2).
  - If an analyst receives an alert stating that a host has downloaded a potentially malicious executable from the internet, the analyst may hypothesize that this is part of the delivery phase of an attack.
  - If the alert is part of the delivery phase, it is likely closely followed by execution/installation and possibly C2.
  - After gathering relevant information from the alert, such as target host, executable name, and time, the analyst could work with the host team to search for evidence of execution on the target host.
  - Likewise, the analyst could check network traffic to search for new outbound connections from that host to the internet or other internal systems.

- The ATT&CK framework can provide extra granularity that aids in alert investigation.
- In addition to laying out phases of an attack, such as initial access, privilege escalation, and lateral movement, the ATT&CK framework lists specific techniques used by attackers to accomplish these goals.
- An analyst may receive an alert such as detected access to SMB admin share but be unaware of what this means.
- Referencing the ATT&CK framework, the analyst discovers that this could be evidence of lateral movement.
- Understanding that this behavior may be related to lateral movement gives the analyst more context, such as understanding that the source host in the event is likely already compromised.


### Intelligence-Driven Event Signatures
- False positives can be frustrating to analysts tasked with investigating IDS alerts.
- Organizations may choose to disable or modify alerts that generate a high volume of false positives to streamline these operations.
- In fact, some alerts are so noisy that they may be disabled by default in open-source rulesets.
- However, occasions arise when intelligence may dictate the activation of previously disabled rules, even if they are extremely noisy. 

- Consider, for example, that an IDS rule alerts on remote PowerShell activity but the organization relies heavily on PowerShell remoting for daily administrative activities.
- This rule frequently generates alerts for legitimate administrative actions and, as a result, has been disabled by the organization.
- However, the organization has received intelligence stating that it has been targeted by an adversary that also uses PowerShell remoting for post-exploitation activity.
- In this case, the organization may decide to enable the rule because the risk of missing malicious PowerShell activity outweighs the high volume of false positives that the rule would generate.
- It is considered best practice to tune these rules or target their deployment to reduce false positives.
- The organization might require administrators to only execute PowerShell commands from a specific administrative workstation and exclude this workstation from the detection rule.
- Alternatively, the organization might enable the rule on only specific high-value subnets.  

<img width="759" height="552" alt="image" src="https://github.com/user-attachments/assets/2d8bb8ac-1ba1-45e8-afa5-a2c7ace2b759" />

# MOD 19
## Creating and Populating New Indexes
### Elasicsearch Indexing
- Elasticsearch is an open-source analytics engine that processes various types of data.
  - Elasticsearch uses its Application Programming Interface (**API**) and **graphical interface** to **ingest data** and allow its users to **store**, **analyze**, **enrich**, and **visualize** the data.
  - The collection of **Elasticsearch**, **Logstash**, **Kibana**, and the **Beats** data shipping tool constitute the **Elastic Stack**. 

- Elasticsearch is used for a wide variety of applications.
- For example, the engine has been used in past Cyber Defense Analyst – Network (CDA-N) lessons to analyze NetFlow data, read logs shipped with Filebeat, and examine uptime statistics.
- Additionally, Elasticsearch may be used to monitor application performance, search data from websites and enterprises, and provide analytics for security systems. 

- Elasticsearch works by first ingesting raw data.
- This data may come from various sources, such as log files or website traffic.
- The data is then processed, which involves parsing, normalizing, and enriching the raw data, as seen in such previous lessons as Logstash and Enriching Data with Filters.
- The processed data is then indexed, at which point the data is ready for use by the user.
- This may involve querying with the API and web interface, visualizing with Kibana, and aggregating to gain insight. 

#### Indexing 
- Elasticsearch uses indexes to store its data. An Elasticsearch index is a collection of related documents.
- These documents are in JavaScript Object Notation (**JSON**) format, meaning that information consists of a key and a corresponding value.
- A key is a field name or property, such as name or age.
- A value corresponds to the key and is a certain data type.
- For example, a name key might have a value of John, and the age key might have a value of 25. 

- Although the process of storing information as key-value pairs is common, Elasticsearch uses a special data structure called an inverted index.
- Instead of storing information as tables and rows (as in traditional Structured Query Language [SQL]), Elasticsearch processes information into lists that contain unique words and their frequencies.
- Additionally, the documents in which the words appear are saved. Figure 19.1-1 depicts the indexing of information into an inverted index:
<img width="1667" height="742" alt="212962d6-df6c-4ac7-b42f-b66772812796" src="https://github.com/user-attachments/assets/34f571b4-a2a0-417f-82c5-882aeafc67b1" />

- **The advantage of using inverted indexes is the speed at which queries return data.**
- **Full-text searches complete quickly due** to the way the words are stored.
- This results in **significantly faster query result times** than traditional SQL databases.
- Elasticsearch specializes in handling large amounts of data, such as logs, large text entries, and other records. 

#### Shards and Replicas 
- Elasticsearch can be distributed across multiple nodes or servers within a cluster (a collection of nodes or servers).
- When an index becomes too large (in which it exceeds hardware limits of the host, causing slow search times due to sheer size of lists), Elasticsearch can divide the index into smaller pieces called shards.
- Shards balance the size of the index across systems.
- Additionally, shards can be migrated automatically by Elasticsearch in the event of a size change within the cluster.
- This keeps the data distributed and balanced across the systems. 
<img width="2048" height="1025" alt="b0787ca7-6295-4029-9b76-f8a79ca24529" src="https://github.com/user-attachments/assets/44f9bee7-21a5-442a-b6d9-eccf56b1b2a4" />


- An Elasticsearch shard is classified as either a primary shard or replica shard.
- Primary shards are the standard method of storing index data. A replica shard, as its name suggests, is a copy of a primary shard.
- A replica shard is always stored on a different node from the primary shard, which helps to ensure that data is not lost in the event of hardware failure. 

<img width="754" height="335" alt="image" src="https://github.com/user-attachments/assets/f020bc2c-28c1-4fe1-8979-16523b8c1dcc" />

### Elasticsearch Index Templates
- An **index template**, as the name implies, is a _template for the creation of an index within Elasticsearch_.
- When creating an index, the user may configure it through **mappings**, **settings**, and **aliases**.
- Settings specify the shard sizing and refresh rates, mapping controls the data schema, and aliases give indexes alternate names. 

#### Index Templates vs. Component Templates 
- An index template may contain one or more building blocks called component templates.
- Component templates, like index templates, specify mappings, settings, and aliases, but they are applied to index templates rather than directly to indexes. 

- As a daily-life comparison, an index may be thought of as a meal being prepared.
- The index template is like the individual ingredients that make the meal.
- However, to save time, the person cooking the meal may purchase pre-packaged salad, bottled dressing, canned soup, and pre-seasoned steak.
- These ready-to-cook parts are similar to a component template in Elasticsearch.
- By combining pre-made food (component templates) with dishes and sides made from scratch (like configuring an index template manually), time is saved in creating the meal (index). 

#### Index Settings 
- Certain settings are specified during index creation.
- These settings are configured to control various aspects of the index both before and after creation.
- Such settings are either static or dynamic.

- **Static settings** are established **only before the index is created** or **after it has closed** (not in use).
- For example, the index.number_of_shards setting defines how many primary shards are to be created for the index.
- Another static setting, index.codec, controls the data compression settings for the index. 

- Unlike static settings, **dynamic settings** may be **set at any time of the index’s lifecycle**.
- For example, the index.number_of_replicas setting defines how many replica shards exist for each primary shard.
- Another dynamic setting, index.refresh_interval, controls how often an index is refreshed. 

#### Mapping
- Mapping is the process of defining how a document’s contents are stored and indexed in Elasticsearch.
- Each Elasticsearch JSON document contains various fields that each have a data type for their values.
- In the process of mapping, a definition is created to define the relevant fields of the JSON document.
- There are two types of mapping:
  - dynamic
    - Elasticsearch automatically adds the new fields as the JSON document is indexed. Dynamic mapping is useful for saving time when mapping and for guiding new users through the process.
  - explicit.
    - Explicit mapping is used to define exact fields for mapping.
    - A user may elect to explicitly define certain fields based on values that could be confused with others.
    - For example, a field that contains a phone number might be explicitly defined to be a string rather than a number so that dashes, spaces, and parentheses are accepted. 

#### Aliases 
- An alias in Elasticsearch is an alternate name for a group of data streams or indexes.
- An alias is used within the Elasticsearch API when making calls.
- An alias is classified as either a **data stream** alias, pointing to **one or more data streams**, or an **index** alias, which points to **one or more indexes**.
- Aliases are useful for performing commands on groups of targets without having to individually specify each.
- For example, assigning the group of Apache, nginx, lighttpd, and Tomcat logs to the weblogs alias allows a user to perform operations on all the indexes simultaneously. 

#### Ingest Pipelines 
- Ingest pipelines are a series of transformations that happen to data before it is indexed.
- Each pipeline contains segments that perform actions; these segments are called processors.
- Like a physical pipeline, the data flows from one end to another, and the processors act sequentially.
- Pipelines have a large range of options to manipulate incoming data.
- For example, a pipeline can add, change, or remove certain fields; automatically capitalize words; and find and replace certain terms. 

#### Text Analysis
- Text analysis within Elasticsearch is a powerful tool that allows users to find relevant results as opposed to exact results.
- Being constrained to searching by case-matching and literal spellings is time consuming and often omits valid results.
- Elasticsearch text analysis solves this by using both tokenization and normalization.

- **Tokenization** is the _breaking of chunks of text into smaller bits_, called **tokens**.
  - By tokenizing the sentence “Working with Linux computers is exciting,” individual words are made into searchable tokens.
  - If the entire “Working with Linux computers is exciting” sentence were indexed as is, the user would have to search for the exact phrase to return the result.
  - However, because the words are tokenized, searching for working, Linux, or computers is, for example, returns the original sentence. 

- **Normalization** further _allows queries to reach the required information by processing tokens in several ways_.
- Normalization of the sentence “Working with Linux computers is exciting” might involve the following:
  - Such **uppercase words** as Working or Linux are changed to lowercase (working or linux) to allow searches to find both capitalizations.
  - **Tenses** for such words as working may be reduced or expanded so that searches for work or worked return a result.
  - **Synonyms** for such words as exciting are considered so that, for example, a search for thrilling returns results with exciting as well. 

- Users may also deviate from the standard text analyzer provided by Elasticsearch and opt for custom-configured analyzers to best fit their needs.

<img width="615" height="305" alt="image" src="https://github.com/user-attachments/assets/019273d3-6640-43cd-b51f-da60dc8d5c0f" />

<img width="772" height="408" alt="image" src="https://github.com/user-attachments/assets/79306fc3-f7f1-478d-9e4b-9ba5bc5a514d" />

### Elasticsearch Data Streams
- Elasticsearch is often used to store data that is append-only.
- In other words, the data being processed is rarely, if ever, modified after it is stored.
- A common example of append-only data is logs, which continuously generate new information that is appended to the prior log entries.
- Log files are rarely modified after storage in order to preserve evidence and other data contained in them. 

- Data streams are mechanisms within Elasticsearch that allow users to store append-only information within multiple indexes and access them with a single alias.
- Queries are submitted directly to the data stream, which intelligently routes the request to the appropriate index. 

#### Data Stream Composition 
- A data stream is composed of one or more indexes called backing indexes.
  - A backing index is automatically generated by the data stream to append new data.
  - Backing indexes are hidden from wildcard searches because they are not usually searched against directly.

- Data streams are designed to be used with data that is append-only.
- Therefore, due to the nature of the content, the data stream creates indexes in an incremental manner.
- The backing index is named as follows:
  - `.ds-[DATA STREAM NAME]-[YYYY.MM.DD]-[GENERATION NUMBER]`

- `DATA STREAM NAME` and `YYYY.MM.DD` fields are self-explanatory.
- The GENERATION NUMBER of a backing index is incremented each time a new backing index is created.
- This process of creating a new backing index is called a rollover.
- The generation number is always six digits, starting at 000001 and ending at 999999.
- For example, a backing index with the name .ds-intrusionLogs-2022.03.13-000012 is from the intrusionLogs data stream; was created on March 13, 2022; and is the twelfth backing index within the data stream. 

- Performing read requests on a data stream is fairly straightforward.
- When a request to read information is sent to the data stream, the request is routed to all the backing indexes.
- This allows each index to simultaneously search for the requested information and return its results. 

- Performing a write request on a data stream is different from a read request.
- Due to the nature of append-only data, the data stream does not allow users to write information to anything other than the most recently created backing index.
- The newest backing index is referred to as the write index, as it is the only target for write requests.
- There are further limitations on what actions can happen to a write index.
- A write index cannot be cloned, be deleted, or have its number of primary shards changed.
- These rules ensure that the data stream’s contents are in chronological order. 

#### Index Lifecycle Management
- Managing indexes within Elasticsearch may be time consuming, especially at a large scale.
- To assist administrators in Elasticsearch, an ILM policy may be implemented.
- ILM policies automatically manage indexes and perform such tasks as enforcing data retention standards, creating new indexes when an old one reaches a certain size, and migrating infrequently used indexes to more cost-effective hardware.
- ILM policies are covered in greater depth in a later lesson. 

## Configuring and Tuning Indexes
### Performance Monitoring
- A variety of factors determines the speed in which Elasticsearch operates and returns results.
- Such factors include hardware configurations as well as certain Elastic metrics. 

#### Hardware Performance Factors 
- Elastic recommends three primary approaches to increase performance from a hardware perspective:
  - Using more powerful hardware, such as Central Processing Units (CPU), storage devices, and memory, is an obvious but important change that dramatically improves results.
    - The use of Solid-State Drives (SSD) instead of traditional hard drives reduces the time needed to read and write to storage.
  - Having the storage device physically connected to the system, as opposed to using remote storage, further increases the speed to access data.
    - Without the need to establish connections to other systems or transfer the data across the network, a directly connected storage device delivers results more quickly.
  - Increasing the number of high-performance CPUs on the Elasticsearch systems further improves search speed. 

#### Specifying Metrics for Performance
- Elastic Metricbeat is a shipper for such system statistics as CPU and memory usage and specifications, file system space details, and disk and network Input/Output (IO).
- In addition to host metrics, Metricbeat monitors statistics similar to the “top” system monitor tool (used to monitor process CPU, memory, and disk usage) on Linux for individual running processes.
- The elasticsearch module for Metricbeat even allows it to monitor important statistics for Elasticsearch itself. 

- Metricbeat can query and return cluster-specific metrics.
- For example, information about nodes, cluster health, and indexes can be obtained and displayed.
- However, if performance is a priority with Metricbeat, the user can opt to tune requests to minimize the amount of returning information as well as adjust the frequency of gathering.
- For example, using `GET /_nodes/stats` returns statistics for all nodes in question.
- Modifying this to `GET /_nodes/<node_id>/stats` gathers statistics on only the specified node, and `GET /_nodes/<node_id>/stats/process` retrieves only process statistics for the specific node.
- Additionally, **Metricbeat** configurations can be modified to search for specific information within indexes.
- Using precision requests within Metricbeat reduces the amount of unneeded metrics returned to the user, which increases the speed of results. 


### Performance Tuning
#### Optimize JSON Document Structure
- JavaScript Object Notation (JSON) documents can be structured in ways that minimize processing power needed to search them.
- For example, eliminating nested data and parent-child data can significantly increase query speed. 

#### Consolidate Fields
- Consolidating multiple fields into a single field for searching also improves query speed.
- For example, a query to search for Hostname, OS, and IP must process each of the three fields.
- However, the copy-to directive in the document combines fields into a single field, such as Hostname_OS_IP, that requires only one search within the document.
- Although combining fields is good for speed, combining too many fields results in returning too much information per search.
- Users must ensure that a combination of too many fields does not hinder queries.

#### Pre-Index Data
- Pre-indexing data within documents also improves search performance.
- For example, if a query must determine an age range for people in an employee directory each time it searches, extra processing power is needed.
- If the age range is defined within the document itself, the query does not need to perform as much work. 

#### Avoid integer and long Data Types
- Another way to optimize documents for search is to avoid using integer or long data types for numbers that are not used in range queries.
- For example, a randomly generated number, such as a ticket Identifier (ID), is generally not used in grouping ranges, as the number does not have any significance beyond being an identifier.
- For these types of numbers, using the keyword data type is preferred because integer and long data types require more processing power by default.  

#### Avoid Scripts
- Avoiding the use of scripts within queries can reduce the load on the search engine. 

#### Round Time Ranges
- Rounding time ranges to the nearest hour, minute, or second (depending on the need for precision) helps with speed. 

#### Merge Read-Only Indexes
- Merging read-only indexes into a single segment simplifies searches and saves time.
- For example, combining shards of log files into one segment allows queries to run only on that location without needing to navigate to others.
- However, this negates the benefits of splitting indexes across multiple shards. A fitting balance between speed and redundancy must be defined to prevent slow speeds or data loss. 

#### “Warm up” Caches
- Elastic recommends that certain caches be “warmed up” when a user predicts that an index will be heavily used and speed is a priority.
- Warming up a cache instructs Elasticsearch to construct the cache and load it with files and ordinals for immediate use.
- Examples of caches to warm up include global ordinals (used in summaries of datasets, called aggregations) and the file system cache (which loads highly used areas of indexes into memory).
- Additionally, using a preference value assists in optimizing the use of caches to certain users. 

#### Consider the Ratio of Shards to Nodes
- The user must consider the ratio of replica shards to Elastic nodes.
- In most cases, a greater number of replicas weighs down the performance of the node that it is on.
- However, this must be balanced with the redundancy and backup that having replica shards provides. 

### Index Lifecycle Management Overview
- Managing indexes in Elasticsearch is time consuming, especially at a large scale.
- To assist administrators in Elasticsearch, an ILM policy may be implemented.
- ILM policies are created either through Kibana (in the management interface) or with the ILM Application Programming Interface (API).
- Once applied, the policy manages Elasticsearch indexes for the user.
- For example, ILM prevents an index from growing too large and degrading the performance of the Elastic node.
  - Or, if an index is older than a user-specified age (such as 30 days), the ILM automatically creates a new one.
  - These examples depict a slice of what ILM policies are capable of doing within Elasticsearch. 

#### Lifecycle Phases 
- An Elasticsearch index resides in one of five phases defined by an ILM.
- Imprecise words, such as actively, frequently, and rarely, are purposely used in the phase descriptions.
- This is because the policy for each index’s lifecycle defines what frequencies, time frames, and other metrics are used in determining the phase.
- The ILM dictates which phase each index is currently in, the actions for each phase, and the requirements to move an index to each phase.
- The five phases, described below, are collectively known as the index lifecycle:
  - **Hot**: An index in the hot phase is being actively and frequently used.
    - Such operations as queries, additions, deletions, and updates are taking place.
  - **Warm**: An index in the warm phase is actively queried like a hot index but is no longer being updated with new data.
  - **Cold**: Indexes in the cold phase are not updated with new data, and they are queried less often than warm indexes.
    - Due to this, cold indexes are moved to slower systems where the data is still reachable but the response times are slower.
  - **Frozen**: Indexes in the frozen phase are similar to cold indexes but even less frequently queried.
    - A frozen index is not obsolete; rather, it simply contains information that is not commonly accessed.
    - Because the data is infrequently queried anymore, frozen indexes are kept on the slowest systems where the data can still be queried but with slow response times.
  - **Delete**: An index in the delete phase is no longer updated or queried. Such indexes are deleted. The delete phase is the final phase of the index lifecycle.

#### ILM Actions
- The ILM has an arsenal of actions that can be carried out on an index.
- These actions can be configured and can occur in different orders, depending on the ILM.
- For example, an ILM may create a Searchable Snapshot of an index, complete the Wait For Snapshot action to ensure that it exists, and then Delete the index.
- The following are all the actions that an ILM is capable of:
  - **Allocate**: The ILM moves primary shards to different Elastic nodes to balance performance and index size.
  - **Delete**: Deletes an index.
  - **Force Merge**: Changes the index to read only and then consolidates index segments.
  - **Migrate**: The index or index shards are moved to another node. Unlike the Allocate action, the Migrate action moves the index or index shard to a system that has the same or similar hardware characteristics as its previous system.
  - **Read Only**: Sets the index to a read-only state, preventing changes from being made.
  - **Rollover**: The current write index is retired, and a new index is designated. Elasticsearch then writes data to this new index.
  - **Searchable Snapshot**: Creates a snapshot of the current index. The snapshot is a read-only chunk of data and not resource intensive. This snapshot is saved as a searchable snapshot for future queries.
  - **Set Priority**: Specifies the priority of an index recovery when a node restarts. When index A has a higher priority than index B, index A is recovered first when the system starts.
  - **Shrink**: Reduces the number of primary shards associated with the index.
  - **Unfollow**: Stops the index from following its leader index and turns the index into a regular index. (A follower index is an index that replicates actions and operations performed on its leader index.)
  - **Wait for Snapshot**: Checks for an existing snapshot of an index before deleting the index. This prevents loss of data by ensuring that a backup exists. 

#### Strategies 
- Many strategies exist for ILM policies to manage indexes.
- Two commonly used strategies are:
  - time-based
    - For a time-based (also called date-based) policy, the ILM focuses on rolling over new indexes based on their age or at a certain date.
    - For example, a weekly index rollover for log files ensures that each week has its own associated index.
    - With this strategy, an analyst can find data based on the time that it was entered. 
  - storage-based policies.
    - For a storage-based policy, ILMs focus on distributing indexes for performance and storage size.
    - For example, if a server has little activity in one week and high traffic the next week, the ILM balances the indexes and nodes to ensure that one does not significantly outweigh the other. 


# MOD 20
## 











