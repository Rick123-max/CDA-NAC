# MOD 6
## Operationalizing Threat Intelligence
### Cyber Threat Intelligence Overview
- At its heart, intelligence is the science and art of understanding the enemy.
- As a discipline, intelligence is as old as warfare itself.
- Commanders have striven to predict the actions of their adversary, whether on a kinetic battlefield or in a futuristic cyberwar.
- Understanding the enemy’s capability and intent is the only way to make informed decisions on one’s own tactics to employ in response.
- Understanding the Operational Environment (OE) — the **conditions**, **circumstances**, and **influences** that affect the employment of capabilities — is paramount to making sound decisions in both offensive and defensive engagements.
- Traditionally in military intelligence, this refers to such things as the enemy’s **firepower**, **aggressiveness**, or **infrastructure** available to resupply their troops.
- In the cyber domain, direct correlations can be made to “traditional” intelligence. The product of these correlations forms CTI.

#### Intelligence in the Cyber Domain
- CTI is analyzed, actionable information, derived **internally** or **externally**, that aids an organization in **identifying**, **assessing**, **monitoring**, and **responding** to cyber threats.
- Because the cyber domain is now a major battlefield for crime, activism, and espionage, CTI has become increasingly essential to cyber defense.
- Relying on current and historical attack knowledge, an analyst can leverage CTI to prepare for future attacks on a defended terrain.
- Almost every tenet of traditional military intelligence can be applied to the cyber domain, not just in a military application.
- The cyber intelligence landscape is vast.
- It is unique in that there are thriving communities in both private and public sectors.
- The **private**, or **open-source**, intelligence community can be found in many different forums; **social media**, **vendor websites**, **blogs**, and **subscription services** are good examples.
- The government intelligence community, simply known as the “Intelligence Community” (**IC**), is less available to the public.
- Both these communities provide valuable resources to an analyst on a Cyber Protection Team (CPT).
- In fact, the CPT construct has an entire work role — the All-Source Analyst (ASA) — dedicated to interfacing with both the IC and open-source intelligence communities.
- Thanks to its countless devices and large intelligence communities, cyber, in general, has no shortage of “data.”
- Analysts use applications to collect data, they store data on servers, and they sift through telemetry data generated by anything that can ship a log.
- It is this data, or information, that eventually becomes intelligence.

#### Data vs. Intelligence
- Joint Publication 2-0, Joint Intelligence, Executive Summary, states the following about information (or “data”) vs. intelligence:
  - **_“Information on its own may be of utility to the commander, but when related to other information about the operational environment and considered in the light of past experience, it gives rise to a new understanding of the information, which may be termed ‘intelligence.’” _**

- In other words, analysis is what separates information from intelligence.
- For cyber defense, analysis is what makes data about threats in the cyber domain become CTI.
- For example, a Domain Name System (**DNS**) lookup for example[.]com is a piece of data.
- Further analysis may indicate that the domain is registered to Threat Actor, LLC, and can be tied to an ongoing espionage campaign.
- That extra analysis step is what makes data useful intelligence.

### The Intelligence Cycle
- Analysts need to understand the application and cyclical nature of CTI to be effective in countering active and future threats on a network.
- CDAs who are part of a CPT are not expected to perform formal intelligence analysis, but they are the consumers and, in some cases, collectors of intelligence.
- Maintaining realistic expectations and knowing what can be provided to the IC increases mission effectiveness throughout the execution of an operation.
- CTI follows the Intelligence Cycle, in which CDAs play an active role.
- Figure 6.1-1 illustrates the six steps of the cycle: **Direction**, **Collection**, **Processing**, **Analysis**, **Dissemination**, and **Feedback**.
  ![eed5e1d5-9f2e-481b-90d8-763547cd5c61](https://github.com/user-attachments/assets/a039dfb1-cadd-4fd9-be57-6a05e7358973)

#### Direction
- CTI analysis is always based on requirements.
- The **first step** of the Intelligence Cycle is to **define a clear requirement or question** for which intelligence should provide an answer.
- Often, for external intelligence, this requirement or question is submitted to an intelligence analyst in the form of a **Request for Information (RFI)**.
  - Uses for intelligence are numerous, so RFIs may ask a wide variety of questions.
- CDAs likely are most concerned with obtaining or collecting tactical intelligence for immediate use defending a network.
- Sometimes, a CPT may be employed to answer or collect information on intelligence requirements given by a higher tasking authority.

#### Collection
- The collection of cyber intelligence is typically the **process of collecting data from networks useful in tracking a threat**.
- However, it may come in other forms as well, such as reading a news article for political or economic information pertaining to a mission partner’s organization or viewing data from an open honeypot provided by researchers on the internet.
- At this point in the cycle, this is just data and not yet intelligence.
- **Collection is not a one-time effort**.
  - It comes in cycles of its own.
  - For example, a CDA may find a suspicious Internet Protocol (IP) address thought to be Command-and-Control (C2) communication to a malicious server.
  - The next cycle of collection on that data might be identifying what domain resolves to that address, followed by a cycle of collecting certificate information on that domain.
  - The cycle continues until enough data is collected to meet the requirement defined prior to collection.

#### Processing
- An analyst should not overlook the importance of processing data to be usable.
- A CDA’s analysis platform should be flexible and configurable enough to view a multitude of different data types in a normalized way.
  - For example, collected logs from firewalls may have different formats, depending on their type or brand.
- It is more useful if a CDA can process the log data to conform to a standard format than if they must manually interpret and compare the logs separately.
- The creation of **visualizations** also falls into the Processing step.
  - **Visualizing data** can greatly **assist a CDA or ASA in identifying trends** and **answering questions proposed** to the intelligence process as a whole.

#### Analysis
- **CTI analysis is based on requirements**.
- Without a requirement, the analysis has no question to answer.
- This is the step in which **data becomes intelligence**.
- Questions asked, or requirements presented, in the initial step of the cycle (Direction) are answered using analysis.
- The Analysis step is distinct from Processing.
  - Whereas the Processing step is usually automated or involves only manipulating data, analysis is done exclusively by a human.
  - This is where a CDA interprets the data that was collected and processed in previous steps.

#### Dissemination
- Intelligence is a shared commodity.
- It should be shared with the entity that requested the information, and it is often shared with a larger community of intelligence analysts.
- CDAs should concern themselves with this step because the format and completeness of the collected data and analysis directly affect the value of the dissemination of intelligence.
- ASAs are leveraged in the step to disseminate intelligence back to the IC.

#### Feedback
- The last step of the Intelligence Cycle is Feedback.
- Did the analysis properly answer the requirement?
  - This is a question that CDAs should answer if requesting or analyzing intelligence.
  - If the intelligence fails to meet the requirement, a new cycle is started in which the error is corrected or intelligence gaps are identified more fully.

### Levels of Intelligence
- When dealing with consuming or collecting intelligence on a network, intelligence comes in different forms and at different levels, depending on requirements given to an analyst.
- Consider the following examples:
  - The political motivations of a threat group.
  - A list of APTs assessed to have the capability and intent to target a mission partner’s network.
  - Domains tied to a specific APT for use in an Intrusion Detection System (IDS).
- Each example represents a different level of intelligence that a CPT may request or be required to fill.
- The three levels, or **taxonomic categories**, of intelligence are **strategic**, **operational**, and **tactical**.
  - The first example listed above represents strategic intelligence; the second, operational intelligence; and the third, tactical intelligence.

#### Strategic Intelligence
- Attacker **motivations**, **politics**, or **trends** that **inform an organization’s policy decisions and planning** represent the **strategic** level of intelligence.
- Often, strategic intelligence is the **trigger for the start of an operation** or a **change to network architecture**.

#### Operational Intelligence
- Attacker **Tactics**, **Techniques**, and **Procedures** (**TTP**) and **capability** are typically viewed as **operational** intelligence.
- This intelligence **informs collection plans** and **mission-planning efforts** **prior to an incident response or hunt engagement**.

#### Tactical Intelligence
- **Immediately applicable intelligence**, such as Indicators of Compromise (**IOC**), **antivirus signatures**, or **IDS rules**, fall into the **tactical** level of intelligence.
- This intelligence is **most useful for CDAs** on a mission **during on-network engagements and retroactive investigations**.

### Strategic Intelligence
- Cyberspace does not exist in a vacuum.
- Strategic intelligence deals with **external**, and **often global**, **influences** on the cyber threat landscape.
- The following are example intelligence requirements that pertain to the strategic level of intelligence:
  - Based on new political developments in the region, what threat groups may be motivated to attack the mission partner’s network?
  - Which trends in cybersecurity indicate a threat to the mission partner’s attack surface?
  - Should active cyber threats dictate a change in collaboration software for the organization?

- As illustrated in these examples, **strategic-level intelligence** deals with **threat actor trends and motivations** and **informs policy decisions**.
- Strategic-level questions have **real financial impact** and may **inform the movement of defense efforts**.
- This intelligence can assist all analysts by **giving context to defense efforts**, even at the most basic and direct levels.

#### Application of Strategic CTI
- During a CPT mission or planning of such a mission, **strategic intelligence** plays an important role in **comprehensively understanding the network environment**.
- In fact, a subset of strategic intelligence called **warning intelligence** is one of the **three main triggers for CPT employment**. (The other two triggers are **campaign planning and detected malicious cyber activity**.)
- Additional characteristics of strategic CTI are as follows:
  - Strategic CTI provides a comprehensive view of the target or attacker, to include motivations, resources, and capability.
    - This can come in the form of target profiles, briefings, or reports.
  - Strategic CTI seeks to predict future attacks based on trends and other forms of intelligence collection.
  - Strategic CTI provides input to planning and policy development.
    - Sometimes it can inform policies on mission partner networks as a by-product of a mission’s final report.
- This all adds up to the explanation of what a CDA should be worried about — or, more specifically, whom a CDA should be worried about — when defending a network.

#### Who Is the Enemy?
- Strategic intelligence boils down to one big question: **“Who is the enemy?”**
- This is not a question to completely craft collection plans around, but it is an **intelligence requirement** that is **fundamental to every decision made** when it comes to **employing CPTs** and other defense teams.
  - For example, assume that a specific organization’s most likely enemy, based on strategic-level intelligence, is motivated by financial gain and is known to sell stolen intellectual property.
  - Knowing nothing else about the network or organization, how would that inform mission planning or network architecture decisions?
    - If the organization in this example is attacked by the most likely enemy, the following is true:
      - Data exfiltration of intellectual property is likely.
      - The servers with the most valuable information are most likely the targets.
      - The attack comes from outside the organization (so the enemy is not an insider).
- These assumptions and intelligence most likely lead the organization to invest in greater protection of the sensitive data or servers and possibly to consider a standalone network for valuable intellectual property.
- The above conclusions from the example intelligence would likely drive follow-on requirements to narrow down TTPs or IOCs for analysis.

### Attribution to the Enemy
- **Attribution is not the job of a CDA**.
- However, attribution is an **important aspect of CTI**.
- Knowing (or assessing) who attacked, or will attack, a network is essential to policy makers and planners.
- Having no idea who attacked a network makes it impossible to track campaigns or anticipate future attacks from any particular entity.
- Furthermore, CDAs can use existing attribution information to more quickly identify and counter a threat actor on a defended network.
- Even though attribution is not the responsibility of a CDA, it is **important to know how CDAs contribute to the process** and **how attribution decisions are made by the IC**.
- By knowing the steps and information needed to attribute an APT, CDAs can distinguish between the threat groups more easily. 

#### Threat Group Tracking
- The concept of a threat group is often misleading.
- What makes up a threat group or APT?
  - Sometimes they are tied and attributed to government agencies, sometimes they are crime organizations, and sometimes they are synonymous with the malware they are known to use.
  - An even more perplexing question is: How is tracking a threat group possible? Why don’t APTs simply change their tactics after every attack?
- **Threat groups**, as a concept, are a **conglomerate of tendencies and identifiable tactics** that **allow for researchers and intelligence analysts to identify patterns**.
  - These patterns are possible, and necessary, for threat groups to operate.
  - After all, when APTs are thought of as someone’s job, patterns make sense.
  - For example, a fast-food restaurant does not serve different hamburgers if someone calls in sick. Threat groups operate similarly.
  - To achieve repeatable success with a team of operators, a repeatable process needs to be implemented. That repeatable process becomes a trackable threat group.
  - In fact, most tracked threat groups are an abstraction and primarily equate to a set of instructions that humans follow, not to the humans themselves.

- Attribution of APTs to organizations that exist outside cyberspace is more challenging than identifying patterns and tactics.
- **Motives**, **politics**, and other **non-technical attributes** need to be analyzed to make that kind of determination. 
- **Tracking threat groups, from a CDA perspective**, is important because it **creates sets of TTPs** that can be **searched** and **planned** for on a network.
- Without having a predictable set of behaviors to search for, analysts are left with virtually infinite combinations of TTPs to plan for.

#### What’s in a Name?
- **Naming APTs or threat groups** is necessary and is a **standard practice for every organization** that tracks them.
- Being able to identify a group with a **short name** is obviously **preferable** to using something like “that group that uses phishing and password spraying and is assessed to be from East Asia.”

- However useful the naming of threat groups may be, every entity that tracks these groups names the groups based on their own criteria.
  - Vendor A’s tracking of a group is not necessarily the same as Vendor B’s.
  - A common misconception of the CTI community is that direct equivalencies exist between threat groups tracked by different vendors.
  - There is a reason for the lack of equivalency: **perspective**.
  - For example, consider the following scenario.
    - Imagine two CTI entities looking at the same attack: one is a vendor who specializes in host-based antivirus, and another is a vendor specializing in firewalls.
    - They each have their own telemetry and data on the incident.
    - It is reasonable to think that the antivirus vendor focuses on the malware signatures on the host and the firewall vendor has a deep understanding of the C2 traffic leaving the network.
    - The use of the same malware in a follow-on attack, but with modified C2 patterns, may cause the firewall vendor to attribute these two attacks to different groups but the antivirus to continue attributing to the original group.
  - This is an unrealistically simplified example, but it illustrates how viewing threat groups from different perspectives causes attribution of APTs to diverge over time.
  - Zooming the example out even more, consider entities that deal in CTI but are not vendors.
  - Such entities as cybersecurity vendors are quite interested in detectable attributes that can protect their customers’ networks, but entities like government agencies tend to be more interested in attribution to humans or other governments.
  - This tends to make attribution by government agencies lean more toward nontechnical attributes of a threat group.
  - CPTs have to take all these perspectives into account when determining which TTPs and APTs to target.
- Keeping this naming and tracking perspective in mind, security researchers and intelligence analysts use a standard framework for attribution decisions: **Malware**, **Infrastructure**, **Control Server**, **Telemetry**, **Intelligence**, and **Cui Bono**, or **MICTIC**.

### MICTIC Framework
#### Overview
- CDAs need to know what type of information is helpful for the IC to have to make attribution decisions so that it can be collected while on mission.
- It is also important to know that a single artifact is often not enough to make official attribution decisions.
- The MICTIC Framework, as described in Timo Steffens’ book Attribution of Advanced Persistent Threats: How to Identify the Actors Behind Cyber-Espionage, is a standard method to tie attack campaigns and attributes to threat groups or sometimes even declare a brand new threat group discovered.
- The MICTIC Framework comprises six evidence categories of an attribution decision:
  - **Malware**
    - Intelligence analysts can track and attribute APTs based on the malware they employ on victim networks.
    - This may seem obvious. However, taking things a step further, some APTs outsource the authoring of malware for their own use.
    - That relationship between the author and the entity that employs the malware is also good evidence for attribution.
    - It may also assist in identifying nefarious relationships between malware authors and multiple APTs.
    - Evidence that CDAs can provide includes **malware samples** and **sandbox reports**.
  - **Infrastructure**
    - The pathways that C2 takes to and from victim networks provide a good indication of who is attacking a network.
    - In fact, some larger nation-state APTs have teams with the sole purpose of setting up infrastructure.
    - By investigating things like **domain name registration** and **IP address ownership**, a pattern may emerge that points to known bad actors.
    - Evidence that CDAs can provide includes **packet captures**, **domain names**, **IP addresses**, and **certificate information**.
  - **Control server**
    - Typically, the Control Server evidence is seized by law enforcement or government agencies from attacker-owned or -rented servers.
    - Certain things like language **settings**, **certificates**, **user accounts**, and **saved files** are quite useful in positively identifying not only threat groups but also human operators.
    - Evidence that CDAs can provide includes **packet captures to C2 servers from victim networks**.
  - **Telemetry**
    - Information logged from **inside victim networks**, such as **command-line logs**, **IDS alerts**, and **antivirus messages**, provide analysts with a **treasure trove of information** useful for attribution.
    - For example, evidence of attacker interactivity on a network that happens only during working hours in Moscow and stops on Russian national holidays might be a good indication the threat group is based in Russia.
    - Evidence that CDAs can provide includes **log information ingested into analysis platforms** and **crew logs indicating observed attacker behavior**.
  - **Intelligence**
    - Of the six evidence categories in the MICTIC Framework, **Intelligence** and **Cui Bono** are the two that **CDAs have little to do with**.
    - **Governments** and **researchers** have the ability to **aggregate many pieces of information** and **collect intelligence that may indicate an attacker's identity** outside a victim’s network.
    - **Intelligence** may be from a **human source**, **intercepted communications**, or any other **traditional intelligence source**.
    - **Cui Bono**
      - An **analysis** of **who would benefit** from the attack **politically**, **financially**, or otherwise.

#### MICTIC Application
- By analyzing each evidence type and comparing it to a known set of APTs, researchers and intelligence analysts can make informed attribution decisions.
- Some attributions are made with more confidence, whereas others may be made with less confidence but still show ties and similarities between attacks.
- Again, making attribution decisions is not a CDA’s job.
- It is, however, highly important from a strategic standpoint for leaders to assess who has attacked, or might attack, in the future.
- Also, **attribution** allows for an analyst to **more effectively plan for**, or **quickly react to**, active **intrusions** on **mission partner networks**. 


### Operational Intelligence
- **Strategic intelligence**, by definition, is **long term** and, as illustrated in the previous section, **takes considerable effort to track and apply**.
- Once stakeholders and analysts are aware of the strategic intelligence for their operating environment, more granularity is needed for operationalizing that intelligence.

- **Operational intelligence** is **higher-level information** on **campaigns** and threat actor **TTPs**.
  - It deals with more **immediately applicable intelligence** to **mission planning and operations**.
  - For example, **strategic intelligence** may **indicate an APT** to initiate a hunt mission for, but **operational intelligence** provides the **TTPs** and **capabilities** of that APT to plan the mission around.

- As each level of intelligence is explained, strong connections between each level are evident.
- For example, operational intelligence on TTPs of attackers is not possible without first using strategic intelligence to attribute and anticipate the adversary.
- Likewise, tactical intelligence relies on operational intelligence to determine what directly pertains to a defended terrain.

- Operational intelligence is **high-level**, but **short-term**, **intelligence**.
- It does not get into IOCs or specific commands an attacker might run on a victim network, but it does **deal with fluid topics like TTPs**.
- It is **observable** and relates to **specific attacks or attack campaigns**, as opposed to the broader approach that strategic intelligence provides.

#### Intelligence in Operations
- Attacker capabilities and intent are the cornerstones of operational intelligence.
- After identifying who the enemy is, the next logical questions asked are “What can the enemy do?” and “What does the enemy want?”
- Based on intelligence and data collected from previous attacks, repeated patterns related to TTPs and actions on objectives by the attacker can give analysts in CPTs insight into what to expect while on mission.

#### Attacker Capabilities
- The capabilities of an APT directly relate to the resources they have available.
- Do they create their own custom malware? Are they able to purchase hundreds of domains on the internet? Do they enhance their operations with their own intelligence collection?
- APTs with the most resources will obviously have the most advanced capabilities.
- When defending against or hunting for attackers with advanced capabilities, it is reasonable to assume that traditional methods of detecting malicious activity on a network will not work.
  - As an example, for a well-resourced APT, antivirus will most likely be ineffective. 
- Similarly, detecting attackers with advanced capabilities is resource intensive.
- Resource intensive might mean it is **expensive**, requires **deeper analysis**, or takes **more time to detect**.
- This creates an imbalance if a highly capable attacker targets a resource stretched organization.

#### Attacker Intent
- Knowing or assessing what an attacker is attempting to accomplish on a victim network makes defending against that attack possible.
- Operational intelligence seeks to determine what an attacker or attack campaign is trying to accomplish based on strategic intelligence indicators.


### Applied TTPs
- Dealing with capabilities and intent goes only so far when planning the defense of a network.
- Knowing, for example, that an attack campaign indicates that an APT is using open-source malware and is altering valuable information on victim networks indicates attacker intent and capabilities.
  - However, this gives only vague information.
- There are more intelligence requirements to be met when planning a defense.
  - Specifically, how does the attacker operate?
- Consider the following, for example:
  - How does the attacker typically gain initial access?
  - What method is usually used for persistence on a host?
  - Will the CPT be able to see clear text communications in network traffic?
  - How much time passes between initial foothold and actions on objective?

- These intelligence requirements all point to TTPs.
- Knowing and mapping TTPs of an APT give an analyst something concrete to build their defense around.

- Using such tools as the **MITRE ATT&CK® framework**, intelligence analysts can effectively communicate operational level intelligence to a CDA in the form of TTPs.
- In essence, an **ATT&CK matrix describing an APT is that threat group’s fingerprint**.
- After all, as discussed early in this lesson, threat groups are merely an abstraction of linked TTPs.

- **Combining APT capability**, **intent**, and **known TTPs** can provide a CPT with the following information:
  - The amount of resources needed to defend a network.
  - Where on the network will be targeted.
  - How the attacker will attempt to accomplish their goal.
- Absent specific IOCs and signatures, this is enough information for a CPT to plan an entire operation around. 


### Tactical Intelligence
- In cyber defense, tactical intelligence is low-level and short-lived intelligence that allows direct response to cyber threats.
- As detailed in the sports example above, it is also the **easiest intelligence type for an attacker to alter**.
- Its perishable nature requires that it is refreshed often, but accurate and timely tactical intelligence can prove invaluable in an **active situation** or **retroactive investigation**.

#### Uses for Tactical Intelligence
- In a cyber defense application, tactical intelligence can be applied directly to an analysis platform and other tools that a CDA uses to interact with an environment.
- Most notably, it is conveyed in the form of IOC.
- Other forms of tactical intelligence are exact commands run on a system by an attacker or other observable tendencies like usernames or network traffic signatures.
- Tactical intelligence has two main uses: **active hunting or engagements** and **retroactive investigations**.
  - Using it during an active engagement can assist CDAs in directly countering a threat.
  - Tactical intelligence during retroactive investigations can provide evidence that malicious activity occurred.

#### Active Hunting
- Using IOCs, IDS signatures, and other readily applicable intelligence on a network during an active intrusion or hunt can deliver quick wins for an analyst.
- By using intelligence on active campaigns or attacks currently happening in cyberspace, relevant indicators can be ingested into an analysis tool for low-effort discovery and countering of attackers on a network.

- However, challenges exist with tactical intelligence in an active environment.
- One challenge is that tactical indicators are easy for attackers to change.
  - By accumulating a large number of IOCs from online repositories or internal intelligence databases, the processing and storage overhead is often not commensurate with the value of the data.
- Another challenge is that commonly, by the time tactical intelligence becomes processed and disseminated, it has already become stale.
  - Also, related to that point, many attackers change their identifiable tactical level signatures between every attack.

#### Retroactive Investigations
- Using known IOCs and other observables when reviewing logs for an investigation of a confirmed attack can be highly valuable.
- The inherent issues with tactical-level intelligence applied to active situations do not exist with retroactive forensics.
- In fact, stale IOCs may be more useful than current information.
- Looking back in time using known indicators may reveal more and provide for a more complete picture of the incident because the more time that passes, the more likely that information will be available for intelligence analysts to disseminate.


### RFI Process
- The process of requesting information or intelligence from the IC or a commercial intelligence team may be formalized, depending on the assigned unit, command, or organization.
- Typically, this function is performed by the ASA in a CPT, but the act of initiating a requirement may be the responsibility of a CDA.
- Although CPTs have assigned intelligence personnel, certain research and information gathering tasks still fall on CDAs.
- This section introduces the process of submitting an RFI and how to interpret its response.

#### The RFI Submission
- To receive specific intelligence about any particular topic or mission, a request must be submitted to the IC.
- Typically, submitting something to the IC means that an intelligence analyst assigned to a higher command does research on the topic and submits a response.
- Stating “Submitted to the IC” really means that it is given to another analyst or team of analysts to query for information on another system.
- If the intelligence requested does not exist already to answer a requirement, the Intelligence Cycle begins, with the RFI serving as the direction.
- Understandably, the entire Intelligence Cycle may take some time to complete, so it is important to be flexible but up front if the due date is swiftly approaching.

#### RFI Criteria
- The request itself should meet certain criteria.
- The IC expects certain items, as follows, to be included in order to respond with a useful answer.

#### Requestor/Requestee
- The RFI should indicate who requested the information, along with good contact information for any follow-up questions or clarifications needed by the intelligence analysts working the request.
- An alternate contact for the request is also helpful, especially in time-sensitive requests.

#### Request
- A request is a straightforward, simple question or requirement that needs to be answered.
- There is an art to crafting a request that leads to an appropriate response.
- After all, intelligence analysts are not mind readers.
- CDAs should be as precise and direct as possible and include an example of the response they are expecting.

#### Due Date
- Depending on where the RFI is addressed, it could be competing with a long list of other requests. Due dates are used to prioritize competing requests.

#### References
- For intelligence analysts to expedite research, it is important that they know which resources have already been checked.


## Threat Actor Research
### CTI Models and Requests for Information
#### CTI Models
- CTI data includes **tools**, **exploitation strategies**, **victims**, **strategic priorities**, **technical and organizational mechanisms**, **cyber infrastructure information**, and other factors that build an understanding of threat actors.
- CTI models are used by cybersecurity personnel to organize data about those threat actors.
- By gathering, processing, and studying CTI, CDAs can make decisions more quickly and accurately. 

- Once CTI is obtained, it can be organized into a CTI model.
- Three common CTI models are as follows:
  - **MITRE ATT&CK**: A knowledge-based CTI model that has users **match TTPs with sequential stages** in the ATT&CK matrix.
  - **Cyber Kill Chain®**: A linear series of stages, derived from a similar model used by the military, that depicts the **steps in a cyber attack**.
  - **Diamond Model**: A model in which, as the name suggests, **incidents are depicted as a diamond**.
    - Unlike other models, the Diamond Model **emphasizes relationships** between components of an attack.
- Although these three models are not the only CTI models used in cybersecurity, they represent some of the most commonly used and prolific ones.
- Professionals use other models, such as Find, Fix, Finish, Exploit, Analyze, Dissemination (**F3EAD**); the Observe, Orient, Decide, Act (**OODA**) loop; and the **Intelligence Cycle**, to organize information as well. 

- **TTP** is a catchall term for describing the **behaviors and tools of a cyber threat actor**, and TTPs **represent a significant topic** when discussing CTI models.
- By identifying the TTPs of an (APT) or other attacker, security analysts may better understand the threat as well as predict future cyber attacks and advise on appropriate mitigating controls.

#### Requests for Information
- An RFI is a term that describes the process of acquiring information about intelligence from intelligence analysts.
- An ideal RFI should contain at least the following aspects:
  - The names of those who requested the information and a backup contact in the event the primary person is not available.
  - A precise description of the information needed.
    - If the request is too broad or vague, the returned intelligence might not cover the depth of information needed.
  - A list of sources that has been checked for information prior to creating the RFI.
    - This helps the analyst avoid receiving information they already possess.
  - A due date. 
- Although these are general requirements, the RFI may change, depending on the specific scenario.

### MITRE AT&CK Matrix
- In 2013, MITRE introduced the ATT&CK matrix, a knowledge-based CTI model.
- This utility allows users to categorize threat actor behaviors based on real-world data.
- By understanding and using the ATT&CK matrix, analysts gain a greater perspective of attacker behaviors and become better prepared for mapping defensive controls in their network.
- Additionally, ATT&CK provides a common language for communication between internal security teams and other personnel outside the organization. 

- To use the ATT&CK matrix, open the ATT&CK website, https://attack.mitre.org, in a web browser.
- Matrices for mobile devices and Industrial Control Systems (ICS) also exist, but for this lesson, the Enterprise Matrix is used.
- The matrix is updated periodically as the cybersecurity landscape changes; as of early 2022, the matrix has 14 Tactics categories.
- A tactic, which is the goal of an adversary’s action, is the broadest component in the matrix.
- Each tactic includes several associated Techniques.
- A technique represents the actions taken by a threat actor to accomplish their tactic.
- Each technique may have associated Sub-Techniques, as a greater degree of granularity may help categorize some malicious behaviors.
- Finally, each technique or sub-technique contains Procedures, which are specific examples of how an adversary carries out their technique or sub-technique.
- Below is the hierarchy within the ATT&CK matrix:
  ![45668aeb-a7c4-4089-9338-26ce2613efa5](https://github.com/user-attachments/assets/88c79efb-55bb-4ccb-b36a-54efa972d9bf)

#### Bandook and ATT&CK
- By cybersecurity standards, the Bandook Remote Access Tool (RAT) is an ancient piece of malware.
- Having emerged in 2007, this malware is written in C++ and Delphi and has been used in cyber attacks against various sectors globally.
- Although its use subsided for some time, such attacks as Operation Manul (2015) and such groups such as Dark Caracal (2017) have brought back some of its popularity.

- Bandook is primarily distributed via phishing emails containing an infected Microsoft Word document with a trojan embedded inside.
- To increase the chances of successful execution, the malicious document instructs the victim to enable macros.
- Once enabled and opened, the malicious macros are downloaded to the victim’s machine.
- Finally, the macro executes a PowerShell payload that downloads and executes the Bandook backdoor disguised as Internet Explorer.
- At that point, the Command-and-Control (C2) server, controlled by the attacker, can issue commands to the Bandook backdoor.
- Examples of attacker capabilities include taking **screenshots**; **uploading**, **downloading**, and **executing files**; and **processing shell** **commands**. 

### Cyber Kill Chain
- The Cyber Kill Chain®, first released by Lockheed Martin Corporation in 2011, is a CTI model that uses a seven-stage timeline, placing a greater emphasis than the ATT&CK matrix or the Diamond Model on the processes and sequence of events in an attack.
- The objective of using the Cyber Kill Chain is to first identify the current stage of the attack and then attempt to disrupt the current “link” of the chain.
- This causes a cascading effect on the adversary.
- An important, additional distinction of the Cyber Kill Chain is its focus on APTs.
- The stages of the Cyber Kill Chain are illustrated in Figure 6.2-2 and summarized as follows.
  ![5ddbc0c0-3276-4713-8f2f-56d7afc2c9f9](https://github.com/user-attachments/assets/3ba8ed58-8153-4c97-a230-2765398c763b)
1. **Reconnaissance**: The first stage of the Cyber Kill Chain, involving multiple processes of information gathering, both Open-Source Intelligence (**OSINT**) and possible **social engineering**.
2. **Weaponization**: A pre-attack stage in which the adversary composes the exploit and vulnerability.
3. **Delivery**: The stage in which the threat actor initiates the first step of the attack. The exploit is delivered to the target via email, Server Message Block (SMB), physically, or via other methods.
4. **Exploitation**: The stage in which the exploit has been delivered and the adversary executes the attack on the target systems.
5. **Installation**: The stage in which the system has been compromised and the attacker places persistence malware on the target.
6. **C2**: The stage in which the adversary establishes communication between a malicious server and the victim’s systems. This allows for backdoor commands and communications to be exchanged.
7. **Actions on Objectives**: The final stage of the Cyber Kill Chain, comprising any activities after the initial attack, aiming to further the interests of the adversary. This stage is relatively open ended. 

- The use of the Cyber Kill Chain is linear, focusing on the attack narrative.
- When using the Cyber Kill Chain, it is important to have a clearly defined stage for any event in a cyber attack, as detecting activity in one stage of the Cyber Kill Chain allows network defenders to prepare for the next actions of the adversary.
- Even if the attack has taken place in the past, using the Cyber Kill Chain structure provides guidance to forensic personnel in determining the previous malicious activities. 

### Diamond Model
![fc88a68e-0e47-41c5-bd0b-580c1082f2fa](https://github.com/user-attachments/assets/1e854a4b-e5d2-4c61-97a7-e7c5a278740c)

- A third CTI model is the Diamond Model of Intrusion Analysis, or, more simply, the **Diamond Model**.
- According to the Diamond Model, every cyber incident may be depicted as a diamond with four “points,” as illustrated in Figure 6.2-3 and described as follows:
  - **Adversary**: The threat actor responsible for the cyber attack or any other malicious activity.
  - **Capabilities**: The TTPs associated with the adversary.
  - **Infrastructure**: The adversary’s assets, both physical and logical, that are leveraged in their operations.
  - **Victim**: The target of attacks, which may be organizations, individuals, or specific vulnerabilities.
- Unlike the ATT&CK and Cyber Kill Chain models, the Diamond Model emphasizes the relationships of each component in the model.
  - The relationships of the four points of the diamond may be analyzed, and greater insight into the threat actor may be gained.
  - These points may be converged into a single phrase:
    - **“An [ADVERSARY] uses a [CAPABILITY] over a(n) [INFRASTRUCTURE] against a [VICTIM].”**

- This is useful, as it illustrates how exactly an attacker employs their skills against a certain victim.
- The **vertical** and **horizontal** axes are important parts of the Diamond Model.
  - The **vertical axis**, connecting the Adversary and Victim components, depicts the **sociopolitical axis**.
    - This axis shows the **reasons and goals for the adversary’s taking action against the victim**.
  - The **horizontal axis**, connecting the Capabilities and Infrastructure components, depicts the **technical axis**.
    - This axis shows the **resources and technological** aspects of the attack.
  - Plotting data into the Diamond Model, unknown information can present itself.
    - For example, knowing the IP address of a C2 domain may indicate the identity of an adversary.
    - The more points of the diamond that have information, the easier it is to infer knowledge of the missing pieces. 

- This gives analysts the ability to look at future attacks and see how behaviors of the past can indicate potential future actions.
- The Diamond Model also offers the ability to discover knowledge gaps, as plotting data into the model can help with highlighting missing information more clearly.
- Finally, the Diamond Model can be incorporated into mitigation planning and security frameworks. 

- Using the same scenario from the Cyber Kill Chain section, a Diamond Model can be created and filled with information, as the following example shows:
  - **Adversary**: Currently unknown.
  - **Capabilities**: Phishing, FTP exploit, logic bombs, persistence malware, credit card information theft.
  - **Infrastructure**: C2 server (IP and registered domain name), email addresses.
  - **Victim**: A financial institution, vulnerable FTP server.
- In the above example, the Adversary component of the Diamond Model is unknown.
- However, by using the information gathered about the capabilities, infrastructure, and victim, it can be reasonably surmised that the adversary is a group of attackers who aim to create financial gain for themselves.
- Through forensic examination of attacker email addresses, domain names, coding styles, IP addresses, and more, this adversarial persona may be further defined.  

### Comparing CTI Models
![94406738-e054-4695-9677-c68c89e5e7dd](https://github.com/user-attachments/assets/1a93d62b-efbd-4fd3-be1c-d3505c6206f4)


## MITRE ATT&CK FRAMEWORK
### ATT&CK Matrix | Techniques
- **Reconnaissance**: Techniques that involve adversaries actively or passively gathering information that can be used to support targeting (for example, staff/personnel, infrastructure).
- **Resource Development**: Techniques that involve adversaries creating, purchasing, or compromising/stealing resources that can be used to support targeting.
- **Initial Access**: Techniques that use various entry vectors to gain an initial foothold within a network (for example, a spear-phishing link).
- **Execution**: Techniques that result in running attacker-controlled code on a local or remote system (for example, PowerShell).
- **Persistence**: Techniques used to maintain persistent access to a system (for example, logon scripts).
- **Privilege Escalation**: Techniques used to gain higher-level privileges on a system or network (for example, process injection).
- **Defense Evasion**: Techniques used to avoid detection (for example, Dynamic Linked Library [DLL] side-loading).
- **Credential Access**: Techniques for stealing credentials, such as account names and passwords (for example, Kerberoasting).
- **Discovery**: Techniques used to gain knowledge about the system and internal network (for example, network sniffing).
- **Lateral Movement**: Techniques used to enter and control remote systems on a network from the already compromised host (for example, Pass the Ticket).
  - Attackers typically have to pivot through multiple machines — usually the weakest link in the chain of machines — to ultimately reach their end objective.
- **Collection**: Techniques used to gather information relevant to following through on the attacker’s objectives (for example, input capture).
- Command and Control (**C2**): Techniques attackers may use to communicate with systems under their control; often disguised to look like normal Hypertext Transport Protocol (HTTP) traffic (for example, domain fronting).
- **Exfiltration**: Techniques used to steal data from a network (for example, exfiltration over web service).
- **Impact**: Techniques used to disrupt availability or compromise integrity by manipulating business and operational processes (for example, firmware corruption).
  - Impact is the result on a system after the attacker accomplishes their ultimate goal and is the most recently added tactic.


## Threat Mitigation
### Cyber Kill Chain
#### Phase 1: Reconnaissance
- The first phase of the kill chain, Reconnaissance, covers initial information gathering before the adversary begins an attack.
- Reconnaissance can be leveraged in an attack cycle in several ways, from identifying potential access vectors to determining the most viable target for conducting an attack.
- When attackers survey a target, they look for as much information as possible, such as network technologies used and services exposed.
- Network technologies include unique **protocols**, **hardware**, and **software** standards such as **optical**, **broadband**, and **radio**, which all provide varying access vectors.
- Examples of services that may be exposed to publicly available networks or even the internet include the following:
  - File Transfer Protocol (**FTP**)
  - Simple Mail Transfer Protocol (**SMTP**)
  - Remote Desktop Protocol (**RDP**)
  - Telnet

- Examples of key data elements that an attacker can leverage that may be gathered by initial reconnaissance methods include the following:
  - Enumeration (list, identify, and research) of internet-facing assets.
  - Internet Protocol (IP) ranges and associated domain names, ports, and services.
  - Implemented technologies or application platforms.
  - Leaked credentials or keys.
  - Leaked documents, logs, configuration files, and backup files.

- One type of attack reliant on the Reconnaissance phase is spear-phishing.
- Through reconnaissance, the adversary can gain knowledge through press releases and an organization’s or employee’s social media presence.
- This can aid attackers in identifying persons to target and topics to address ineffective phishing messages that are likely to trick victims.

- Threat actors can aggregate actionable data about a target entity through the use of Open Source Intelligence (OSINT) collection.
- **OSINT** is the method of **gathering information through publicly available sources** on the internet. 

- Three types of information gathering exist in security:
  - **passive**
    - Passive OSINT techniques are those that do not engage or interact with the target entity.
    - The requirement of these methods is that no artifacts or traces of the queries are left behind for the target to discover.
    - Techniques of passive reconnaissance include the following:
      - Collecting public information and records regarding network or person.
      - Aggregating unprotected social media information.
      - Recovering previous versions of the target's website (for example, via WayBack Machine).
      - Querying such search engines as Google or Shodan for exposed devices or services belonging to the target.
  - **semi-passive**
    - Semi-passive OSINT techniques directly interface with the target but in a way that is plausibly deniable and blends in with standard traffic.
    - Techniques may include interacting with a target's website or other online services but blending into the white noise of normal traffic and performing all actions from a non-attributable IP/domain.
    - If the attacker is detected in a later phase of the Cyber Kill Chain by the target, all evidence of these methods would be non-attributable.
  - **active**.
    - Active OSINT techniques are methods that directly interface with a target and can leave an identifiable signature.
    - An example of an active OSINT technique is a port scan.
    - Conducting a port scan rapidly queries available ports on a target and can provide a great deal of information to aid in an attack.
    - However, many modern security systems are configured to detect such activity and flag it as malicious.

#### Phase 2: Weaponization
- After identifying a viable entry into a target network, an attacker enters the Weaponization phase, in which they develop their attack with the goal of ensuring an initial point of presence in the victim’s network and enabling additional access.
- The activities that an attacker performs in this phase occur on a workstation or network controlled by the attacker, so a defender does not have a real-time view of the attacker's actions during this phase.
- The Weaponization phase may include an attacker crafting an enticing social engineering ploy to compel an employee to open an attachment or click a malicious link.
- Other times, the attacker may require a custom-crafted payload; in these cases, the Weaponization phase occurs when the parameters of this malware are **defined**, **compiled**, and **tested**.

- The Weaponization phase is informed by the results of reconnaissance.
- For example, with successful information gathering, a threat actor can determine the host endpoint protections in place in a victim's environment.
- At that point, upon compiling and testing their malicious payload, attempts can be made to evade that specific technology's detection.

- The Weaponization phase may take some time if there are special needs for the exploit or payloads that need to be developed.
- Advanced attacks may require research and development in an isolated lab environment prior to conducting an actual attack.
- Also, a series of unknowns exist when going into any remote cyber attack; even if the exploit works, a chance exists that the payload may fail and need to be reworked.

- To exploit an identified vulnerability in the target's network, an attacker uses private, custom, or open-source exploits.
- In any case, the actor needs to weaponize their tool to match the unique system and network architecture requirements of the target they intend to attack.
- In addition, they usually implement obfuscation techniques to evade network and host detection capabilities.

- There are no guaranteed defenses against some weaponization techniques, such as with a zero-day technique.

- Examples of weaponization techniques include the following:
  - **Obfuscate** (for **example**, **encrypt**, **encode**, or **reorder**) a malicious executable to evade antivirus (AV) detection.
  - **Modify** open-source exploits to target a specific target's IP and security configuration.
  - **Build** a novel exploit against a target device or platform (**zero-day**).

#### Phase 3: Delivery
- In the Delivery phase, an attacker attempts to circumvent any controls in the defended network to deliver their weaponized payload to their target.
- A host of TTPs exists that an adversary may use to deliver their exploit.
- The chosen TTP is dependent on information gained about the target from the Reconnaissance phase.
- For example, the attacker may identify a vulnerable web server in the target’s network and then deliver the exploit by sending specially crafted packets over the internet.
- Or the attacker may learn that their target regularly visits a certain website and therefore delivers their payload via a watering hole attack.
- Given the ubiquity of email, attackers most likely attempt to deliver their exploit via malicious attachment or to entice a user to click a link to prompt a malicious download via phishing. 

#### Phase 4: Exploitation
- The Exploitation phase takes advantage of an identified vulnerability in the target environment with the intent of triggering the attacker’s delivered code.
- The vulnerability could be technical and target a vulnerable application or Operating System (OS), or it could be non-technical and target the user.
- For a technical vulnerability, two exploit vectors generally exist:
  - **remote**
    - Remote exploits target network-enabled services and devices that are not properly patched or configured.
    - These exploits allow an attacker to achieve Remote Code Execution (RCE), where they can run code on a remote target.
    - Web servers, FTP servers, email servers, RDP, Server Message Block (SMB), and a host of other popular services all have been the target of RCE attacks.
  - **local**
    - Local exploits target local services and leverage attack methods to execute malware or escalate privileges.
    - An attacker needs to gain an initial foothold on their target to take advantage of a local exploit.

- Aside from the vector, exploits take advantage of three types of vulnerabilities:
  - **Public**: Publicly disclosed vulnerability that has an available patch.
  - **N-day**: Publicly disclosed vulnerability with no current patch.
  - **Zero-day**: Undisclosed vulnerability.

- The National Vulnerability Database (**NVD**) maintains a repository of publicly disclosed vulnerabilities alongside references that provide supplemental information related to the vulnerability, such as the **vendor advisory**, **technical details**, **workarounds**, and **mitigations**.
- Often, a vulnerability disclosure may lead to publicly available exploit code as attackers and security researchers analyze the vulnerability’s technical details or reverse-engineer the patch. 

- Undisclosed vulnerabilities (**zero-days**) have an inherently greater cost to exploit because they are **less readily available** and **difficult to discover**.
- Therefore, attackers exploit zero-days sparingly due to fear that the more they exploit zero-days, the more likely they are eventually detected.

#### Phase 5: Installation
- During the Installation phase, the attacker installs a Remote Access Trojan (**RAT**) or reconfigures a system to maintain persistence on a target machine.
- This way, should they want future access, an attacker does not have to repeat phases 1 through 4.

- To enable persistent access, an attacker must choose an appropriate technique that varies from target to target and may depend on their level of access.
- A normal user does not have the same privileges and access as a root or administrative user.
- Even the most adept attackers might have trouble picking the perfect approach for each situation.
- An attacker may choose to install malware such as a RAT.
- Alternatively, on a Windows machine, an attacker may simply add a remote desktop user if the service is available or leverage a similar technique with other remote management services, such as (WinRM).

- Non-malware-based installation on a Unix system could include adding an authorized Secure Shell (SSH) key to the target system, enabling the attacker to connect to the target remotely with SSH without needing a valid password.
- Additionally, many Unix systems come with Netcat, which can be used to open a port and listen for traffic.
- An attacker could daemonize (run in the background as a service) a Netcat connection on a given port, which provides the attacker on-demand Command and Control (C2).

- Because an attacker may want to maintain a presence for a long time, attackers try to masquerade their activities to make them appear legitimate or benign to users and security tools.
- Examples of masquerading include naming a RAT with a filename that matches the name or location of legitimate files or resources, adding a double extension in the filename to disguise the true file type (for example, .zip.exe), and modifying a malicious file’s metadata to match that of a legitimate file.

#### Phase 6: C2
- Regardless of the method of communication, the purpose of C2 is consistent: **maintain control of compromised assets to perform additional operations**.
- This requires three components:
  <img width="3334" height="776" alt="3d01eb95-23c4-41fa-bba0-3791b06fbd2a" src="https://github.com/user-attachments/assets/3880c7a0-61ac-4aa3-80e7-3c308920e23f" />

- Generally, C2 traffic conforms to one of two paradigms:
  - **scheduled**
    - Scheduled C2 generally involves an installed program or service that reaches out to an attacker-controlled resource for its management instructions.
    - This type of control requires the attacker to have procured or compromised another resource on the internet.
    - This C2 method through an outbound connection means that this type can be installed on devices that are not directly reachable from the internet (such as those behind a firewall).
  - **on-demand**.
    - On-demand C2 requires that the service or port be accessible to the attacker, which may not always be possible, depending on the network topology.
    - Installations that respond to on-demand C2 can include malware configured to listen on an otherwise-unused port or remote control services like RDP or SSH, if associated accounts have been created or compromised.

- The desirable attributes of a C2 channel are generally consistent and should include the following:
  - **Be Stealthy**: The data flow in the channel should be either hidden under encryption or obscured by mimicking normal network traffic through similar channels (or both).
  - **Be Indistinct**: The command source and victim machine should appear to be related so traffic between them does not stand out.
  - **Be Redundant**: The channel itself should be difficult to shut down or block if discovered.
- Rarely are all three attributes available to an attacker.
- For instance, access by SSH provides encryption, but SSH traffic from an unknown source through a network edge likely appears suspicious to network defenders.
- As with installation, knowing which C2 channel is appropriate for which environment is both an art and a science and might require an extensive amount of triage. 

#### Phase 7: Actions on Objectives
- If the attack was successful, the attacker then begins the next phase of the attack: Actions on Objectives.
- This phase is largely defined by the goal of the attacker’s campaign.
- During this phase, the attacker uses their C2 channel to control their target and achieve their purpose.
- Typical actions taken at this phase include **enumerating attached devices**, **credential harvesting**, **Active Directory (AD) compromise**, and **stealing sensitive files**.
- Extreme examples of next steps by threat actors in this final phase include disruption of network services and data destruction.
- Alternatively, the initially compromised host may just provide a pivot point to move laterally further into the target network.
- Depending on the objectives of the attacker, this phase may include direct “hands-on keyboard” interaction with the target. During this interactive portion of the attack, the attacker experiences a high risk of exposure; therefore, tradecraft is essential.
- Advanced Persistent Threats (APT) are skilled at hiding, whereas less-skilled attackers or those with a shorter timeline do not use stealth and may not care about being caught.

### Disrupting the Chain
- Each phase of the kill chain presents an opportunity for a defender to disrupt, deny, and degrade an adversary’s attack.
- If a defender can put a halt to an attack by enacting a mitigating control, it forces the attacker to either change tactics, move on to another target, or stop their attack.
- Ideally, an organization should strive to be a “hard target,” where the effort involved to penetrate their defenses convinces a would-be attacker to concentrate elsewhere.
- Furthermore, the earlier in the chain that a defender can detect and prevent an attack, the less impactful that attack is.
- All organizations wish they could have an endless budget and ability to hire and retain an outstanding security team to protect their networks.
- However, not all organizations are well equipped, in which case the organization needs to perform a proper terrain analysis and identify their key terrain.
- From there, the organization should put in the proper mitigating controls to thwart the attacker at each phase of the chain.
- A full rundown of how to perform a terrain analysis is beyond the scope of this lesson, but the next section examines a few examples of how to disrupt the chain at each phase.

#### Disrupting Reconnaissance
- Disrupting the Reconnaissance phase requires limiting the public exposure of information for an organization and its employees.
- The less an attacker can learn about a target through passive and active reconnaissance, the more difficult it is for them to craft their weapon.
- In addition, an organization that seems “boring” on the web is more likely to be passed over by an attacker than an organization that appears “flashy.”

- The most extreme defense is for an organization to have no web presence and to not allow their employees to share information about their roles or the company on social media platforms.
- For most businesses, however, it is a great benefit to have an online presence to advertise themselves, gain more business, and increase revenue.
- Therefore, it behooves an organization to find the appropriate balance between sharing too much and too little.

- There are sensible steps that an organization can take to combat passive reconnaissance.
- Some of these steps are as follows:
  - **Enforce a social media acceptable use policy**, which is an organizational policy that prescribes what information employees can share on the web (for example, social media profiles, blogs, wikis).
  - **Follow Operations Security (OPSEC) best practices** to prevent the adversary from observing or stealing sensitive or critical information.
  - **Restrict web crawlers**; that is, prevent indexing by search engine spiders, or limit their access to only certain web pages and content. 
- Inevitably, as an organization grows in size, it becomes exponentially more difficult to control the release of sensitive information.
  - Therefore, regular training of employees to be conscious of what they share online is needed, along with providing an appropriate reporting channel for employees to report concerns and potential violations.

- Whereas disrupting passive reconnaissance requires a human element, disrupting active reconnaissance typically takes a technical approach.
- Here an organization must try to limit their attack surface as much as possible and restrict an attacker’s ability to conduct scanning against their network.

- Some protective measures an organization can take to disrupt active reconnaissance are as follows:
  - Disable unused ports and services.
    - This limits the number of paths that an attacker can take to infiltrate a network.
    - Modify server error messages.
      - Reduce the amount of information that an attacker can glean from scanning.
    - Implement a firewall Intrusion Detection System/Intrusion Prevention System (IDS/IPS).
      - Provide early detection of scanning activity, and block connections from The Onion Router (Tor) and third-party Virtual Private Network (VPN) exit nodes.

#### Disrupting Weaponization
- It is impossible to stop an attacker from weaponizing their payload because this act occurs on the attacker's side.
- However, it is possible to defend against techniques used for weaponization. 

- The first and most effective defense against weaponization for an organization is to ensure that its systems are patched for known vulnerabilities.
- This prevents an attacker from coupling their payloads with known exploits.
- Patch management exponentially raises the difficulty for an attacker to penetrate an organization’s network by forcing them to research and develop novel exploits.
- An effective patch management process begins with having an accurate Asset Inventory Management System (AIMS), where version and patch information for each system and its components are stored and updated.

- In addition to patch management, an organization should create secure baselines, meaning that any unneeded capabilities should be disabled from the onset.
- For example, attackers commonly use Microsoft Office macros as a technique for weaponizing. An effective mitigation is to configure a system to disable macros as part of a secure baseline.
- Other configuration settings that should be modified or hardened include the following:
  - **Disabling browser plugins**.
  - **Disabling Microsoft Office add-ins**.
  - **Enabling behavior prevention on endpoints**, such as Attack Surface Reduction (**ASR**) on Windows 10.

- Weaponization, along with later phases in the kill chain, can also be disrupted with the addition of defensive tools.
- This helps an organization to create defense-in-depth.
- For many attackers, the act of weaponizing their payload is done with an automated tool, or weaponizer.
- Unless the attacker has the resources to spend in creating their own weaponizer, they likely use a publicly available one.
- Well-known, publicly available weaponizers include the Veil Framework, Social Engineering Toolkit, Burp Suite, and Metasploit.
- Generally, every weaponizer, even custom-made ones, leaves artifacts and patterns in their use that can be detected by an IDS or prevented by an IPS.
- Organizations should ensure that they regularly update the signatures for these security tools.

#### Disrupting Delivery
- Once an attacker has weaponized their payload, they deliver it into the targeted organization’s network.
- Numerous technical controls can be put in place to disrupt delivery, regardless of the vector used by the attacker.
- Similar to the Weaponization phase, organizations should invest in adding defensive layers with IDS and IPS tools on the host and network side.

- An effective mitigation to disrupt delivery by web is to employ web filtering via a secure web gateway or web proxy.
- Security technology has greatly improved for this control type over the years.
- Many solutions in this market today can statically and dynamically analyze web pages for malicious content and then make an informed decision to allow or disallow the traffic to an employee’s web browser. 

- Because email is such a popular delivery mechanism for an attacker, it is highly recommended that an organization put in measures to inspect emails before the messages make it to their intended recipient.
- One technique that attackers do to add legitimacy to their email phishing attacks is to spoof the sending address.
- To combat this, it is possible to configure email authentication methods like DomainKeys Identified Mail (**DKIM**) and Sender Policy Framework (**SPF**).
- A phishing email may contain a malicious attachment for the user to open or a Uniform Resource Locator (URL) for a recipient to click on.
- Solutions are available that can automatically detonate the attachment or URL in a malware sandbox and report the analysis.
- To combat malicious attachments delivered via email, an organization, if possible, should create an allowlist of acceptable file extensions and deny or strip any attachment that does not meet that criterion.
- It is also common practice for URLs to be sanitized to prevent employees from automatically clicking the links without first inspecting them or to even direct the link to an isolated web browser to prevent possible infection. 

- Organizations, particularly ones in air-gapped networks, should be prepared to defend against malware delivered physically via removable media (for example, Universal Serial Bus (USB), CD/DVD, or Secure Digital [SD] cards).
- The most effective approach to deter this threat is to disable the use of removable media altogether unless allowed through exception.
- When allowed through exception, it is recommended to check for the presence of malware by scanning the device on an isolated workstation prior to inserting it into the intended system.

#### Disrupting Exploitation
- The Exploitation phase occurs once the attacker’s malware has slipped through all the defenses.
- This is when the intent is to run the delivered code.
- Because most of this phase is dependent on the techniques used during the Weaponization phase, many of the same defenses apply.
- These defenses include **patch management**, **configuration of secure baselines**, and **addition of tools** that could help detect and prevent exploitation attempts.
- Those steps help mitigate against exploit attempts against the system, but what about exploit attempts against the user?

- For phishing attacks attempting to exploit a user, a critical defense is user awareness.
- Employees must be regularly trained so they know how to spot a potential phish, whether it comes from email, Short Message Service (SMS), web delivery (for example, a social media post with a malicious link), or some other medium.
- Part of the user awareness training must include a reporting mechanism to report potential threats.
- Although humans are well known for falling victim to phishing attacks and remain the most-used vector for delivery, they can also be one of the best sensors for spotting an attack.
- It takes only one employee from a pool of targeted employees to report the phishing attempt, which, in turn, brings attention to the phishing attempts that were missed.

#### Disrupting Installation
- Armed with a successful exploit, the attacker focuses on gaining persistent access, typically through the installation of a RAT or through reconfiguring the system and creating a backdoor.
- At this phase, a critical component to disrupting installation is to **limit user privileges as part of the system’s secure baseline**.
- Normal users should not be able to make registry changes or install new software.
- If possible, system administrators should **disable macros and PowerShell for Windows-based systems** and **confine users on Linux systems to chroot jails**.
- In addition, organizations should ensure that **monitoring is in place** to catch abnormal process activity via a User Behavioral Analysis (**UBA**) or Endpoint Detection and Response (**EDR**) engine.
- Examples of abnormal activity include the following:
  - Microsoft Word spawning PowerShell
  - Apache Tomcat spawning Bash
  - Java calling rundll32

- When an attacker’s malware is discovered or installation is prevented, if conditions permit, the malware should not be immediately destroyed.
- It is important to study it and understand its capabilities.
- Gathering Indicators of Compromise (IOC) related to the malware helps to find additional victims and put the necessary protections in place to disrupt future attacks and later phases of the kill chain by the actor.

#### Disrupting C2
- At the C2 phase of the kill chain, the actor has managed to install their chosen persistence mechanism and wants to open a channel to remotely manipulate the system.
- Usually, actors choose common protocols and ports to create this channel for two reasons: **to blend in with normal traffic** and **to avoid being blocked by network edge devices**.
- Therefore, the most common C2 protocols used by attackers are web ([HTTP/HTTPS]) and Domain Name System (**DNS**), as these are generally always open within a network. 

- All traffic leaving an organization’s network should be scrutinized for its destination and protocol.
- As in the Delivery phase, web filtering is an effective mitigation to prevent an attacker from setting up a C2 channel.
- Organizations should **configure clients to proxy their DNS and web requests** and **block domains and IP addresses that are known as bad**.
- In addition, it is highly recommended that a **proxy block connections to recently registered domains**, as threat actors frequently stand up new infrastructure to perform their attack.

- When possible, organizations should invest in Deep Packet Inspection (DPI) tools that break apart the header and analyze the content of network packets.
- DPI provides defenders with a more robust opportunity to detect and prevent threats by providing additional capabilities, such as the ability to do the following:
  - **Reconstruct delivered malware**.
  - **Identify potential data leaks**.
  - **Recognize protocol traffic over a mismatched port** (for example, DNS request over Transmission Control Protocol [TCP] port 80).

- Most attackers encrypt their communication channel using Secure Sockets Layer (SSL) or Transport Layer Security (TLS) protocols.
- As a result, organizations that do not add SSL/TLS decryption to their security stack are leaving an open door for an attacker to take advantage of.
- A drawback of decrypting traffic, however, is network latency because it adds processing time, so it becomes a balancing act for each organization to weigh the threat against the disadvantages of implementing any new control.

- Defenders can also disrupt encrypted communications by gathering and pivoting on the information contained in the SSL/TLS certificates used by the adversary.
- The approach here, which does not require DPI, is to fingerprint the SSL/TLS negotiation between an infected host and the attacker’s C2 infrastructure.
- One method of fingerprinting this negotiation is known as JA3.
- JA3 creates an MD5 hash based on the decimal values of bytes in certain fields of the Client Hello packet: **Version**, **Accepted Ciphers**, **List of Extensions**, **Elliptic Curves**, and **Elliptic Curve Formats**.
- Armed with this fingerprint, a defender can bolster their hunting to prevent future compromises or identify active ones.

#### Disrupting Actions on Objectives
- Once the attacker has set up their communication channel, they move toward completing their objective.
- This is the last opportunity that a defender has to disrupt their activity before serious impacts can be inflicted.
- Those impacts depend on the attacker’s intent, so defenders should try to understand the threat scenarios that they face before an attacker ever reaches this phase.

- Unless the attacker was lucky enough to immediately land on a system that meets their objectives, a typical goal for attackers is to further their access by elevating their permissions and moving more deeply into the victim’s network.

- For example, a bank would be concerned with an attacker who is financially motivated and intent on modifying wire transfers and bank accounts.
- The attacker may manage to phish a bank employee and compromise that employee’s workstation, but that may not be enough access for them to fully achieve their goals.
- They need to elevate their privileges and gain access to the system that handles such transactions.
- This is where proper **network segmentation** and **authentication controls** are a defender’s best defenses to disrupt the attacker at this phase.
- These controls at least slow the attacker down, increase their dwell time (the length of time an attacker is present on a victim’s network), and give the defenders more time to discover the adversary and eradicate them. 

- Even the most secured organization may have an attacker eventually reach this phase.
- It is critical for defenders to have **incident response playbooks** in place for the possibility of such an occurrence.
- Having a preplanned course of action helps to reduce the defender’s Mean Time To Respond (MTTR), which is the average time required to restore a system to an operational condition after learning about a cyberattack or intrusion.




# MOD 7
## Defensive Posture
### Undserstanding Defensive Posture
- **Defensive posture** is the _overall state of cybersecurity defenses for an organization and is determined by reviewing the entire picture of security within an organization_.
- This includes the following categories:
  - **Security policies**: All operating policies and how they relate to security, as well as security policy and procedures.
  - **Security training**: Security personnel training as well as entire organizational staff training.
  - **Security architecture**: All assets, asset configurations, and the network topology of an organization.
  - **Risk management**: Vulnerability management, risk analysis, and patch management.
  - **Security controls**: Physical and logical security controls to defend the organization.
  - **Security personnel**: The training and experience level of security personnel for an organization.
- Assessing defensive posture gives an overall idea of the security currently in place and efforts needed to ensure proper protection.
- Of the categories above, a **CDA should focus on security architecture**, **risk management**, and **security controls**.
- All other areas fall under the scope of organizational leadership and are handled at the organizational level.

#### Measuring Defensive Posture
- Organizational leadership is in charge of deciding the level of defensive posture of their organization.
- Leaders should work in conjunction with cybersecurity personnel to analyze all organizational aspects of security.
- Risks should be analyzed alongside all defensive measures in order for leadership to dictate the overall level of defensive posture.
- The MITRE (ATT&CK®) framework can be used to analyze defensive posture.
  - This process consists of reviewing all aspects of threats through the different sections of the ATT&CK matrix.
  - All areas from within the ATT&CK matrix are compared against defensive capabilities. 

- Analysis of these common tactics is performed in two ways.
  - The first way is from a high-level review, without a specific threat in mind.
    - This high-level analysis focuses on preventing or mitigating the overall threat of common adversarial tactics.
    - For example, high-level analysis of the common tactic of privilege escalation should ensure that mitigating controls exist to prevent general privilege escalation threats and monitor for the threat of privilege escalation occurring within the organization.
  - The second type of analysis is far more granular and is applied on a per-threat basis.
    - This level of analysis is used to review every aspect of a specific threat and ensure defenses are in place to help mitigate or eliminate the threat.
    - This is far more resource intensive and is generally prioritized for threats that pose extremely high risks to the organization.
- Another method for analyzing defensive posture is use of a Security Information and Event Manager (**SIEM**).
  - SIEMs can be used to gather an overarching picture of the defensive posture for the security architecture.
  - This picture can be gathered by parsing through logs and then generating searches that correlate many statistics about security operations.

- A defensive posture dashboard can provide defensive posture and architecture information at a glance.
- This dashboard typically includes the state of devices on the network, graphs and charts of key log and network activity, details from security assets, vulnerability metrics, alerts, and statuses of all potential incidents.
- One issue with this type of measurement is that getting a SIEM tuned to accurately provide the defensive posture can take significant time — even years — depending on the size and complexity of an organization.
- SIEM dashboards can be prone to false positives, so time and resources are required to tune dashboards to an adequate baseline to reflect the organization’s defensive posture.

#### Network Defensive Posture
- When performing Enable Harden functions while on mission, Network Analysts are primarily focused on strengthening the defensive posture of the network.
- Whereas Host Analysts are more concerned with specific controls and patches to strengthen the defensive posture of individual devices, Network Analysts must look at defenses from the network perspective. 
- When addressing overall defensive posture, a holistic approach must be taken.
  - However, Host or Network Analysts may create more specific dashboards that can be used to narrow the scope to just the host activity or network activity.  
- A network defensive posture dashboard should include **port**, **protocol and service information**, **IP** address information, **network traffic trends**, **anomalous network activity**, and **signatures or alerts specific to network traffic**

#### Defensive Posture and Cyber Threat Intelligence
- Knowing an organization is at risk from threats or threat actors alters defensive posture.
- An organization may have an extremely strong defensive posture, but that could completely change if new vulnerabilities are found that impact their operations.
- If an unknown vulnerability or threat is identified, the defensive posture may be weakened until adjustments are made to mitigate the new threat.
- Due to this, a need arises to integrate knowledge of new threats and vulnerabilities into operations.
- This process is called intelligence-driven Defensive Cyberspace Operations (DCO). 

- Intelligence-driven operations have always been an integral part of all operations in the U.S. Department of Defense (DoD), including DCO.
- In essence, the goal of intelligence-driven DCO is to leverage Cyber Threat Intelligence (CTI) to establish and strengthen the defensive posture of an organization.
- The understanding of threat actors and their techniques is key to building secure architecture and finding new detection measures for threats.
- Using intelligence-driven DCO minimizes risk and emboldens defensive posture.

### Implementing Intelligence in Operations
- Intelligence should be ingested into operations to adequately defend against known threats.
- Using intelligence, security analysts can **identify patch recommendations for known threats**, **devices that need additional security controls**, and **monitoring/alerting priorities**.
- When using CTI, cybersecurity personnel should gather an understanding of TTPs used by threat actors that are related to operations.
- Leveraging TTPs can help to identify areas where threat activity can be discovered and defensive measures can be implemented.
- Additionally, knowledge of TTPs can be used to create multiple defensive measures.
- Creating layered defenses for known threats can drastically improve defensive posture.
- When a new threat is identified, the ATT&CK framework can break down the individual phases of the attack.
- Using this knowledge, security controls can be implemented, altered, or created to increase defenses for the new threat.

### Deliberate Defense Posture
- Cyber Protection Teams (CPT) may need to establish a deliberate defense posture to adequately defend against adversary activity.
- In some instances of an engaged attack, threat actors may be in an area with poor visibility or network gaps that result in a weakened defensive posture and defensive capabilities.
- The CPT modifies the environment in cyberspace to disrupt actions in those areas and force threat actors into areas where CPTs have better visibility and capability to respond to threat actor activity.

### Securing Device Configs through CTI
- When analyzing any network, ensuring secure configurations on assets is a priority.
- Threat intelligence can provide even further insight into different vulnerabilities that are actively being exploited within an industry.
  - This can lead to prioritization of vulnerabilities over others due to a known active threat.

- Intelligence may direct that certain **ports**, **protocols**, **software**, and **devices** are part of a threat actor’s **TTPs**.
- This can lead to **immediate patching priorities** for these devices, **alterations to current configurations**, **creation of new alerting or response actions**, or even **removal of devices**.
- In some cases, recently patched or upgraded devices may be the target of a newfound threat. This can lead to rolling back to previous versions that had fewer security issues if it is deemed safer for operations.

### Establishing Monitoring Priorities with CTI
- SIEMs can be used to implement intelligence directly into operations by creating specific searches and alerts that monitor for cyber threats found through intelligence.
- Through root cause analysis of malicious activity, specific alerts and monitoring capabilities are generated to avoid repeat offense of this activity.
- Leveraging SIEM tools, intelligence can be used to drive the creation of new searches and alerts as soon as threats become known.

- Additionally, (**IDS**) and (**IPS**) can be used to create or highlight alerts that monitor for specific threats that have been identified through intelligence.
- These systems use signatures for known threats that create a log or block traffic if the signature’s requirements are met.
- The signatures generally look for specific traffic patterns or file accesses that are associated with known malicious activity.
- They can generate false positives as well, which means the system needs to be tuned and catered to each organization’s architecture.

- Many SIEM and IDS/IPS tools have intelligence capabilities that allow them to connect to a CTI feed.
- These provided threat feeds can be used to ingest known alerts or searches to identify new or previously unidentified threats or capabilities of threat actors.
- CTI feeds are great solutions for organizations that may not have an internal intelligence capability.
- However, integrated CTI feeds are often a paid service, so organizational leadership must determine if this method is worthwhile.

### Training with CTI
- Personnel within an organization are always at risk of being targeted by an attack from social engineering, phishing, and similar attacks.
- Developing training in conjunction with threat intelligence can help to keep personnel informed of possible threats that may affect an organization.
- If an organization is at risk of being attacked by a specific APT, then training should be made showcasing their attack methods and how these threats could be directed toward the organization.
- Using this intelligence in training is a critical portion of increasing the defensive posture of an organization.



## IOCs in Search and Filters
### Types of IOCs
<img width="1801" height="1166" alt="e16f03f2-58c8-400a-83b3-fcf93bfa4885" src="https://github.com/user-attachments/assets/1f595e9a-94fc-403a-948c-9f24f5706e32" />

#### Hash Values
- A hash value is an output from a hashing algorithm whenever a specific input is given.
- It is the most trivial type of IOC for malicious threat actors to change.
- For example, a virus might make trivial nonfunctional changes to itself that change the hash value of the file, or a payload being delivered by a malicious threat actor might be trivially padded, encrypted, or compressed in order to evade hash-based detection methods.
- Although hash values can be incredibly useful to locate malicious files, more sophisticated attackers can easily evade any method of detection using hash values.

#### IP Addresses
- IP addresses indicate the origin or destination of potentially malicious network traffic.
- Due to the ubiquity of Virtual Private Network (VPN) servers and other factors, IP addresses can be easily masked or changed by a malicious threat actor.
- Restricting traffic from known malicious threat actors might still hinder or delay them, and these values can still be useful in post-compromise analysis.

#### Domain Names
- Malware, (C2), and data exfiltration schemes may use domain names for various reasons.
- These reasons include rapidly changing IP addresses, submitting (DNS) queries to exfiltrate data via nontraditional channels, and attempting to hide as legitimate traffic by using legitimate-looking domain names.
- New domain names can be easily registered by a malicious threat actor; however, this generally requires a monetary expenditure.

#### Network/Host Artifacts
- Network and host artifacts consist of the portions of network traffic or data found on hosts that indicate potential malicious activity.
- Examples of network or host artifacts include **client requests**, **server responses**, **files**, **registry entries**, and **system log entries**.
- Although this encompasses every file on a system and every piece of traffic passing over the wire, this category of IOCs focuses on specific pieces of data that can be used to determine if activity is legitimate or malicious.
- Denying a malicious threat actor specific network or host artifacts can require more of an investment in time or effort to evade.
- For example, if a specific user agent string is identified as malicious, blocking all traffic from that source whenever that user agent is encountered might require the malicious threat actor to reconfigure or recompile the tool that generates that IOC.

#### Tools
- Tools are the actual software that a malicious threat actor uses to perform their attacks, such as password-cracking utilities, exploitation frameworks, and Remote Access Tools (RAT).
- Allowing only legitimate software to execute via allowlisting can be very effective in preventing the usage of tools by a malicious threat actor.
- From a networking perspective, analysis of the upper layers of the Open Systems Interconnection (OSI) model can be used to fingerprint malicious tools and deny their access.
- Denying entire tools to a malicious threat actor can require pivoting to new, less-understood tools or significantly slow down discovery, exfiltration, or exploitation of a network.

#### Tactics, Techniques, and Procedures
- Detecting and denying entire (TTP) is, as its position on the pyramid indicates, the most effective level to be operating at when hindering malicious threat actors.
- Operation at this level often relies on collections of IOCs from various sources to be able to detect, alert, and deny their usage.
- The ability to block or detect entire TTPs can not only hinder malicious threat actors; it can also laser-focus responses or detection of malicious activity, allowing for rapid response.

### Leveraging IOCs with Suricata
#### Overview
- Suricata rules are used for creating and sharing network rules across multiple platforms and device vendors.
- The ubiquity of the Suricata format makes it a common choice for sharing network rules related to IOCs in threat intelligence reports with various databases maintained for emerging threats.
- Knowing how to read and write these rules can help analysts to perform their duties.

#### Usage
- Many vendors use Suricata rules for generating alerts or filtering traffic.
- A rule can indicate the intended action to be taken for a match, although some vendors’ products may ignore (or be configurable to ignore) an action and use it entirely for notification purposes only.
- The following actions are valid for a particular rule, according to Suricata’s documentation:
  - **alert**: Only generates an alert.
  - **pass**: Stops processing packet.
  - **drop**: Drops the packet; generates an alert.
  - **reject**: Rejects the packet, sends a (TCP) (RST) packet or an (ICMP) unreachable response to the source or sender depending on the type of packet.
  - **rejectsrc**: Same behavior as reject.
  - **rejectdst**: Same behavior as reject, but sends the response to the destination.
  - **rejectboth**: Same behavior as reject, but sends the response to both source and destination.
- These actions provide flexibility in the usage of rules, allowing any particular rule to be modified to either alert or filter with various behaviors for rejecting or dropping the packet.

#### Format
- Example 1

```
alert tcp $HOME_NET any -> $EXTERNAL_NET 6969 (msg:"ET P2P BitTorrent Announce"; flow: to_server,established; content:"/announce"; reference:url,bitconjurer.org/BitTorrent/protocol.html; reference:url,doc.emergingthreats.net/bin/view/Main/2000369; classtype:policy-violation; sid:2000369; rev:6; metadata:created_at 2010_07_30, updated_at 2010_07_30;)
```
- This rule matches (and by default, alerts) whenever a BitTorrent client connects to a server and announces its availability.
- The matching criteria for this rule can be broken down as follows:
  - **tcp**: Alerts on TCP packets rather than User Datagram Protocol (UDP) packets. Some application protocols, such as DNS, are also valid options.
  - **$HOME_NET any -> $EXTERNAL_NET 6969**: Matches a connection on the internal network portion, with any source port, connecting to any external network on port 6969.
  - **flow**: to_server,established;: Matches only on packets sent to the server, for TCP connections after the initial handshake.
  - **content:"/announce";**: Inspects the payload of this packet, looking for specific content (/announce, in this case).

- The particular artifact that is the potential IOC for this rule consists of a connection to a server on port 6969 and a payload being sent to the server containing the value /announce.
- The presence of matching network traffic is a very strong indicator that a BitTorrent client is in use.
- Although legitimate uses of BitTorrent exist, its unexpected existence in traffic when forbidden by usage policies indicates potentially unauthorized or unwanted usage of the network by a user.
- This rule also contains some meta information about this rule:
  - msg: Indicates what message this rule should display in the alert output.
  - reference: Flexible and varies from rule to rule, but some sort of reference relating to this rule.
    - In this example, BitTorrent’s documentation as well as a meta entry in the emergingthreats.net database for this rule.
  - classtype: This keyword gives some additional information about this rule by classifying it.
  - sid: A numeric Identifier (ID) that can uniquely identify this particular rule.
  - rev: Typically incremented whenever a newer version of a particular rule is created and distributed.
  - metadata: Freeform keyword that allows extra data to be associated with this rule; used in this case to give information about the dates associated with this particular rule.

- Example 2

```
alert http $HTTP_SERVERS any -> $EXTERNAL_NET any (msg:"ET ATTACK_RESPONSE MySQL error in HTTP response, possible SQL injection point"; flow:from_server,established; file_data; content:"Warning"; content:"mysql_"; fast_pattern; distance:0; threshold:type both,track by_src,count 1,seconds 60; classtype:web-application-attack; sid:2020507; rev:3; metadata:affected_product Web_Server_Applications, attack_target Web_Server, created_at 2015_02_24, deployment Datacenter, signature_severity Major, tag SQL_Injection, updated_at 2016_07_01;)
```
- The particular artifact that is the potential IOC for this rule consists of a (HTTP) connection in which the server sends content to the client that is consistent with the output of a MySQL error message.
- Error messages like this should never be sent to a client; the server might be misconfigured or poorly secured.
- These alerts might indicate that a malicious threat actor is actively testing for a Structured Query Language (SQL) injection vulnerability. 

- The new matching criteria are broken down as follows:
  - **file_data**: This keyword, valid for HTTP rules, indicates that the body should be examined.
  - **content:"Warning"; content:"mysql_";**: Multiple content values are allowed in a single rule.
  - **fast_pattern**;: Modifies the rule to have it match on the previous entry first. This can be used to optimize the Central Processing Unit (CPU) usage of a rule.
  - **distance:0;**: Indicates that this content should be found immediately following the previous content for this rule.
  - **threshold**:type both,track by_src,count 1,seconds 60;: The threshold keyword allows for a rule to alert only when enough triggering values are found within a specific timeframe.
    - With an alert threshold of 1, this alert triggers on any match; however, this rule can be tweaked to the needs of the network by changing this value.


## CTI-Based Hunts
### Threat Hunting Refresher
#### The Threat-Hunting Loop
- Threat hunting is one of the primary functions of CPTs.
- The Threat Hunting Loop phases, in order, are as follows:
1. Creating a hypothesis.
2. Investigating the idea.
3. Uncovering new patterns and Tactics, Techniques, and Procedures (TTP).
4. Informing and enriching analytics and detection capabilities.

- To summarize the lesson Vulnerability Based Hunts, vulnerability-based hunts use vulnerabilities identified within an organization to drive the hypothesis for planning an approach to hunting threats.
- Once an indicator has been found, analysts are able to inform and enrich an organization’s detection and defensive capabilities through detection signatures and intelligence.
- Similar to how vulnerability-based hunts use vulnerabilities, CTI-based hunts leverage such attributes as motivations of threat actors to drive the approach to hunting threats on a mission partner’s network.
- If an Advanced Persistent Threat (APT) is motivated to target the mission partner, intelligence can be used to leverage TTPs and Indicators of Compromise (IOC) attributed to that APT to uncover evidence of an attack.

#### Targeting Strategy
- The methodology of identifying areas within the mission partner’s network that are likely to be targeted is defined as a targeting strategy.
- Essentially, accounting for such factors as network inventory, Key Terrain in Cyberspace (KT-C), intelligence-related vulnerability reports, and threat assessments helps in deciding how to prioritize the effort spent conducting analysis during a hunt.
- Whereas vulnerability-based hunts more heavily emphasize vulnerabilities, CTI-based hunts rely more on intelligence to determine prioritization of analysis during a hunt.


### Exploitation Indicators
- Vulnerabilities comprise two main categories: those that can be exploited locally and those that can be exploited remotely.
- An analyst must know what to look for regarding both types of exploitation categories when conducting a hunt.

#### Local Exploits
- Local exploits, as the name suggests, are exploits that can be triggered only from within the system being exploited.
- One of the most common examples of a local exploit is the variety of privilege escalation attacks that target Operating System (OS) mechanisms.
- Indicators of local exploitation are likely observed from endpoint detection solutions and system logs.
- This means that indicators from network metadata are follow-on activities, such as Command-and-Control (C2) traffic, pivoting, reconnaissance, and exfiltration. 

#### Remote Exploits
- Remote exploitation is not to be confused with remote code execution, although remote code execution exploits provide the most obvious example of remote exploitation.
- Remote exploitation is one method APTs leverage to gain a foothold or move laterally.
- Many (**NIDS**) and (**IPS**) signatures focus on identifying this type of activity, as triggering these types of exploits requires some type of sequence or amount of data to be sent through a network in a predictable manner.
- Although network signatures represent the simplest method of detecting remote exploits, they should not be relied on, as signatures need to be tuned.
- Additionally, it is not feasible to have a signature for every exploit variant, and the signature databases must be kept up to date.
- On top of that, the appliances that use them must be implemented correctly.
- Finally, in many instances, signatures are incapable of inspecting a remote exploit’s payload as threat actors use encryption to obfuscate their activity. 

#### Combining Exploits
- One assumption analysts make is that vulnerabilities not labeled as critical or high are less likely to be harmful and, therefore, are less likely to be used by threat actors.
- Analysts should consider how the vulnerabilities uncovered on an operation are leveraged in combination to cumulatively pose a larger threat.
- This use of several “lesser” vulnerabilities in succession is more likely to occur when an adversary is acting against a mature environment. 

#### Observation of Exploitation Alerts
- Although many signatures are written to identify exploitation within an environment, finding evidence of direct exploitation is unlikely.
- Understanding how different types of exploits can be used to identify malicious activity not directly determined by a signature’s alerts is critical.
- Anticipating how successful exploits could be used to advance an attacker's goal is also critical.

### Open Source-Research
- Analysts should supplement their current understanding of any given situation with further research.
- Research regarding hunting commonly deals with TTPs, IOCs, past and currently trending vulnerabilities, and potential indicators of malicious activity. 

#### Publicly Available Exploits
- Vulnerability reports almost universally provide some type of reference to related Common Vulnerabilities and Exposures (CVE), which are a great starting point for searching for publicly available exploits.
- Both code and tutorials on how to exploit vulnerabilities are highly accessible with a simple query through any search engine, using such keywords as the specific CVE and exploit or tutorial.
- Some resources are more reputable than others; the Exploit Database is a good choice.
- The ability to review exploit source code is beneficial in identifying what data is being sent to trigger the exploit and providing insight into what an analyst should look for during a hunt.

#### The C2 Matrix
- Understanding the tools and techniques available to adversaries is critical to an analyst’s success.
- Adversaries use some form of a C2 framework to efficiently orchestrate their efforts.
- Although more advanced adversaries use customized frameworks for this, it is important to understand how these frameworks are designed.
- To accomplish this, the C2 Matrix is a useful tool.
- The C2 Matrix catalogs many C2 frameworks and summarizes their features in a matrix.

#### Security Product Vendors and Social Media
- Many larger security product vendors have blogs that provide a reputable source of information about current topics in cybersecurity, often with varying levels of technical detail.
- Additionally, the employees of these vendors may use social media platforms (for example, Twitter) to promote themselves and their companies by sharing information on their personal research in cybersecurity.
- It is beneficial for analysts to follow these sources of information to stay current with the constantly evolving battlefield of cyberspace.

### Anomaly-Based Detection
- The key to anomaly-based detection is identifying anything outside well-defined patterns.
- An observed baseline of network behavior, the defined rules of a protocol, inventoried devices, system and service uptime, and even documented problems are all well-defined patterns with regard to hunts.
- The following should be considered when using an anomaly-based detection strategy.

#### Traffic Volume
- The fluctuation of traffic volume normally falls within a predictable pattern in any given 24-hour period.
- Any deviation from that pattern is of interest and can indicate a range of concerns, including C2 traffic, reconnaissance, lateral movement, data staging for exfiltration, data exfiltration, or tool staging.
- Much of the context is dependent on such factors as whether an established baseline exists, how current that baseline is, network accountability, and perspective of the statistical information, such as ingress/egress or internal/external traffic.

#### Data Flow
- The direction of communication and context of data flow may be indicators of abnormal activity.
- Is a server acting as a client for a system that is identified as a workstation?
- Is peer-to-peer traffic standard or unusual?
- Abnormal flow of communications may indicate lateral movement, pivoting, Man-in-the-Middle (MitM) or spoofing attacks, or C2 chains.

#### Protocol Standards
- Protocol standards are documented through Requests for Comments (RFC). However, their implementation in practice is left to the application designer.
- Typically, application designers adhere to the standards defined for the protocol they are working with.
- However, threats have found that certain definitions within a given protocol may be ignored in order to carry control data within them.
- Common examples of this are C2 frameworks that use Hypertext Transfer Protocol Secure (HTTPS) request fields to transfer data.
- Additionally, malformed protocol data may be used to trigger remote exploits.

#### Endpoint Accountability
- Although servers within an organization’s environment are fairly easy to maintain inventory of, network accountability of workstations within an organization may be challenging.
- Many security vendors provide some type of solution that tracks systems and attempts to identify any systems unaccounted for.
- However, the effectiveness of those solutions may vary.
- An unaccounted-for system may be unpatched, provide an obsolete service, miss a mandated security solution, or even be a malicious device on the network.
- Effort must be taken to identify the purpose and ownership of any unaccounted system.

#### Change Management
- Any changes outside the change management chain should be questioned.
- Attackers may make any changes they deem necessary to better enable their success.
- These changes may actually include fixes to avoid unwanted attention of a key system, modifications of networking equipment to allow access to a network, log erasure, or service configuration changes to support such an effort as an MitM attack.

#### Errors and Crashes
- Service downtime and errors may be indications of a direct exploitation attempt, as many exploits leverage application flaws to create a preferred state of execution.
- The conditions that cause these desired flaws to be leveraged normally cause an application to crash or throw errors.
- If a device has a history of crashing or a short period of throwing errors unexpectedly, those events may be indicators of exploitation attempts.

### Synergies
- Although CTI-based threat hunting is the immediate focus of this lesson, the following are a few added benefits that apply to each role of a CPT.

#### Cyber Threat Emulation
- Being able to understand and emulate threats by leveraging such CTI-fed data as motivation, IOCs, and TTPs is a core function of Cyber Threat Emulation (CTE).

#### Threat Mitigation
- Discovery and Counter Infiltration (D&CI) produces signatures and additional intelligence from CTI and CTI-based hunts, which, in turn, can be applied to such mitigations as custom IPS signatures and configuration-based hardening.

#### Training
- Any threat hunt that attributes observed and new TTPs of threat actors can add value to future training by maintaining documentation of those TTPs.


### Applying CTI to Hunts
#### Cyber Threat Intelligence
- Maintaining realistic expectations and knowing what can be provided by the intelligence community increase mission effectiveness.
- Although CDAs in a CPT are not expected to perform intelligence analysis, they are the consumers of intelligence.
- Therefore, it is beneficial for those analysts to understand the different levels of intelligence and their significance at each level.
- By understanding and applying CTI, analysts are most effective in countering active and future threats on a network. 

##### Levels of Intelligence
- When dealing with consuming or collecting intelligence on a network, intelligence comes in different forms, depending on requirements given to an analyst.
- These forms of intelligence range from domains tied to a specific APT for use in an Intrusion Detection System (IDS) to a list of APTs assessed to have the capability and intent to target a mission partner’s network or even the political motivations of a threat group.
- Ultimately, intelligence can be broken down into three levels:
  - **strategic**
    - Strategic intelligence is associated with attacker motivations or politics, trends, or information that might inform policy changes at the organizational level.
    - Often, strategic intelligence is the trigger for the start of an operation or the change to network architecture.
  - **operational**
    - Operational intelligence includes such information as attacker TTPs and capability.
    - This intelligence informs collection plans and mission-planning efforts prior to an incident response or hunt engagement
  - **tactical**
    - Tactical intelligence is immediately applicable intelligence, such as IOCs, antivirus signatures, or IDS rules.
    - This intelligence is most useful for CDAs on a mission during on-network engagements.

#### Capabilities and Defensive Posture
- A key factor to success with any hunt mission is gaining situational awareness of the mission partner’s Operational Environment (OE).
- Part of the situational awareness includes the mission partner’s ability to feed network metadata into the CPT’s analysis framework as well as their overall detection capabilities and defensive posture in relation to threats identified by CTI.
- This process parallels the evaluation and analysis of the Area of Operations (AO) logical map and mission partner’s defense capabilities.

#### Collection Capabilities
- Depending on the situation (for example, limited storage capacity), it may be impossible to have all collection points feed into a Security Information and Event Manager (SIEM).
- During such scenarios, an analyst may need to separately review data from individual collection points to account for any gaps in their analysis.
- Having an understanding of how service data is processed in a network aids in accounting for these gaps.
- The key to an analyst’s success is being able to identify where these blind spots are and, if possible, how to compensate for them.

#### Detection Capabilities
- To identify whether malicious activity has occurred, a minimum amount of relevant data must be collected (where minimum depends on the malicious activity).
- If such data does not exist or cannot be collected, then compensating analysis must occur or a particular level of uncertainty must be accepted.
- A key task in hunting is being able to determine existing limitations in order to increase a conclusion's certainty to the extent possible.

#### Efficiencies
- In addition to identifying indicators of malicious activity within a mission partner’s network, an objective of CDAs should be to build efficiencies into processes.
- One notable example of this is developing signatures from queries that are used during a hunt.
- Typically, it is a trivial task to convert a search query for an IOC into a signature that can be shared both with the mission partner and with the security and intelligence communities at large.
- Another example is to build a dashboard around detecting several TTPs within the CPT’s analysis framework.
- The concept of computational efficiency should also be considered.
- This concept of computationally efficient queries has to do with the required overhead needed to perform a query.
- If a query used within a dashboard requires a high computational load, then the time required to process that task has the potential to hinder a dashboard’s resolution.

#### Defensive Posture and CTI
- In addition to identifying the mission partner’s collection and detection capabilities, an effort must be made to measure their overall defensive posture.
- Defensive posture includes such technical controls as **IPSs**, **patching**, and **anti-malware**; administrative controls like security training and usage policies; and other mechanisms, such as intelligence-driven operations. 

#### Establishing Monitoring Priorities with CTI
- Monitoring priorities should align with input from intelligence sources on threats to the mission partner and the capabilities of the mission partner.
- It would be counterproductive to plan on using capabilities the mission partner does not have or collecting network metadata without relevance to input from intelligence.
- When establishing which data sources to use, leverage what is available in the OE that accounts for the TTPs an APT uses.

#### Situation Analysis
- Part of the mission analysis phase in preparation for a CTI hunt mission is the situation analysis.
- A situation analysis takes into account the insight intelligence provides on APT TTP applications in relation to the AO.
- The objective of situation analysis is to describe potential adversary courses of action by accounting for critical systems, applications, and IOCs, and identify areas of interest.
- Areas of interest allow for prioritization of collection efforts on specific data, establishing alerts, and enabling automated responses. 

### CTI-Based Hunt
#### Preparation
- In any hunt, some time must be allotted for preparation.
- For CTI hunting, part of preparation includes defining a hypothesis around the intelligence received and integrating CTI into analysis tooling.
- Thus far in this lesson, trainees have prepared by using TheHive to receive an RFI response and apply the observables as a search filter.
- Trainees then cloned an existing dashboard to make a mission-specific dashboard with the search filter applied.
- The remaining step in the preparation process is to develop an initial hypothesis based on intelligence.

#### Conducting the Situation Analysis 
- In the RFI response, aside from the provided observables, it was mentioned that the APT is likely to use psexec, Distributed Component Object Model (DCOM), (WMI), and tainted share access for lateral movement.
- Additionally, the APT is most likely to gain initial access through phishing email attachments and attempt to gain access to data on file shares.
- The mission partner’s network is a Windows domain environment that does have shares in place, and they do use psexec for administrative tasks.
- All detection and data collection capabilities are limited to the packet collection provided through port mirroring through the mission partner’s infrastructure.
- All PCAPs have been fed into Security Onion and Arkime for analysis.

- Based on these factors, it can be concluded that if the APT gains access to the mission partner’s network through phishing, then the APT has some level of domain user access.
- With domain user access, the adversary can likely enumerate and access the file share and also laterally move.
- They are most likely to use psexec to laterally move due to the availability of the utility, as it already exists in the OE. 


# MOD 8
## Intro to Suricata
### Intrusion Detection Systems
- Information systems use IDSs as a mechanism to monitor for threats on a network.
- Similar to an alarm that sounds when an intruder is detected in a secure physical area, IDSs do not stop an intrusion, but they do seek to alert defenders when an intrusion is detected or suspected.
- **IDSs** are _hardware or software applications that monitor for malicious activity or policy violations based on specific criteria_.
- The criteria could be based on signatures or anomalies, depending on the configuration of the detection system.
- There are two classifications of IDS:
  - Host Intrusion Detection Systems (HIDS)
    - Host-based IDSs are software applications that are able to monitor for suspicious activity and malware on a host.
    - Typically, they monitor internal systems in addition to network traffic traversing the network interfaces of an endpoint.
    - Different from antivirus or Endpoint Detection and Response (EDR) agents, HIDS applications monitor and log actions rather than prevent the action altogether.
  - Network Intrusion Detection Systems (NIDS)
    - When searching for anomalies or signatures in network traffic, NIDS use a preset list of indicators or rules to log behavior to a Security Information and Event Management (SIEM).
    - Dozens of open-source solutions are on the market for NIDS, most notably Suricata and Snort®.

### Benefits and Drawbacks of IDSs
#### Benefits of NIDS
##### Flexible Deployments
- Intrusions can be detected in many ways.
- NIDS can be deployed almost anywhere that network traffic can be mirrored.
- This means that lateral, client-to-client traffic can be monitored as easily as traffic at an ingress/egress point of an enterprise network.
- It all depends on the capability of the network infrastructure.
- Typically, technologies like IPSs are deployed similarly to a firewall at security boundaries of a network.
- IDSs have the flexibility to sit inside a network between assets.

##### Custom Rulesets
- Given the standard ruleset and syntax of most IDSs available (including those included in the Joint Deployable Mission Support System [JDMSS] kit), rules are easily written to be tailored to a specific environment.
- This gives analysts the ability to write custom rules conforming to intelligence given to the team before a mission.
- It also gives analysts the option of writing rules on the fly as indicators are observed on the network during operations.

##### Protocol Awareness
- IDSs have the capability of detecting, inspecting, and dissecting most standard protocols on a network.
- This means that protocol headers and specific protocol information can be used in alerts.
- For example, Suricata can inspect Kerberos traffic in a Windows Active Directory (AD) environment to alert on specific errors or ticket requests.

##### Passive Nature
- Being placed on a network where traffic is only mirrored gives analysts the opportunity to monitor for a larger array of signatures or anomalies.
- As opposed to IPSs, they do not block any traffic.
- Therefore, rules can be tuned and modified without fear of blocking legitimate traffic from a larger network.
- Passive deployments give analysts freedom to be as general or as granular as a mission requires.

#### Drawbacks of NIDS
##### Inability to Prevent Attacks
- As the name suggests, NIDS software only detects intrusions; it does not prevent anything.
- Analysts are required to monitor logs and alerts to determine if anything needs further investigation or action.
- This results in the need for a robust automated response system or a workforce to manually parse logs.

##### Frequent False Positives
- The downside of being flexible and passive is that rules tend to be noisy.
- Tuning rules is a necessity for efficient use of an open-source ruleset or a default list of policy-based rules.
- Furthermore, an analyst must manually determine if a suspected false positive is, in fact, a false positive, which takes time and effort.

##### Frequent Signature Updating
- Rulesets of NIDS rules require frequent updating to keep up with the emerging malicious activity in the cyber world.
- This requires an internet connection, which might not be possible, depending on the system being used.
- Also, every round of updates likely causes the need for more tuning and the likelihood of false positives occurring.

### Suricata
- Since its creation in 2009 by the Open Information Security Foundation (OISF), Suricata has steadily climbed to become the cybersecurity community’s most popular IDS engine.
- Suricata is an open-source tool used in many hardware appliances and software suites that use rule syntax similar to Snort (the original IDS engine, currently developed by Cisco®).
- Suricata has many features outside the IDS engine that can be leveraged for security operations by a CDA.
- Configuration of all the features is out of scope of this lesson, but the functionality of each feature is described below.

#### Intrusion Detection System Features
- The IDS features of Suricata are extensive. Given its standard rule syntax and easy installation, Suricata can be implemented on any network with the ability to mirror network traffic to a monitoring server.
- Additionally, the logging format is universal, meaning that it can be sent to any log collector that accepts messages formatted in JavaScript Object Notation (JSON) (for example, Elasticsearch or Splunk).

- As illustrated later in this lesson, the IDS rule format allows for flexible monitoring and alerting.
- Multiple protocols can be dissected and advanced searching can be implemented through the use of hexadecimal characters and regular expressions.

- Not only are the rules easy to create, but a number of open-source and subscription rulesets are also available.
- The most popular ruleset available is the Emerging Threats Open ruleset curated and distributed by Proofpoint®, which provides thousands of rules covering multiple categories and severity levels.

#### Intrusion Prevention System Features
- The Intrusion Prevention System (IPS) features of Suricata are similar to the IDS features.
- The IPS functionality uses the same rule format, protocol dissection, and logging interoperability.
- The greatest difference is in its deployment scenarios and keywords in the rulesets.
- Unlike an IDS, an IPS needs to be deployed inline, meaning that network traffic must flow through the IPS as opposed to being mirrored.
- This requires a hardware device that can sit between senders and receivers on a network.
- Many vendors use Suricata to determine what network traffic is allowed or blocked as a part of their IPS functionality.
- Typically, when Suricata is deployed in an IDS capacity, there is only one action that each rule takes when traffic matches a rule: alert.
- However, when deployed as an IPS, the rest of the available action keywords are as follows:
  - **alert**: Only logs an alert signifying that a rule was matched.
  - **pass**: Passes the traffic without further inspection.
  - **drop**: Drops the matching traffic and logs an alert.
  - **reject**: Drops the traffic and sends an unreachable error message to the sender.
  - **rejectsrc**, **rejectdst**, and **rejectboth**: Do the same as reject, but send the error message to the source, destination, or both.

#### Network Security Monitoring
- Past just the IDS/IPS use cases, Suricata can also perform logging of traffic characteristics based on protocol observed.
- Nineteen total protocol dissectors are built into Suricata.
- The following are examples of the dissectors useful for Network Security Monitoring (NSM):
  - Hypertext Transfer Protocol (HTTP): Inspects HTTP sessions and extracts header field information, such as hostname, content-type, method, user-agent, and cookie.
  - Domain Name System (DNS): Logs DNS queries, answers, types of query, and other information useful for monitoring DNS on a network.
  - Transport Layer Security (TLS): Monitors TLS session for information that can be seen in cleartext and logs it. For example, hashes of certificates along with issuer and certificate chain are logged.
- In addition, network flows can be logged similar to other popular NSM engines like Zeek.
- Suricata logs the beginning and end of sessions, along with source and destination IP addresses and ports.

### Suricata Rule Anatomy
- Every Suricata rule follows the same structure.
- Some Suricata rules are longer and more complex than others, depending on the signature or criteria they are attempting to find.
- There are virtually endless possibilities for matching on traffic and protocols flowing through a network.
- The following is an example of a simple Suricata rule from the Emerging Threats Open ruleset:
  ```
  alert dns $HOME_NET any -> any any (msg:"ET DNS DNS Query for Suspicious .co.be Domain"; dns_query; content:".co.be";
  fast_pattern; nocase; isdataat:!1,relative; classtype:bad-unknown; sid:2013124; rev:5;)
  ```
- As illustrated in the example, Suricata’s rule format and language are human readable.
- This rule does the following:
  - The alert keyword at the beginning signifies that the rule only creates a log of the event and takes no action on the traffic itself.
  - It only analyzes DNS traffic, as seen early in the rule.
  - $HOME_NET any -> any shows that traffic direction is from the home (or internal) network to any network.
  - The remainder of the rule becomes more difficult to decipher.
- In summary, it matches on any DNS query that includes, but does not begin with, .co.be, without regard for case of the letters.

- Each rule is broken into three distinct sections — **Action**, **Header**, and **Options** — that are described below.
- These sections have their own standards for structure and syntax that must be followed exactly. Otherwise, the rule does not work, and Suricata does not process any matches.

### Action
- Every rule begins with an action.
- The actions were described earlier in the lesson but are repeated here for clarity:
  - alert: Only logs an alert signifying that a rule was matched.
  - pass: Passes the traffic without further inspection.
  - drop: Drops the matching traffic and logs an alert.
  - reject: Drops the traffic and sends an unreachable error message to the sender.
  - rejectsrc, rejectdst, and rejectboth: Do the same as reject, but send the error message to the source, destination, or both.

- The action determines what happens to the matching traffic when detected by Suricata.
- Because the most common use case for a Cyber Protection Team (CPT) for Suricata is IDS services, alert is the best action to focus on.
- Most importantly, the action must be all lowercase and be at the beginning of the rule.
- Of the three Suricata rule sections, the action is the simplest.
- It is always one word, and only a typographical error can cause any real problems.

### Header
- The next section, header, defines **protocol**, **source**, **destination**, and **direction** of matching traffic.
- It must be written in a **specific order**, and, unlike the action section, the header has some **specific syntax rules** that must be followed.

#### Protocol
- The second word in every rule is always the protocol that it is written to match.
- **It is a required item**.
- Twenty-seven options exist in the current version of Suricata, and each protocol has its own corresponding options that come later in the rule.
- The most common protocols used are as follows:
  - tcp (Transmission Control Protocol)
  - udp (User Datagram Protocol)
  - ip (Internet Protocol)
  - http (Hypertext Transfer Protocol)
  - dns (Domain Name System)
  - tls (Transport Layer Security)

#### Source and Destination
- Every rule requires a source and destination, even if the rule should alert on any traffic.
- Additionally, the source and destination both require an IP address (or addresses) and a port.
- The typical syntax is as follows:
  - `<SOURCE-IP> <SOURCE-PORT> -> <DESTINATION-IP> <DESTINATION-PORT>`
- Each item in the source and destination section of the header is separated by a space.
- Such shortcuts as Classless Inter-Domain Routing (CIDR) notation are accepted.
- Operators may be added to source and destination fields that help group and add efficiencies to the rules
  <img width="1667" height="459" alt="16a1e9d3-4eee-4ffd-9c4f-1ec52ff33dc2" src="https://github.com/user-attachments/assets/bd98581b-bf80-4c7d-898a-0e8394e336a0" />

- Suricata uses variables often in the source and destination fields that can also be paired with the above operators.
- These variables are defined in the configuration file called suricata.yaml, usually found in the /etc/suricata/ directory.
- The most common variables are **HOME_NET** and **EXTERNAL_NET**.
- By default, **HOME_NET** is set to **all private IP address ranges** and represents the **inside of a network**.
- **EXTERNAL_NET** should always be set to the negation (that is, opposite) of HOME_NET, resulting in **all IPs not included in the internal network**.
- If EXTERNAL_NET is not set to the negation of HOME_NET, some IP addresses may be excluded from inspection.

#### Direction
- Every rule must define in which direction the traffic must flow to match.
- Only two valid directions may be defined in a rule: left to right, and both directions.
- The proper syntax for signifying left-to-right traffic flow is a hyphen followed by a greater-than symbol: **->**.
- Using a less-than (<) symbol instead of a hyphen signifies bidirectional traffic flow: <>.

- Every section of the header is required.
- The example rule uses variables for source and destination IP addresses, but a rule may also use actual IP addresses or the term any.
- In the source port field, any is signified, and a list of ports is specified for the destination (ports 80 and 8080).
- Direction flow is left to right. The example is representative of many common prewritten rules in available rulesets.


### Options
- The Options section defines what Suricata looks for in the network traffic after the header matches its source and destination.
- Few required fields exist in the Options section, but a strict syntax must be followed when writing rule criteria.
- Some requirements when writing options are as follows:
  - The Options section must be surrounded by parentheses.
  - Each option (including the last option) must be trailed by a semicolon.
  - Every rule must have a Signature ID (sid) option.
    - This is usually the last or second-to-last option defined by convention (not by rule).
  - Some options have settings that require a colon separating the option name and the settings it requires.
    - The settings also must be wrapped in quotes if there are spaces in the text.

#### Meta Keywords
- Meta keywords have no effect on the behavior of Suricata when matching traffic, but they define how Suricata reports and tracks each rule. The most common meta keywords are described below.

##### msg (Message)
- For all practical purposes, the msg field is the **name of the rule**.
- It should give some contextual information to alert the analyst as to which traffic was matched.
- This is a free-form option, but as the rules for options state, this requires a colon and the settings wrapped in quotes if there are spaces in the text.

##### sid (Signature ID)
- Every rule requires a sid. This is a locally significant, but arbitrary, number.
- The option requires a number to identify each rule (or signature) in order for Suricata to internally keep everything organized.
- If two rules have the same sid specified, Suricata does not process them and has unpredictable behavior throughout the entire ruleset.
- It is conventional for this to be the last option (or second to last if a revision [rev] number is specified).
- The Options section still ends with a semicolon, even though no option follows the sid field.
- Analysts can also specify a revision number by using the rev option and the same format. This is omitted from the example in the interest of brevity.

#### Payload Keywords
- Payload keywords appear in almost every Suricata rule.
- The following are a few keywords that an analyst should be able to identify and properly implement in a signature.
- They identify what Suricata is searching for inside network traffic.

##### content
- The content keyword is the most common keyword outside the meta keywords described previously.
- This keyword defines the data that Suricata looks for in network traffic to match the rule.
- It is added to the rule in a similar fashion as the msg and sid keywords.
- The example (as described in the msg option) is looking for a cURL HTTP user agent.
- Thus, the content option signifies curl as a string to look for.
- cURL (Client Uniform Resource Locator) is a command line utility for requesting contents of hosted website data.

##### nocase
- The option nocase is inserted after the content keyword to signify that Suricata should match on all cases of letters in the content.
- For example, nocase after content:cURL would match on curl, CURL, or cUrL (among other combinations).

##### startswith/endswith
- Suricata gives the ability to search for content at the beginning or end of a piece of network traffic data.
- The startswith and endswith keywords do exactly that.
- For example, if an analyst were searching for a DNS request for the .biz top-level domain, using the endswith keyword would ensure that it matches only on that top-level domain and not another domain name with .biz in another portion of the domain.

- The ongoing example does not include startswith or endswith, but the use of these keywords is common.
- Other payload keyword examples are **depth**, **offset**, **distance**, **isdataat**, and **pcre** (Perl Compatible Regular Expressions). 

#### Modifier Keywords
- Each protocol has an abundance of its own keywords used to narrow what Suricata is looking for in the network traffic.
- For example, there are TLS keywords that tell Suricata to only look for a certain certificate or certificate serial number.
- The example uses HTTP keywords because the user agent field is a part of an HTTP header in web traffic.

- There are two types of modifiers: **content modifiers** and **sticky buffer modifiers**.
- Unfortunately, there is no way to tell if the modifier used is a content modifier or sticky buffer other than to reference the Suricata documentation.

##### Content Modifiers
- Of the two types of modifiers, content modifiers are the older style.
- They look “**backward**” in the rule to create a match.
- An example of a content modifier is http_uri, which looks for the Universal Resource Identifier (URI) of an HTTP request. Using that modifier, it is written after the content option.

##### Sticky Buffer Modifiers
- Contrary to how content modifiers look backward in the rule, sticky buffer modifiers go forward.
- For example, http_response_line is a sticky buffer keyword that looks for HTTP responses from the web server.

## Introduction to NetFlow
### NetFlow Introduction
#### Packet Capture
- Packet Capture (PCAP) is a method of capturing traffic in a computer network and then analyzing the contents, or packets.
- This is critically important for CDAs, as it provides oversight to a network’s operations.
- PCAPs have a wide range of uses, such as building audit trails, monitoring bandwidth, and tracking devices.
- Additionally, PCAPs are an essential tool in maintaining network security.
- For example, PCAP data can be used to track malware activity, study past attacks, and define network-based Tactics, Techniques, and Procedures (TTP). 

- With the vast advantages and capabilities provided by PCAP data, an important fault exists that is inherent to the format and depth of the data: **size**.
- Full packet capture files grow quickly as information is captured by a sensor or interface.
- Although the number of packets captured depends on the activity level of the network, PCAP files can grow from kilobytes (KB) to megabytes (MB) to gigabytes (GB) and beyond very quickly.
- When storage capacities are limited, the sizes of these files can become a problem.
- Although wiping PCAP logs is a solution, losing potential past forensic data is a risk.
- Additionally, examining massive quantities of PCAP data is time consuming for analysts.
- To navigate these challenges, a method of capturing packets from a high level that requires less storage space and analysis time is needed.

#### NetFlow
- Created by Cisco in 1996, NetFlow is a feature that, like PCAP, provides the capability of capturing IP traffic that is entering or exiting a routed interface on a router or switch.
- However, unlike PCAP, NetFlow records only the high-level data, such as IP addresses, destination and source ports, and timestamps.
- This allows an analyst to quickly comb through large amounts of captured information as well as view traffic patterns and anomalies.
- Because NetFlow data does not contain the contents of each packet, the storage demand is substantially smaller than PCAP data. 

- NetFlow uses session data, which is a high-level, low-detail form of information.
- Specifically, it uses the 5-tuple, which comprises the following:
1. Source Internet Protocol (IP) address 
2. Source port number
3. Destination IP address
4. Destination port number
5. Protocol in use
- These five attributes are derived from the header of the IP packet.
- In addition to these, the identifier (ID) of the input interface and the type of service (ToS) byte are included.
- Together, these seven attributes form a flow. 

- But how does NetFlow gather this data?
- In a three-component system, data is collected, transported, and analyzed.
- These components are as follows:
  - **NetFlow Exporter**: Not a dedicated physical device. Rather, it is a router, switch, or firewall, for example. This device has an interface (or interfaces) configured to gather packets into flows. Once gathered, the flows and flow records are sent to the NetFlow Collector.
  - **NetFlow Collector**: A system that receives the flows from the Exporter and then stores and processes the flow data.
  - **NetFlow Analyzer**: A software application that provides the statistics and insights into the flow data. 
  <img width="1999" height="1630" alt="e7f0023d-418d-4479-83b3-9a7ff745d99e" src="https://github.com/user-attachments/assets/bf367480-b060-417a-863a-4e87a082b35f" />

- Using NetFlow allows for greater visibility into a network’s traffic.
- Whereas using full packet captures or NetFlow can be seen as an either/or situation, many industry experts agree that combining the two to properly utilize their advantages is the best solution.

### Using NetFlow to Analyze Traffic
1. From the terminal, cd into the netflow directory
2. run the following command to view a summary of a file: `nfdump -r netflow_1 -I`
  - `-r` specifies the file nfdump will be receiving input from
  - `-I` prints the summary statistics.
   <img width="522" height="431" alt="ebddcf71-b070-4f7a-80d3-81535e38fe88" src="https://github.com/user-attachments/assets/85d85051-6d1d-4b00-acd2-f44bebc40bc8" />

3. Given the summary, run the following to view top 10 IP addresses that sent highest number of packets: `nfdump -r netflow_1 -s ip/packets -n 10`
  - `-s` prints statistics for a specified record type
  - `-n` prints the number of records that are needed.
  <img width="1148" height="345" alt="59586695-f4c1-4617-9c48-4db695d1fd85" src="https://github.com/user-attachments/assets/d24126e6-b439-4b7f-ae49-64a9f43f502d" />

4. Run the following to view port information: `nfdump -r netflow_1 -s dstport/packets -n 10`
  <img width="1149" height="357" alt="5acc55ba-2e2d-4311-a629-8a6f30a09f18" src="https://github.com/user-attachments/assets/1b867306-1ccf-4a92-8730-21d0c6ad8a26" />

5. Run the following to view IP addresses that sent most HTTPS traffic: `nfdump -r netflow_1 -s srcip/packets 'port 443'`
   <img width="1152" height="283" alt="40536caa-fe2a-4378-b9d8-f8b18bcacce7" src="https://github.com/user-attachments/assets/f13563da-4685-455d-8c47-0ff73b140e49" />

6. Run the following to view IP addresses receiving most IP traffic: `nfdump -rnetflow_1 -s dstip/packets 'port 443'`


### Netflow Versions
<img width="1999" height="1472" alt="88ae65c1-6cf2-4dc9-a201-df3abbb643f7" src="https://github.com/user-attachments/assets/0dfef5f3-050f-4184-baf7-8c026f043ea0" />

#### Netflow v5 vs. NetFlow v9
- The two most commonly used versions of NetFlow are v5 and v9.
- Despite being much older and less capable than v9, NetFlow v5 is significantly more popular.
- This largely is due to the fact that **more devices support v5 than v9**.
- A reality of technology adoption is that many people and organizations continue to use what works (such as NetFlow v5) and refrain from “unnecessary” changes and hassle.
- Additionally, the process of changing to a newer version could create a gap in traffic collection, leading to temporarily reduced network visibility.
- Because of this, NetFlow v5 lives on and is only slowly being replaced by v9. 
  <img width="1999" height="1472" alt="e5a12901-2477-43e8-b346-ff1d4538fbea" src="https://github.com/user-attachments/assets/58db03b2-ec8c-40c3-8441-f2e9c18a901f" />

#### IPFIX and NetFlow 
- **IPFIX** (IP Flow Information eXport), as its name suggests, is a non-Cisco standard for exporting IP flow information.
- **IPFIX** is commonly mistaken for being just NetFlow v10.
- This is primarily due to the fact that IPFIX is directly based on NetFlow v9, supporting backwards compatibility for v9 traffic, and was created by several individuals who worked on v9.
- However, the differentiating factor that separates IPFIX from NetFlow is flexibility.
- **IPFIX** is **compatible with more devices and vendors**, allowing it to exist in more networks.
- Additionally, **IPFIX** (with its backwards compatibility) supports **up to 238 data field types**, whereas **NetFlow v9 recognizes only 79**.
- However, a side effect of IPFIX’s flexibility is a **lack of specialization toward a particular type of device or traffic**.
- Despite this, IPFIX remains a robust and adaptable solution to analyzing network flows. 

##### sFlow
- Standard Flow (sFlow) is a **packet-sampling technology**.
- Whereas NetFlow does not capture full packets, **sFlow copies and samples packets with a user-defined frequency**.
- By sampling packets, resource utilization is reduced.
- Additionally, sFlow packets contain more information than NetFlow data due to the inclusion of partial or full packet details.
- Finally, sFlow packets are sampled and exported in real time, whereas NetFlow must cache its data before exporting it to be processed.
- A large downside of sFlow is its accuracy, as packets may be missed between samples. 

## Network Traffic Visualization
### Visualizing Network Traffic
- Network visualization is the process of visually presenting networks via logs and dashboards, such as histograms and line graphs.
- To understand network issues, Network Analysts examine data at the packet level.

- Network visualizations assist analysts with displaying data that cannot be easily comprehended by analyzing raw data.
- A Network Analyst must know what connections are being made and traversing a network environment in order to form a full picture of the defended terrain.
- Although not a standalone Cyber Protection Team (CPT) function, a prerequisite to successful CPT missions is the validation and verification of the mission partner’s documentation to ensure an accurate common operating picture.
- During this step, CPTs analyze the supported organization’s network diagrams, terrain information, and configuration documents.
- However, these documents are not always present to close information gaps.
- Using tools to create network visualizations is imperative to a successful CPT operation.

### Anomalies
- PCAPs contain network data that allow the CDA to perform network forensics and deep packet inspection.
- Problems within PCAPs can be detected as traffic anomalies.
- Anomalies generate alerts by such detection tools as Intrusion Detection Systems (IDS) or Security Information and Event Management (SIEM) servers and can be visually represented.
- For example, an anomaly may be a system crash, malformed packets, unusual contact with an unknown host, or a large amount of data being transferred over a short period of time.
- The following are some ways to classify anomalies:
  - **Non-human error**: Interrupted or abnormal communications due to equipment failure.
  - **Human error**: Interrupted or abnormal communications due to misconfigurations or mistakes.
  - **Malicious Cyber Activity (MCA)**: Network activity infiltrated and attacked by an adversary. Networks can also be victims of insider attacks.  

#### False Positives
- For Network Analysts, distinguishing between normal operation and suspicious activity in which an anomaly is present may be challenging.
- Network detection mechanisms may be used to find Indicators of Compromise (IOC) or anomalies within a network.
- A network-based IOC is information that can be captured on the network and may objectively describe an intrusion of a network.
- The following are some common network-based indicators:
  - IPv4 address
  - IPv6 address
  - X509 certificate hash
  - Domain name
  - Text string
  - Communication protocol
  - File name
  - Uniform Resource Locator (URL)

- These network indicators can be seen at the host level as well; however, they are classified based on where they are initially found within a network.
- Classifying indicators as either host or network assists with operational approach and analytical scheme.
- Because host-based indicators are not in the role or expertise of a Network Analyst, it is imperative that Network Analysts seek assistance from Host Analysts within the team to conduct host-level investigations.   
 

### Creating Visualizations Using WireShark
- Wireshark captures – or sniffs – the network to view traffic that goes through a network adapter.
- Network data streams are sent to specific addresses but can be seen by using a packet sniffer on a network interface that is in promiscuous mode, which informs the (NIC) to accept all traffic it sees on the network.
- **Libpcap** and **WinPcap** are the **kernel-level drivers** that enable sniffing. 

- Wireshark provides analysts with an assortment of options to evaluate network performance graphically based on multiple variables.
- Wireshark is extremely valuable; however, it is not the best tool to understand what is actually traversing the network at a higher level

### Creating Visualizations Using Arkime
- Arkime, formerly known as Moloch, exposes APIs and augments security infrastructure to store and index network traffic.
- The network traffic is captured in standard PCAP format and cataloged in an Elasticsearch database, which provides fast indexed access.
- Because Arkime stores and exports all packets in standard PCAP format, such PCAP-ingesting tools as Wireshark can be used with Arkime to conduct traffic analysis.
- Arkime can ingest the PCAP data and visually display it in different graph formats, such as histograms or line graphs, to easily parse through network data visually to detect outliers within a network environment. 

- PCAP retention is based on available sensor disk space within the network environment.
- Metadata retention is based on the Elasticsearch cluster scale, but these settings can be altered. 

- Arkime consists of three components:
  - **Network Traffic Capture System**: Monitors network traffic, writes PCAP files to disk (permitted storage), parses data, and sends metadata to Elasticsearch.
  - **Viewer**: Transfers PCAP files through Elasticsearch metadata.
  - **Elasticsearch**: Provides statistics and network visualizations of network data from PCAP. 

- Arkime and Security Onion sensors can be configured on a network to have overlapping coverage of network traffic.
- This allows the CDA to pivot to Security Onion, where the information can be gleaned from Arkime analysis. 

<img width="1999" height="1192" alt="5aeddb6d-0c04-4917-95af-d5e14ae56a50" src="https://github.com/user-attachments/assets/eb016c4b-c6f6-41e9-af07-78f3ed2683f4" />


### Creating Visualizations Using Kibana
- Kibana is a search portal and data visualization tool in the Elastic Stack.
- When a network is configured with Elasticsearch and Logstash, Kibana can index log data from network endpoints and aggregate it for search and visualization.
- Kibana can function as a SIEM by correlating data across multiple sources to provide insights about user activity in a network.

- One of Kibana’s most powerful features is the ability to deploy visualizations to help quickly identify anomalous or malicious behavior.

- On production networks, there are often too many logs to parse manually.
- Visualizations allow users to analyze a large amount of data simultaneously and identify patterns that would not have been evident using individual searches.


# MOD 9
## Packet Capture Tools
### Capturing and Processing Network Data
- The most important hardware component for monitoring network traffic is the sensor.
- A sensor is a combination of hardware and companion software used to perform collection of network traffic. 
- This lesson compares the tools **Stenographer**, **Arkime**, **Wireshark**, and **Tcpdump** included in the JDMSS kit.
- Although all the tools mentioned provide Full Packet Capture (FPC) functionality, each tool has key, distinctive features that warrant their use, depending on mission requirements. 

### Environmental Impact of Enabling PCAP
- Packet Captures (PCAP) provide a copy of network data, potentially providing insight into Malicious Cyber Activity (MCA) on a network.
- Newer hardware may automate certain investigation processes, with larger datasets stored and indexed. 
- In hindsight, teams often filter large quantities of data to extract the same streams of network data while not necessarily needing the payload itself.
- There are several disadvantages to having FPC enabled within the network environment, including the following:
  - **PCAPs are voluminous, requiring significant storage space.**
  - **The storage space for PCAPs can be costly.**
  - **PCAPs can be difficult to store, manage, and index.**

- FPC data can cause the sensor to exhaust available hard drive space or malfunction if data retention is not handled properly, as FPC data requires more disk space than other traffic capturing options.
- For example, the FPC data may try to write to the disk more quickly than it can delete, which may cause sensors to malfunction.
- To mitigate this, a CDA should create a separate noncritical partition or system drive.

### Navigating Pitfalls
- A pitfall is anything that results in an unexpected negative outcome within a network environment.
- Several tips and best practices can help CDAs to navigate and avoid common pitfalls. 

#### Use Proper Resources
- The document that usually provides CDAs with the most insight into the overall design of a mission partner’s network environment is the network map.
- Concurrent with CPT Mission Analysis (MA), CPTs must employ passive and active measures to generate current, accurate information regarding MCA and (MRT-C) to produce a logical network map and close information gaps.
- The network map forms the basis of the CPT’s visualization tools and must scale appropriately to the situation.
- As critical input, the network map forms the foundation on which courses of action are developed based on the operational approach and analytic scheme of maneuver employed by the CPT. 

#### Network Ingress/Egress Points
- When the appropriate resources are available, a sensor should be placed at each distinctive ingress/egress point in the network.
- Ultimately, any MCA occurring on a mission partner’s network involves data being communicated into or out of the network environment.
- Sensors attached to these ingress/egress points capture this data. 

#### Visibility of Internal Internet Protocol (IP) Addresses
- When performing detection and analysis, it is critical to determine which sensor is the subject of an alert.
- If sensors are placed in the wrong spot within the network environment, these alerts may be undetected. 

#### Proximity to Critical Assets
- Over-collecting network data can be problematic.
- This should be taken into account when placing sensors within a network environment.
- Driven by operations and intelligence, CPT functions focus on enabling the security of critical assets and MRT-C critical asset function.
- **MRT-C** is defined as, but not limited to, all **devices**, **internal or external links**, Operating Systems (**OS**), **services**, **applications**, **ports**, **protocols**, **hardware**, and **software** on servers **required to enable the function of a critical asset**. 

#### Securing Sensors
- Analysts need to ensure the integrity of the data used for securing critical assets.
- FPC data files contain sensitive information pertaining to the network environment.
- An adversary could leverage this data to expand their foothold within the network environment.
- Key steps to keep sensors intact include the following:
  - Application of the latest updates and security patches.
  - OS hardening.
  - No internet access to sensors.
  - Virtual Local Area Network (VLAN) segmentation.
  - Network/host-based Intrusion Detection Systems (IDS).
  - Two-factor authentication (2FA).


### Specific Requirements
- When selecting a PCAP tool for a mission, requirements for using a specific tool vary greatly from site to site.
- This is because specific system and network requirements depend on the **total bandwidth being monitored**, the **type of network traffic**, **rule sets**, and **policies loaded**.
- Depending on the size and threats faced by a network, sensors may have varying roles within the phases of the collection process.
- The concepts presented help strengthen the decision-making that goes into defining collection requirements. 

#### Characteristics
- The right FPC tool is needed to operate in environments that may contain extremely high sustained network traffic rates.
- Without scaling collection tools to meet throughput requirements within the network environment, Central Processing Unit (CPU) cycles are wasted and incomplete data is gathered. 
- The **most efficient capture tool** is one that **drops the smallest number of packets on the sensor** and **contains enough features to ensure that the network data is accessible and stored**. 

- Every tool has specific requirements needed to properly function and collect data as a standalone sensor within a network environment.
- The FPC tools discussed in this lesson typically have the following characteristics but vary due to traffic rates and data retention:
  - **Large amounts of hard disk space** in a Redundant Array of Independent Disks (**RAID**) configuration for storing network traffic and associated data.
  - A **minimum of 4GB of RAM**, with **at least 1 GB extra for every interface connected** to a Switched Port Analyzer (**SPAN**) port or **network tap**.
  - **One CPU core per interface**.
  - **Multiple network interfaces**, with the **appropriate number and media type required by the SPAN ports** or network taps. 

#### Collection-Only Sensors
- A collection-only sensor **logs collected FPC data and session data to disk** and sometimes generates other data, such as statistical and Packet String (**PSTR**) data based on the collection.
- A collection-only sensor is **barebones**, with **no extra software**.
- Analysis is done separately from the sensor as relevant data is pulled to other devices as needed.

<img width="1999" height="591" alt="fd3afa5d-4d2a-46d5-86a6-54657c78bb9d" src="https://github.com/user-attachments/assets/acfc8a0f-1c72-4c61-9a74-00b1d0b91535" />

#### Half-Cycle Sensors
- A half-cycle sensor is the **most common type of sensor** deployment and performs **detection tasks**.
- For example, a half-cycle sensor **logs PCAP data to disk** but also **runs a NIDS**, either in **real time from the NIC** or in **near-real time against the PCAP written to disk**.
- **Data is pulled back to another device** when analysis is done rather than being performed on the sensor itself. 

<img width="1999" height="555" alt="cd931dd4-7a2f-4d86-804a-f3ffea7bbea9" src="https://github.com/user-attachments/assets/28fb25e7-dab5-4efd-9f98-0f174f022172" />


#### Full-Cycle Sensors
- A full-cycle sensor implements a **full suite** in which **collection**, **detection**, and **analysis** are **all performed on the sensor**.
- Examples include **profiles**, a **User Interface (UI)**, and the **installation of a NIDS UI**.
- A full-detection sensor **performs all the tasks on the sensor** rather than on another device. 

<img width="1999" height="555" alt="bd51923e-6b50-4a87-9916-73ad64030842" src="https://github.com/user-attachments/assets/921e518b-e9ba-4cfb-ba8c-e9e702a283c3" />

### Specific Limitations
- Every tool capable of FPC may be limited due to availability of **compute**, **memory**, or **storage resources**.
- Sensors’ resource limits can be calculated to ensure they meet the minimum requirements to accomplish the mission.

#### CPU
- The amount of CPU resources required to efficiently run an FPC tool depends on the type of sensor being deployed.
- A **collection-only sensor** **does not require a significant amount of processing power**, as the tasks are not processor intensive.
- **Detection** and **analysis** functionality **pulls the most CPU resources**, so **half-cycle and full-cycle sensors require additional CPU cores**.
- The **cores should be mapped to the tools** being deployed in the network environment **before deployment occurs**.  

#### Memory 
- **Collection of network traffic requires a smaller amount of memory** allocated than analyzing network traffic.
- A large amount of memory allocated to FPC tools improves their performance under larger data loads.
- **Stenographer**, however, is a **less resource-intensive tool** and **benefits CDAs in a resource-constrained environment**. 

#### Hard Disk Storage
- No two networks are the same, and proper storage must be allocated for full content stored in PCAP format as well as logs stored in databases such as Arkime.
- Berkeley Packet Filters (**BPF**) can also be **applied to sensors to filter out unwanted data** and **minimize compute resources**. 
- **NOTE**: Networks grow over time, and storage may have to be reevaluated at some point to adhere to a feasible retention period for each data type. 
- To roughly estimate full content data requirements, the formula below can be used to determine the storage needed to store FPC data:
  - **Hard drive storage for one day = Average network utilization (in megabits/second) ✕ 1 byte/8 bits x 60 seconds/minute ✕ 60 minutes/hour ✕ 24 hours/day**
- Hard drive space used by databases may be calculated as follows:
  - **Hard drive storage for databases = 1/10 ✕ full content storage size** 

#### Summary
- Quantifying the differences in the use of various tools can be helpful, especially when attempting to determine specific requirements to avoid limitations.
- However, requirements and limitations vary drastically based upon the network environment, as no two networks are the same.


### Stenographer
- Stenographer is a lightweight FPC utility consisting of several processes for buffering packets to disk for intrusion detection and incident response purposes.
- Stenographer provides a high-performance implementation of NIC-to-disk packet writing, handles deleting those files as the disk fills up, and provides methods for reading back specific sets of packets quickly and easily.
- Stenographer is integrated into Security Onion to essentially function as a sniffer on the sensor.

#### Why a Stemographer?
- Stenographer consists of a **stenographer server**, which **serves user requests** and **manages disk storage.**
- A **stenotype child process** is used to **sniff packet data** and **write the data to disk**, communicating with stenographer by unhiding files when they are read. 
- The user scripts **stenocurl** and **stenoread** provide **simple wrappers around curl** that allow analysts to **request packet data from the stenographer server with ease**.
- Once a set of packet positions is computed for each index file, the user scripts seek the data, read the packets out, and merge them into a single PCAP file.
- The resulting PCAP file is returned via stenocurl as a stream to standard output (**STDOUT**).
- **stenoread** passes the PCAP through **Tcpdump** in order to allow for additional filtering, writing to disk, and printing to readable format.

#### Pros and Cons of Stemographer
<img width="684" height="275" alt="image" src="https://github.com/user-attachments/assets/b343901e-c96c-4372-be4e-0aa08cf1a195" />

#### Use Cases and Limitations
- The design of Stenographer makes it operate poorly with performance characteristics in network environments under certain circumstances.
- Limitations to using the FPC tool include the following:
  - Large PCAPs (above 4 GB) are not efficiently supported.
  - Packets do not show up immediately.
  - Packets are stored in 1-megabyte (MB) blocks.
  - Stenographer flushes one block of data every 10 seconds. 

#### Summary
- Stenographer is a lightweight FPC utility comprising several processes.
- It is best used in situations in which data needs to be written to disk quickly and efficiently.
- Stenographer is also best suited for mission requirements in which PCAP files are not excessively large due to retention policies. 


### Arkime
- Arkime exposes APIs and augments security infrastructure to store and index network traffic, providing fast, indexed access.
- The network traffic is captured in standard PCAP format and is cataloged in an Elasticsearch database, which provides fast indexed access.
- Because Arkime stores and exports all packets in standard PCAP format, such PCAP-ingesting tools as Wireshark can be used with Arkime for deep traffic analysis.
- Arkime can ingest the PCAP data and visually display it in different graph formats, such as histograms or line graphs, to easily parse through network data for detecting outliers within a network environment.
- Arkime is best suited for an FPC system with metadata parsing and searching and can be scaled to handle tens of gigabits per second of traffic. 
- PCAP retention is based on available sensor disk space within the network environment.
- Metadata retention is based on the Elasticsearch cluster scale, but these settings can be altered. 
- Arkime consists of three components:
  - **Network Traffic Capture System**: Monitors network traffic, writes PCAP files to disk (permitted storage), parses data, and sends metadata to Elasticsearch.
  - **Viewer**: Transfers PCAP files through Elasticsearch metadata.
  - **Elasticsearch**: Provides statistics and network visualizations of network data from PCAP. 

#### Requirements
- To use Arkime efficiently, minimum system requirements must be met.
- However, this is dependent on traffic rates and data retention.
- Minimum system requirements are as follows:
  - Enough disk space pertinent to the network environment.
  - Total bandwidth must include Receive (RX) and Transmit (TX) bandwidth.
  - Minimum 6 cores.
  - At least 16 GB of memory. 
- The amount of disk space needed is based on the average network traffic rate, the number of days of data retention required, the number of machines being used to capture data, and the average amount of traffic each machine can handle. 

#### Pros and Cons
<img width="692" height="387" alt="image" src="https://github.com/user-attachments/assets/13818c89-65a6-4776-9bdb-74e8d5b6bf4a" />

#### Best Capabilities
- Arkime uses Viewer to provide data tables to CDAs based on a specific time range within a PCAP file.
- These data tables are best suited to parse and analyze network traffic.
- Below are some best capabilities that make Arkime efficient to use as a CDA. 

##### Sessions
- The primary view is the Sessions page, which contains many controls for filtering sessions.
- When viewing Arkime session details, an additional packets section is visible under the metadata sections denoted by the green plus (+) icon.
- When expanded, the PCAP data can be analyzed.  
<img width="2001" height="1198" alt="61b1a7f8-7053-483e-9ea6-e5cb3421c6f3" src="https://github.com/user-attachments/assets/dc1c9f99-513d-4056-86bd-fa17e836484c" />

##### SPIView
- Session Profile Information (SPI) displays unique values with session counts for each of the captured fields.
- This page lists session/log metrics such as protocol, source and destination IP addresses, and source and destination ports.
- The values can be expanded for further analysis to display the top network values along with each value’s cardinality for the fields of interest.
<img width="2001" height="1189" alt="d623eaf6-c6fb-4299-ba86-dc8bfb1f84ad" src="https://github.com/user-attachments/assets/882169be-aa98-419a-8116-db769af64ec5" />

##### SPIGraph
- SPIGraph displays a temporal view for the top values of any field.
- The top values are visually represented over time as well as geographically.
- This can be useful for identifying trends within a network environment over time.
- For example, traffic using a particular protocol when seen sparsely at regular intervals on the histogram may indicate an IP check by an adversary, polling, or beaconing. 
<img width="2001" height="1189" alt="1b12d4cc-b2b9-4036-b5df-a3c5843839f5" src="https://github.com/user-attachments/assets/c5c2db75-50d2-478a-a762-970426e5d9d7" />

##### Connections
- Connections displays the connection points of the IP addresses communicating across the network environment.
- This makes it easy to visualize logical relationships between network hosts. 
<img width="2001" height="1189" alt="e2fbe201-1bf0-41cb-9265-f075ce9154eb" src="https://github.com/user-attachments/assets/bf197568-cc4a-44d1-9079-852f84e8360a" />

##### Files
- File sizes of PCAPs on disk, imported PCAPs, and all files that are viewable can be seen under the Files tab. This is useful for resource purposes.
<img width="2001" height="805" alt="cb760402-665f-4a37-bd89-eee0a54b1fc9" src="https://github.com/user-attachments/assets/c23fed19-8498-47f3-bbf9-f6141dc767de" />

##### Stats
- Statistics for each node and index may be seen under the Stats tab.
- The sub-tabs that follow, such as Capture Stats, display capture resources that a certain FPC uses. 
<img width="2001" height="412" alt="4234f2ce-83c2-4b28-890c-fd45bcadd49b" src="https://github.com/user-attachments/assets/6ed98d45-06fb-40e6-8312-5f713f0483e7" />

#### Summary
- Arkime can be used to apply search queries to analyze suspicious network activity within a mission partner’s network environment.
- Arkime also parses Hypertext Transfer Protocol (HTTP) source content and decodes data.
- This assists trainees in identifying MCA across a network while identifying OSs of a network device based on traffic and pinpointing OS fingerprinting activities.

### Wireshark
- Wireshark captures – or sniffs – the network to view traffic that goes through a network adapter.
- Network data streams are sent to specific addresses but can be seen by using a packet sniffer on a network interface that is in promiscuous mode, which informs the NIC to accept all traffic it sees on the network.
- **Libpcap** and **WinPcap** are the kernel-level drivers that enable sniffing. 
- Wireshark first captures the data from a network interface and then processes the capture by breaking it down into segments, packets, and frames.
- The data is then presented in the context of addressing, protocols, and data.
- Wireshark provides analysts with an assortment of options to evaluate network performance graphically based on multiple variables.

#### Requirements
- To use the Wireshark FPC tool effectively, minimum system requirements must be met.
- However, this is dependent on traffic rates and data retention.
- The minimum system requirements are as follows:
  - 32-bit x86 or 64-bit CPU.
  - At least 400 MB of available RAM; must be more to retain larger PCAP files.
  - At least 300 MB of available storage space; must be more to retain larger PCAP files.
  - NIC that supports promiscuous mode.
  - WinPcap/libpcap capture driver.
  - Use Cases and Limitations
- Wireshark can handle a wide range of protocols, but its primary functions are to capture packets and display them.
- Like any other tool, Wireshark is best suited when it is the right tool for the job.
- Below are scenarios when the use of Wireshark is ideal:
  - To search for a certain protocol or stream between devices.
  - To search for the root cause of a known problem.
  - To analyze time stamps, protocol flags, or bits.
- Although not ideal, Wireshark can also be used for the following:
  - To discover which devices or protocols are top talkers.
  - To view a rough network traffic picture.
  - To analyze conversations and endpoints between devices.
- Wireshark tends to require large amounts of storage for large PCAP files, making it less than ideal in some situations.   

#### Pros and Cons
<img width="691" height="271" alt="image" src="https://github.com/user-attachments/assets/d9ed9dd2-b5db-4ad0-89f1-6d652c790b11" />

#### Summary
- Wireshark may be ideal for determining the root cause of an understood problem.
- However, the tool may not be ideal for browsing network traffic or making high-level judgments about a mission partner’s network.
- Wireshark can be overwhelming to use and is best used when the problem or situation is understood beforehand. 


### Tcpdump
- Tcpdump is a popular FPC and analysis tool included in the JDMSS kit.
- It is useful for command-line packet analysis to parse through data rapidly.
- Tcpdump is integrated into Security Onion but not enabled by default.
- The libpcap library allows Tcpdump the ability to capture packets from the wire, analyze packets, troubleshoot connection issues, and apply filters as it uses the BPF syntax. 

#### Pros and Cons
<img width="693" height="321" alt="image" src="https://github.com/user-attachments/assets/34a84c20-027a-4502-a7a4-eb059bd732ec" />

#### Use Cases and Limitations
- Tcpdump can handle a wide range of protocols.
- Unfortunately, some limitations exist when it comes to using Tcpdump, as the tool relies on the analyst for much of the interpretation of individual packets.
- This may seem counterintuitive, but it challenges the analyst and provides a fundamental understanding that can be better applied to any packet analysis tool. 

##### Uses
- Uses of Tcpdump include the following:
  - A web browser is hanging and cannot load pages from its server. Run Tcpdump to check the following:
    - Domain Name System (DNS) query
    - HTTP request to server
    - Server response
- Debug an attack, such as Denial of Service (DoS). Run Tcpdump to check the following:
  - Source address
  - Destination address
  - Type of traffic

##### Tcpdump on an Interface
- When running Tcpdump on an interface, the packet is copied on a switch ingress to the Switch Card Control Processor (SCCP), which then sends the packet to the host to be captured. 

##### Limitations
- One of Tcpdump’s greatest limitations is that it does not use Wireshark’s protocol dissectors.
- This means that Tcpdump does not interpret Layer 7 protocol information and is limited by network hardware.
- When running Tcpdump, a hardware switch interface **limits packets to 200 per second**.
- Therefore, if Tcpdump is running on an interface processing more than 200 packets per second, the output PCAP file does not include all the packets.
- A capture filter cannot be used to avoid this limitation, as it refers to the total volume of packets traversing the interface at the moment of capture. 


### Read Packets from a File
- If CDAs are engaged reading network data from large PCAP files, it may become imperative to use filters to parse through relevant data only.
- TCP uses the BPF format like Wireshark capture filters. 

#### BPF
- The BPF syntax uses primitives, which consist of a single statement combined with one or more qualifiers, followed by a value such as a name or ID number.
- Primitives refer to a section of a protocol header such as port, host, or TCP port.
- Table 9.1-5 details which qualifiers are used to form an expression.
  ![c1874f3c-c79d-4ac8-9473-36a3cb392f21](https://github.com/user-attachments/assets/7808e2cf-5deb-422e-8f95-25e0b557431f)

  ![7615574b-c0f8-46a3-85af-456738be9f68](https://github.com/user-attachments/assets/6b6f4bcf-ddfc-4ee8-9de3-1235ff4a3507)

#### TCP
<img width="694" height="77" alt="image" src="https://github.com/user-attachments/assets/3f4c2fb3-b99a-4992-88b6-2c185294017c" />

#### UDP
<img width="613" height="60" alt="image" src="https://github.com/user-attachments/assets/b1f509ca-88c8-425a-85de-7b3d77753110" />

##### tcpdump SYNTAX
- Read the network traffic from the PCAP: `tcpdump -nr hunt.pcap`
- Triple the Verbosity: `tcpdump -nvvvr hunt.pcap`
- Display each packet in hex: `tcpdump -nvxr hunt.pcap`
- Display in ASCII and hex: `tcpdump -nvXr hunt.pcap`
- Create a new file containing only packets matching BPF Filter: `tcpdump -nvr hunt.pcap 'tcp dst port 80' -w hunt-2.pcap`
- Read newly created file: `tcpdump -nvr hunt-2.pcap`
  


## Packet Capture Methods
### Reasons for Full Packet Capture
- FPC offers benefits for the cybersecurity world that are similar to those of a security camera in the physical world.
- When a building is broken into, an investigation is easier if a security camera records the incident.
- The same principle applies to a monitored network.
- FPC allows for the re-creation of network events and can significantly help a cybersecurity incident response.
- Network security monitoring tools often use a negative security model, meaning that they alert only if they detect an intrusion.
- Before alerting, however, the tools need a signature or heuristic to detect.
- These same tools are often “late to the fight,” and detection for an investigation is too late.
- FPC can assist in this by providing the most complete picture of the events.
- For example, a cyber defender might use a PCAP to identify where an undetected exploit or attack occurred.

### Analyzing Packet Capture Files
#### Libpcap\WinPcap\Npcap Libraries
- The Libpcap PCAP software library is the main library used in such tools as **Wireshark**, **Tcpdump**, and **Snort**®.
- Although Libpcap was created in 1994, it remains the de facto standard for Linux today.
- For Windows, Libpcap was ported over as WinPcap. Although it is still available, WinPcap is no longer maintained or supported and has been replaced by Npcap.

#### File Format
- For all versions of Libpcap, the standard file extension is .pcap
  <img width="1999" height="804" alt="2368925e-6c84-4290-a8fd-a74391837e2a" src="https://github.com/user-attachments/assets/f486de45-1ddd-4ae0-b806-c5f51a49826d" />

#### Global Header
- The global header contains the following attributes:
  - **Magic Number** (**magic_number**): Used to detect the PCAP file format and the byte ordering. The magic number for a PCAP file is 0xa1b2c3d4.
  - **Major and Minor Version Numbers** (**version_major/minor**): Current version is 2.4.
  - **Time Zone** (**thiszone**): Provides the time zone of the PCAP file, relative to the time in seconds from Greenwich Mean Time/Coordinated Universal Time (GMT/UTC).
  - **Significant Figures** (**sigfigs**): Theoretically used to denote how accurate the time stamps are. In practice, this attribute is usually 0.
  - **Snapshot Length** (**snaplen**): The amount of data used for each frame in the capture file. Typically, this number is 65535, but it could be limited by the user to reduce storage during the network capture.
  - **Network** (**network**): The link-layer header type that specifies the type of header of the capture type, such as 1 for Ethernet and 105 for 802.11 (wireless).

#### Packet Header
- Each packet header contains the following attributes:
- **Timestamp Seconds** (**ts_sec**): The date and time when the packet was captured, represented in seconds since the epoch January 1, 1970 00:00:00 GMT.
- **Timestamp Microseconds** (**ts_usec**): For regular capture files, represents the microsecond (ms) offset from the Timestamp Seconds attribute.
  - For nanosecond (ns)-resolution capture files, represents the ns offset from the Timestamp Seconds attribute.
- **Original Length** (**orig_len**): The length of the packet as it appeared on the network when it was captured.
- **Included Length** (**incl_len**): The actual number of bytes captured and stored in the packet.
  - This number could be less than the orig_len if limited by snaplen in the global header.

#### Packet Data
- The packet data portion of the PCAP file contains the data coming from the network, including link-layer headers.

#### PCAP Next Generation
- Wireshark’s default capture output is PCAP Next Generation (.**pcapng**).
  - This file format is an enhanced version of the .pcap specification.
- A few .pcapng enhancements are as follows:
  - Combine captures from multiple interfaces into a single file.
  - Tag capture files with information about the Operating System (OS), hardware, and sniffing software.
  - Add metadata attributes to each interface, including the interface name, dropped packets, and capture filter used when sniffing.
  - Add text comments to the whole capture file as well as to individual frames.
- To view the **metadata** of a capture file, an analyst may open the file in Wireshark and navigate to **Statistics > Capture File Properties**.
- With the added metadata information, .pcapng files carry a **higher Operations Security (OPSEC) risk**.
- Analysts should sanitize any extraneous information from the .pcapng file before sharing it with external parties.

### Capinfos
- During an investigation, remaining organized and thoroughly documenting work during an investigation can be challenging, especially when time is limited.
- A Network Analyst may begin one PCAP analysis, move to another, and quickly lose track of their progress.
- Giving each PCAP file a distinctive, descriptive name helps with organization.
- Even so, analysts commonly lose track of how and where a PCAP file was generated. Tools like Capinfos can help keep track of stored PCAP files.
- **Capinfos** is a **command-line tool** that can **read one or more capture files** and **output statistics and metadata** related to them.
- The output may be in long or table format.
  - Long format is meant to be human readable, whereas the table format is useful for generating a report for a spreadsheet or database.
  - By default, Capinfos outputs the results in long format.
- The analyst may customize the output by using command-line options.

<img width="1999" height="1048" alt="ce205f03-ded8-40ce-9984-4ed989bc26c0" src="https://github.com/user-attachments/assets/f1ebb61a-5884-4ead-b91f-0e675b9685b6" />

- Run the following to record metadata of a pcap: `capinfos -A CTI-Hunt-fn1.pcap`

<img width="644" height="477" alt="image" src="https://github.com/user-attachments/assets/74e028fe-f6fa-4259-93f1-ceea8f788afe" />

### Capturing Network Traffic
- Two common methods are available to capture network traffic: **port mirroring** and **hardware taps**.
- Switched Port Analyzer (SPAN) is the Cisco® implementation of a port mirror on a switch.
  - Both a SPAN port and tap duplicate traffic to a listening system, or sensor.
  - However, a SPAN port is a software capability of a switch, whereas a tap is a physical hardware device that connects directly into the cabling portion of a network.
- Over time, SPAN has been developed to include different configurations that help overcome its original limitations of only allowing mirroring on one device.
- Remote SPAN (RSPAN) and Encapsulated Remote SPAN (ERSPAN) are two common configurations that meet different requirements, such as allowing mirroring over more than one device and allowing mirroring over Internet Protocol (IP).

### Local SPAN
- A SPAN port is the simplest way to get packets to a sensor because it uses existing hardware.
- Creating a SPAN port is also known as port mirroring, because the SPAN port reflects the duplicated traffic from one or more ports or Virtual Local Area Networks (VLAN) to another destination.
- If the destination is a network analyzer, then the mirrored data can be reviewed for malicious activity.
- Source SPAN interfaces monitor received, transmitted, or bidirectional traffic.
- Traffic entering or exiting the source interface is then mirrored to the destination interface.
- SPAN sessions come with some limitations.
- A Network Analyst should be familiar with these restrictions so they can identify when a SPAN port can be used.
  - Sources can be physical ports or VLANs but cannot be mixed in the same session.
  - SPAN sessions do not interfere with the normal operation of a switch. However, an oversubscribed SPAN destination interface may drop packets.
    - For example, a 100-megabits/second (Mbps) source interface mirroring to a 10-Mbps destination interface may not be able to duplicate traffic quickly enough for complete reproduction of traffic.
  - SPAN sessions may be configured on disabled interfaces. The SPAN session does not become active until a destination interface and at least one source interface or VLAN for the session are enabled.
  - Missing data is not forwarded on a SPAN port.
  - A switch prioritizes normal network traffic over SPAN traffic, which may result in dropped packets when mirroring traffic.

- Full-duplex interfaces are challenging for SPAN sessions.
- A full-duplex source port requires, in essence, a destination interface that is capable of at least double its speed.
- Because all send and receive traffic from the source interface must only be transmitted from the destination interface, a full-duplex source port with 100% utilization has only half of its traffic mirrored by a SPAN session if the destination interface is capable of only the same speed as the source.

- Given the above limitations, SPAN is still the most common technology used for mirroring network traffic on Cyber Protection Team (CPT) missions.
- Because they do not need additional hardware or network outages for installation, SPAN sessions are typically the most palatable option for duplicating network traffic.

### RSPAN
- Local SPAN sessions are limited to a single device. In other words, if a mission partner has 10 switches that require network analysis, 10 separate SPAN sessions and destination interfaces must be configured.
- This also requires 10 separate interfaces for capturing the traffic for analysis.
- To combat these limitations, RSPAN was developed to allow for mirroring traffic from multiple devices into a single destination.
- **RSPAN** is an **Open Systems Interconnection (OSI) Layer 2 configuration**.
- It works by mirroring the traffic of the source ports of an RSPAN session onto a dedicated RSPAN VLAN.
- The VLAN is trunked to the other switches, allowing its session traffic to be transported across multiple switches.
- On the switch with the destination port for the session, traffic from the RSPAN VLAN is mirrored out of that destination port.
- Unlike local SPAN, where the source and destination ports exist on the same switch, the source and destination ports for RSPAN reside on different switches.
- This requires a separate RSPAN source session to be configured in the switch and a separate RSPAN destination session to be configured in the switch.
- Although RSPAN configuration in the switch may be more time consuming because a VLAN must be created and RSPAN sessions configured, it is still more efficient at monitoring multiple devices on the same network.
- For example, an organization that needs three devices configured with port mirroring still chooses RSPAN due to its resource efficiency. 

### ERSPAN
- Local SPAN and RSPAN are limited to a single location.
- They do not have the ability to monitor traffic that does not occur on the Local Area Network (LAN).
  - This means that for organizations with networks spread across different states or countries, monitoring must be done locally at each physical location.
- ERSPAN was developed to allow the transportation of mirrored traffic over IP. 

- **ERSPAN supports OSI Layer 3** and, with that, the ability to **route mirrored traffic across multiple physical networks**.
- This is beneficial for organizations with multiple distributed network environments that are geographically spread out.
- ERSPAN encapsulates mirrored traffic via Generic Routing Encapsulation (**GRE**) to a distant endpoint.
- The traffic is encapsulated at the source router and is transferred across the network.
- The packet is decapsulated at the destination router and then sent to the destination interface.

- Although ERSPAN has benefits over local SPAN and RSPAN, it includes some disadvantages.
- ERSPAN is quite **resource intensive**.
- A CPT should consider this when deciding on a port mirroring configuration.
- An organization that decides to use ERSPAN must ensure appropriate resource availability exists.
- ERSPAN is also **only supported on Cisco Catalyst Switch Series 6500/6000**. 

- Setting up a SPAN Port on a CISCO catalyst switch:
  - Configure FE 0/1 as source interface: `Catalyst-3550(config)# monitor session 1 source interface fastethernet 0/1`
  - Configure FE 0/24 as the destination interface: `Catalyst-3550(config)# monitor session 1 destination interface fastethernet 0/24`
  - Verify the SPAN config is set up correctly: `show monitor session 1`
  <img width="291" height="130" alt="af5bf3e5-18bd-4c2c-8a02-0ec7612b4b9f" src="https://github.com/user-attachments/assets/ab8d661a-4e2e-419f-aca6-2ba4cd2913e4" />

### Network TAP
- A network tap is a hardware component that connects with the cabling infrastructure of the network and copies packets for monitoring.
- A tap sits between two endpoint devices and the traffic is seen and then copied for visibility of the networked traffic. 
<img width="1999" height="881" alt="bc0ce2fa-01f8-439e-a645-f1fa7a8e58c7" src="https://github.com/user-attachments/assets/f92e2853-39b4-4e73-9490-ef2fb2e2121f" />

- The two primary types of taps are passive taps and active taps. 
  - Passive taps require no power of their own and do not directly interact with any network components.
    - A passive tap works by using a splitter to send the transmitted traffic of both endpoint devices to two separate monitor links.
    - The splitter uses a split ratio to determine how much light is being diverted to the monitoring links.
    - For example, if there is a 70/30 split, 70% of the original light is being sent to its destination, and 30% is being sent to the monitoring link.
    - The monitor links only transmit traffic with no ability to receive traffic or pass traffic back into the system. Two monitoring links are used to prevent oversubscription.
    <img width="1999" height="964" alt="d2d8da01-25a1-4e56-868f-8ab48934492a" src="https://github.com/user-attachments/assets/8d668ea8-ad6d-41a8-b20d-9cca01f0e270" />

  - Active taps require their own power source to regenerate signals.
    - There is no split ratio to consider with active taps because they receive the traffic and retransmit it to both the network and monitoring destinations.
    - Active taps are preferred over passive taps in environments where the light levels are too low to use a splitter or where a signal needs to be converted.

- Outages may occur when connecting a tap to a network cabling infrastructure that is already built.
- To help avoid this, it is best to set up taps as the infrastructure is being built.
- If possible, taps should be set up on any critical network link even if it is not continuously monitored.
- In the event of a breach or when troubleshooting requirements, having taps set up at those critical links allows the data to be more readily available.
- Whereas passive taps are generally preferred due to not being impacted by power outages, more active taps are now being made with failovers that provide redundancy, like a battery backup.

- Both active and passive taps are available in various forms of media, such as **copper** and **fiber**, and can perform **media conversion**.
- The speed of the tap may range from **10/100/1000 Mbps** all the way up to **100 gigabits/second** (Gbps).
- Another consideration when looking at taps is their size.
- Taps come in different form factors, including portable size and rack-mountable and high-density chassis.
- The form factor, speed, and media type are all dependent on the needs of the network.
- Determination of which tap to use on a network should include consideration of factors like scalability and cost.
- Generally, taps with faster speeds and larger form factor are going to be more expensive than their smaller, slower counterparts.

- Network taps can also perform multiple functions and modes from the same device, which allows the option of flexibility within the network.
  - **Tap “Breakout”** is the most common function for a tap. This mode sends each side of traffic to separate monitoring interfaces. The tap does this to ensure no packets are lost to high-priority monitoring tools.
  - **Aggregation** works by merging both sides of traffic streams into one monitoring interface. By doing so, appliance costs are reduced and packet broker interfaces are consolidated.
  - **Regeneration** creates multiple copies of network data to send to multiple sensors from one tapped link. This has the ability to send the same data to multiple destinations.
  - **Bypass** prevents a single point of failure in the network by bypassing the failed device to reduce network downtime.
  - **Filtering** allows filtering of specific data streams to be monitored. Filtering also prevents oversubscription of interfaces during aggregation.

### Monitoring Host Traffic
- Some of the tools used to monitor traffic directly on a host are programs like **Wireshark** and such utilities as **netstat** on Windows and Linux and **Tcpdump** on OS X.
- Such tools enable the analyst to view host network traffic information, such as host connections, process Identifier (ID) of connections, and more comprehensive network data.
- Using netstat on a Windows or Linux machine provides access to detailed network traffic information happening on the host.
<img width="1999" height="575" alt="a2b72fa2-f086-4372-bc87-98977a27437c" src="https://github.com/user-attachments/assets/9df811f8-c5ed-4379-81d0-cf9a87d252f1" />
<img width="1999" height="575" alt="a557231d-119a-4dee-b226-7000f4010819" src="https://github.com/user-attachments/assets/402bbb4c-bda8-472c-8663-5b10d2e29260" />

### Comparing SPAN, Tap and Host
- Each network traffic monitoring tool has its own best use case scenario and situations in which other options would be optimal.
- Decisions on the best monitoring tool for a network configuration should include consideration of such factors as cost, installation, administration, life span, redundancy, and ease of use.
- A passive tap is simple to install and requires no power of its own, although if a network is already set up, an outage occurs when installing the tap.
- Host-based network monitoring may be done without additional software or hardware and uses utilities and tools already on most systems.
- However, host-based network monitoring is best used for individual devices and is not considered a viable option for large-scale monitoring.
- SPAN is a tool that requires no physical installation, which means it can be accessed and set up remotely.
- Certain SPAN configurations, like ERSPAN, are available only on specified models and platforms, so its compatibility is greatly reduced.

## Port Mirroring
### Mirror and SPAN Ports
- Port mirroring is the process of copying traffic that traverses through one or more interfaces of a networking device and sending the duplicated traffic to a specified destination.
- Normally, this is accomplished through the use of switches, but it is not necessarily limited to such devices.
- A version of port mirroring is Cisco’s Switched Port Analyzer (SPAN) port, which was at one point proprietary and has since been adopted by other networking specialized software producers.
- Although it is common for mirrored ports and SPAN ports to be used interchangeably in conversations, SPAN ports are just one variation of port mirroring.
- Mirrored ports should not be confused with network taps, as a network tap is a physical device with several key functional differences that are discussed later in this section. 

#### Types of Port Mirroring
- Port mirroring is categorized according to where the duplicated data is sent and how it is accomplished.
- When duplicated data is sent to an interface located on the same physical device, it is simply defined as local port mirroring.
- If the duplicated data is forwarded to a separate physical device’s port, it is defined as remote port mirroring.
- Cisco further split remote port mirroring into two forms of SPAN ports: Remote SPAN (**RSPAN**) and Encapsulated RSPAN (**ERSPAN**).
  - RSPAN sends duplicated data to a destination interface via a dedicated Virtual Local Area Network (VLAN) tunnel.
  - ERSPAN uses Internet Protocol (IP) routing to send duplicated data to a destination.
- Furthermore, each form of mirroring can be configured to duplicate traffic unidirectionally (either inbound or outbound) or bidirectionally. 

#### Port Mirrors vs. Network Taps
- The primary difference between a mirrored port and a network tap is that a network tap is a dedicated device that does not have any influence on data that it replicates, whereas a mirrored port potentially modifies packet timestamps and omits such lower-level data as corrupted or malformed data.
- When considering mirrored ports, identify whether they introduce significant networking overhead.
- Mirrored traffic is treated with lower priority and introduces some overhead, with the main factor being the volume of mirrored data.
- Although network taps do not introduce networking latency, they are typically expensive solutions that pose a point of failure, as they must be installed inline on key network locations.

- When considering whether to implement a network tap or port mirror for data analysis requirements, an analyst must consider the benefits of a mirror port.
- Port mirroring is effectively a free solution, as it utilizes existing infrastructure.
- Additionally, it is virtually invisible on a network because it requires only a configuration change and can be configured remotely as long as the devices on which the mirroring is implemented are remotely accessible. 
 
#### Port Mirroring vs. Traffic Mirroring
- Port mirroring and traffic mirroring are both considered mirroring functions, but there is one major difference.
- Port mirroring replicates all data, whereas traffic mirroring applies a filter before replicating data in order to only reproduce a portion of the data that passes through an interface.
- With SPAN ports, traffic filtering is done by applying an Access Control List (ACL).
- The type of ACL and specificity of the ACL are dependent on the device vendor.

#### Port Mirroring Best Practices
- The primary use case for Network Analysts to implement a mirrored port is to capture data for analysis.
- When implementing mirrored ports, several best practices must be taken into account:
  - Know where traffic flows in order to capture the data intended for analysis.
  - Avoid capturing excessive data in order to preserve storage and avoid the risk of introducing network latency.
  - Identify whether a filter can be used to reduce irrelevant data from being captured.
  - Avoid capturing duplicate data.

### Placing SPAN Ports
1. Utilize Putty to SSH onto the router
2. enter `enable` then `config t` to get to the Configuration mode
3. Run the following to enter mode to configure ERSPAN session: `monitor session 1 type erspan-source`
4. RUn the following to configure ERSPAN session source interfaces: `source interface g3-4`
5. Run the following commands to specify traffic destination (Using a GRE tunnel): `destination`, `erspan-id 1`, `ip address 199.63.64.31`, `origin ip address `75.21.1.2`
6. Exit

#### Verify ERSPAN with Wireshark
1. From Wireshark, specify the port to listen on and filter for GRE protocol using `ip proto 0x2f` in the `...using this filter:` block.
2. From a different workstation, ping the device you want to test `ping -t 103.28.93.2`
3. On Wireshark, verify no traffic is captured yet. This shows ERSPAN is not enabled.
4. Run the following to enable ERSPAN: `no shutdown`
5. Re-verify that traffic is being captured.

### Implement Port Mirroring
1. Use putty to enter into a router `putty.exe -ssh trainee@<IP>`
2. Run `configure` to enter VyOS config mode
3. Run `set interfaces ethernet eth2 mirror eth3` to config a local port mirror
4. Run `sudo tcpdump -i eth3 host not 200.200.200.2 and not arp` to confirm it has not been set up yet
5. Run `commit` to set up the port mirror

### Implement Filters
1. Run `ping -t 103.28.93.2` to verify cmd.exe is still running the ping
2. Putty into the router, and enter the config mode `putty.exe...` 'enable`, `config t`
3. Run the following to define an ACL filter: `ip access-list extended no-icmp`, `deny icmp any any`, `exit`
4. Run the following to modify config for ERSPAN `monitor session 1 type erspan-source`
5. Run the following to apply filter and exit editing `filter access-group no-icmp` `exit`
6. Verify in Wireshark no packets are being captured
7. Re-enter the config modification
8. Run the following to remove the filter: `shutdown`, `no filter access-group no-icmp`, `no shutdown`, `exit`
9. See that Wireshark has captured packets again


### Platform Variety and Implementation
- The exercises in this lesson introduce trainees to the process of implementing mirrored ports, but although the process may vary among platforms, the key concept of facilitating data collection is the same.
- The objective of port mirroring from the eyes of a Cyber Defense Analyst (CDA) should be that mirroring ports is the means for collecting data for analysis. 

#### Platforms
- As shown in the labs, port mirroring can be achieved on more than one type of platform.
- Based on the platform, capabilities and constraints must be taken into account.
- These factors may be proprietary technology, such as Cisco’s SPAN ports, or legacy system versions, as is the case with the VyOS device.
- The key takeaway for CDAs is that they need to work with network technicians to identify those mirroring capabilities and limitations when factoring in cyber terrain visibility during operations.

#### Implementation
- Other than fringe cases, several highly common use cases for port mirroring exist, and all involve some type of monitoring functionality.
- The most applicable for this Cyber Defense Analyst – Network (CDA-N) course and common use case is to facilitate data collection for cyber defense analysis in one form or another, such as monitoring for malicious traffic.
- Other common use cases include monitoring specific business critical services, tracking network metrics, and troubleshooting services.


## Capture Planning
### PCAP Buffers Explained
- Packet capture is an extremely powerful tool for Network Analysts.
- Full packet captures allow analysts to forensically investigate an entire stream of network traffic.
- Capturing the entire stream of network traffic collects not only the underlying data being transmitted but also all header information for the traffic, such as source, destination, and protocol type.
- Packet capture can be a powerful tool, but there are also numerous pitfalls to consider.

- Performing packet capture is an extremely disk-intensive operation.
- The amount of disk space required for the storage of PCAPs is equal to the size of the traffic traversing the interface.
- On a busy link, this can consume disk space at a rapid rate.
- Additionally, as traffic continues to traverse the interface, PCAP files can become large. Large PCAP files can be difficult and inefficient to work with.

- PCAP buffers can assist in alleviating the storage and efficiency burden of PCAP operations.
- Because there is a wide variety of PCAP tools that approach PCAP operations in different ways, the specific options pertaining to buffering may differ between these tools.
- Regardless, the intent and results are often the same.

- Consider the following example of a PCAP implementation:
  - An organization decides to implement packet capturing on a primary edge router.
  - The organization has made the decision to capture all traffic that is traversing the edge of the network in either direction for forensic and analysis purposes.
  - During the 8 peak hours of the day, the 100-megabit-per-second (Mbps) network link is 100% utilized.

- In this example, a 100-Mbps link that experiences 100% utilization over 8 hours would result in a total data volume of 360 gigabytes (GB).
- If the PCAP tool did not implement any kind of buffering, the result would be a single PCAP file that is 360 GB in size.
- A PCAP file of this size would be extremely difficult and inefficient to work with.

- For this reason, PCAP tools offer varying capabilities to tune how PCAP files are written.
- Most tools allow PCAP files to be limited by size or time.
- Some tools also allow PCAP files to be rotated when disk space is full.
- Rotating files allows old PCAPs to be deleted as new ones are written to the disk.

### PCAP Buffers in Arkime
- Arkime is a packet capture and searching tool. Arkime consists of a capture service and viewer service.
- The capture service listens on an interface and performs packet-capturing operations.
- The viewer service provides a web interface allowing an analyst to work with the captured PCAP data.
- **NOTE**: Arkime was formerly called **Moloch**.
  - Some commands and outputs in the lab that follows may make references to Moloch, as not all Arkime components use the new name.
  - Lab steps provide the necessary commands to be used based on the scenario.

- In addition to the Arkime capture and viewer services, **Arkime** relies on **Elasticsearch**.
- **Arkime** leverages **Elasticsearch** to index session-related information from PCAPs processed by Arkime.
- Actual PCAP files are written to the disk on the system running the Arkime capture service.
- Although understanding the minutiae of this configuration for the following lab is not necessary, it is important to understand that raw PCAPs are stored on the system running the Arkime capture service.
- Buffer settings explained in the lab impact how the capture service processes PCAPs on the capture system, not the data in Elasticsearch.
- Arkime has several settings that impact how PCAPs are written to disk on the system running the capture service. 
<img width="1999" height="883" alt="011d7dce-5181-4518-a2b0-630f27df19b8" src="https://github.com/user-attachments/assets/2220acac-09a4-4f10-ae78-d0cf16827860" />

#### Modify PCAP Buffer Configs in Arkime
1. Run sudo su to change to root
2. RUn the following to view contents of folder where PCAPs are written: `ls -la /data/molach/raw/`
3. Run the following to stop Arkime capture and viewer services: `systemctl stop molochcapture.service molochviewer.service`
4. Open the Arkime config file `/data/molach/etc/config.ini`
5. Locate the config Items `maxFileSizeG` and `maxFileTimeM` (lines 79 and 83)
6. Set `masFileSizeG` to 1 and `maxFileTimeM` to 1
7. Locate the `freeSpaceG` on line 112 and set it to 15%
8. Run the following to restart Arkime: `systemctl start molochcapture.service molochviewer.service`
   

### PCAP Buffers in Stenographer
- Stenographer is a lightweight PCAP utility designed to write packets to disk quickly, up to 10 gigabits per second (Gbps).
- Stenographer also manages disk usage and automatically rolls over the oldest packets when it runs out of storage.
- Because Stenographer was designed for speed, it lacks common PCAP functionality, such as a web interface, custom query language, and complex Transmission Control Protocol (TCP) stream reassembly.
- **Stenographer** indexes packets only up to the **Transport layer** of the Open Systems Interconnection (OSI) model, and queries are written in Berkeley Packet Filter (**BPF**) syntax. 

- Unlike Arkime, **Stenographer does not write data to disk in the standard PCAP format**.
- Instead, **Stenographer writes raw data to disk directly from the AF_PACKET buffer**.
- This format consists of blocks of capture data that contain a small header followed by a linked list of packets.
- The result is that the stored capture files cannot be opened directly with traditional network analysis tools like Wireshark or Tcpdump.
- To retrieve packet data, analysts must query Stenographer with such tools as Stenoread or Stenocurl.

- Stenographer has two main components that control packet capture:
  - The **Stenographer process is the main server** that is responsible for processing user requests, managing disk utilization, and execution of the Stenotype child process.
  - **Stenotype** is the **engine responsible for sniffing packets** on the wire, indexing those packets, and then writing the data to disk.

- Stenographer and Stenotype both offer useful configuration options to assist analysts with managing disk utilization, capture buffers, and output file sizes

<img width="800" height="557" alt="9732fe47-8a89-4404-8393-37ab6c2bbd25" src="https://github.com/user-attachments/assets/01818033-1609-46cf-97e6-edb057fbaee7" />

#### Stenographer Capture Optimizations
1. SSH onto manager node, and change to root: `ssh -l trainee 199.63.64.92` `sudo su`
2. In a new tab, ssh into the forward node and sudo: `ssh -l trainee 199.63.664.93` `sudo su`
3. From the forward node, view th config file that controls behavior of stenographer: `cat /opt/so/conf/steno/config`
4. From the forward node, use `ps -ef | grep stenotype` to view settings applied to running stenotype process
5. Switch to Manager node, and view the sensor.sls file, this is the config file to modify and pus hcahnges to forward node: `cat /opt/so/saltstack/local/pillar/minions/onion-fn1_sensor.sls`
6. Open this file in a text editor, and change the `steno` section to reflect the following:
   <img width="242" height="136" alt="image" src="https://github.com/user-attachments/assets/bd552dc9-a6eb-458c-b5cd-441062220b26" />
7. Switch back to forward node, and run `so-pcap-restart` to force node to get updates.
8. View the config file and verify changes are applied:
   <img width="1004" height="496" alt="5ce7b4bd-96a4-4b85-82fc-2bf4edd9304b" src="https://github.com/user-attachments/assets/a076e9db-41e7-4fe6-ac29-8d5b87ab553f" />
9. run the `ps -ef | grep stenotype` to verify visible config options are applied.


### BPF Introduction
- Capturing all traffic in PCAP operations is an effective approach to have a complete forensic trail.
- However, full packet capture remains an extremely resource-intensive operation.
- Additionally, scenarios exist in which capturing all network traffic may not be the desired result.
- Packet filtering is an additional PCAP technique that can significantly tune PCAP operations to meet the desired intent as well as significantly reduce resource requirements (for both processing and storage).

- The BPF syntax is widely used across PCAP tools to limit the scope of PCAP operations.
- Most modern tools support the BPF syntax for capture or display filtering. (How BPF is implemented depends on the specific tool in use.)
- Packet filters are used to restrict the packets that are captured by PCAP tools to only those packets that satisfy the filter.
- This can significantly reduce the scope of PCAPs so that they only contain traffic that is desired.
- Packets not matching the filter are typically discarded and not captured as a part of the PCAP.

- Consider the following example: An organization has several servers located within a single subnet.
- The servers in the subnet are heavily used among users; thus, there is a consistently high volume of network traffic within the subnet.
- A PCAP sensor is implemented to capture all traffic within the subnet. Cyber Protection Team (CPT) analysts have been asked to analyze traffic for only one server within the subnet.
- If the PCAP sensor has not been configured with any packet filtering, it captures all network traffic for the entire subnet.
- With a properly implemented packet filter, the sensor can be configured only to capture traffic to and from the desired host within the subnet.

### BPF Syntax
- A properly formed BPF expression contains one or more primitives.
- Primitives consist of an identifier (such as an Internet Protocol [IP] address, hostname, or port number) preceded by one or more qualifiers.
- There are three kinds of qualifiers for the BPF syntax, as follows:
  - **type**: Determines the type of thing that the identifier refers to. Examples are host, net, and port.
  - **dir**: Specifies a direction to or from the identifier. Examples are src (source) and dst (destination).
  - **proto**: Restricts the match to a specific protocol. Examples are ether, tcp, and udp.
<img width="1667" height="1653" alt="98601d28-4fe3-4bbe-98a2-13c8dea8c7da" src="https://github.com/user-attachments/assets/d26e658f-72e0-4b41-9e42-205741f42365" />

- In some applications, a single BPF expression may not provide the level of filtering necessary.
- The BPF syntax, shown in Table 9.4-4, supports operators to include multiple expressions to be applied on a filter.
  <img width="1999" height="493" alt="7c6aed3a-a0e6-417b-b06c-c0fb3a9aae7b" src="https://github.com/user-attachments/assets/177ba514-e193-44ef-9d17-ba25dce1894e" />

<img width="1999" height="1014" alt="6645c9d4-d704-4dc3-9a34-58768eacd3f9" src="https://github.com/user-attachments/assets/2649d4d1-c669-4d8d-b808-7ab04f1db11c" />

### BPF in Arkime
- When Arkime is deployed with a default configuration, no BPF is applied to the capture process.
- This means that Arkime captures all packets on the interface being monitored.
- Although this provides a full capture of all packets traversing the interface, it may be desired to capture only specific packets matching a filter.
- Consider the following scenario: An analyst has been provided with PCAP from a power plant network for initial analysis. The analyst used TShark to gather statistics regarding the contents of the PCAP. They observed that a significant amount of the traffic in the PCAP was attributed to Modbus TCP. (See the output below.) This Modbus traffic uses TCP port 502, and the analyst wants to exclude this traffic when ingesting the PCAP for initial analysis.

1. Open Arkime
2. from the terminal, sudo su into root and run the following to stop Arkime capture and viewer services: `systemctl stop molochcapture.service molochviewer.service`
3. Uncomment out the bpf config line
4. Aplly the following BPF to the config file: `bpf=not tcp port 502`
5. Save and exit the file
6. Start the Arkime services: `systemctl start molochcapture.service molochviewer.service`
7. Use tcpreplay to ingest the pcap. `tcpreplay -t -i ens224 /root/powerplant.pcap`
8. In Arkime, filter on `port.dst == 502` showing 0 results, proving it works.

# MOD 10
## Packet Display FIlters
### Common Wireshark Display Filters
- Each filtered protocol may have fields added that further filter displayed traffic.
- For example, the display filter of **ip.addr** shows all traffic related to a specific Internet Protocol (IP) address (such as **ip.addr == 192.168.22.5**).
- If the source or destination IP address needs to be specified, the command **ip.src** or **ip.dst** may also be used as a display filter.
- The **(protocol).(field)** format is applied to other protocols with a variation of the fields used.
- For example, http may be used to show only Hypertext Transfer Protocol (**HTTP**) traffic, but adding **.accept** shows HTTP traffic **accepted by the web server.**
- The same is true for tcp, which can show either all Transmission Control Protocol (TCP) traffic or specified port numbers or protocols.

<img width="1999" height="942" alt="0ac2dee8-2a0b-4142-b378-c66fe3095a32" src="https://github.com/user-attachments/assets/d4e6972a-ec61-4d1c-8205-94fe8381962d" />

### Filtering Session-Layer Protocols
<img width="1999" height="1678" alt="cea597a6-a28f-4a3a-9ba1-167934d20978" src="https://github.com/user-attachments/assets/47eb5949-bf58-4efb-ae2f-fb99a0abd2f4" />

### Wireshark Logical Operators and Display Filters
<img width="1981" height="1999" alt="e72b9357-7ae7-4f4f-bd57-3a8b517667eb" src="https://github.com/user-attachments/assets/824a9c67-e0ce-412a-8120-6c4c33af53bf" />

### Applying Display Filters
- In tshark, run the following commands:
  - `tshark -i eth0` captures traffic on an interface
  - `tshark -Y tcp` filters for tcp
  - `tshark -i eth0 -Y tcp` combines the two
  - Run the following to create a new PCAP file limiting to only TCP port 443 and traffic to or from 202.84.73.91:
    - `tshark -r fullday.pcap -w maliciousip443.pcap -Y "ip.addr == 202.84.73.91 and tcp.port == 443"`
    - In the above, `-r` is the file to be read, `-w` is the file to be written to, `-Y` is the filter to be applied.
  - Adding only traffic to and from a different IP: `tshark -r maliciousip443.pcap -w output.pcap -Y "ip.addr == 172.16.3.3"`
- To quickly analyze statistics of newly created PCAP: `capinfos maliciousip443.pcap`

## Data Reassembly
### Finding Data Streams
- Wireshark can follow data streams of the following protocols:
  - Transmission Control Protocol (TCP)
  - User Datagram Protocol (UDP)
  - Datagram Congestion Control Protocol (DCCP)
  - Transport Layer Security (TLS)
  - Hypertext Transfer Protocol (HTTP), HTTP/2
  - QUIC
  - Session Initiation Protocol (SIP)
- Following the data streams above, Wireshark can extract data from both the client side and server side of the conversation.

- TShark allows following of data streams using: `-z follow,prot,mode,filter[,range]`
  - In the syntax above, **prot** specifies the transport protocol as one of the following:
    - **tcp**: TCP
    - **udp**: UDP
    - **tls**: TLS or Secure Sockets Layer (SSL)
    - **http**: HTTP streams
    - **http2**: HTTP/2 streams
    - **quic**: QUIC streams
  - **mode** specifies the output mode as one of the following:
    - **ascii**: American Standard Code for Information Interchange (ASCII) output with dots for nonprintable characters.
    - **ebcdic**: Extended Binary Coded Decimal Interchange Code (EBCDIC) output with dots for nonprintable characters.
    - **hex**: Hexadecimal and ASCII data with offsets.
    - **raw**: Hexadecimal data.
    - **yaml**: YAML format.
  - **filter** specifies the stream to be displayed as one of the following:
    - **ip-addr0:port0,ip-addr1:port1**: Specifies Internet Protocol (IP) addresses and TCP or UDP port pairs.
    - **stream-index**: Specifies stream indexes and is used for TCP, UDP, TLS, and HTTP. A stream index is a number used to uniquely identify a TCP or UDP stream.
      - TLS and HTTP both use TCP stream indexes.
    - **stream-index,substream-index**: Specifies streams and substreams and is used for HTTP/2 and QUIC due to their use of multiplexing.
  - **range** optionally specifies which “chunks” of the stream should be displayed.
- As an example, tshark -z "follow,tcp,hex,1" displays the contents of the second TCP stream (the first is stream 0) in hex format.

### Reassembling Data
- Network communications often transport large amounts of data in individual packets, but it is extremely common for that data to be too large to be carried in a single packet due to the maximum transmission size of the packet and protocol.
- The large data may be spread across multiple packets, which are unreadable on their own.
- Wireshark can reassemble data spread across multiple unreadable packets.
- Wireshark can also export files from network traffic by selecting **File > Export Objects**.
- This feature scans the selected protocol streams in a currently open capture file or running capture.
- Wireshark and TShark can export files from five network protocols:\
  - Digital Imaging and Communications in Medicine (DICOM), a protocol for management of medical imaging information.
  - HTTP, which transfers web files over a network.
  - Internet Message Format (IMF), which can export emails from Simple Mail Transfer Protocol (SMTP) traffic.
  - Server Message Block (SMB), used in file sharing.
  - Trivial File Transport Protocol (TFTP), a lightweight version of File Transfer Protocol (FTP) that uses UDP instead of TCP.
- There are no encrypted protocols in the above list. Decrypting encrypted traffic is discussed in the upcoming lesson Decrypting SSL/TLS.

### Reassemble an Attack
- Run the following to display all traffic containing "tradetracker" in a PCAP file: `tshark -r sensoroni_onion-fn2_1023.pcap -Y "frame contains tradetracker"`
- Run the following to create a targeted PCAP file containing only traffic from the suspicious IP:
  - `tshark -r sensoroni_onion-fn2_1023.pcap -Y "ip.addr == 104.53.222.103" -w exported_objects/evidence_1.pcap`
  

## Decrypting SSl/TLS
### Overview of SSL/TLS
- Hypertext Transfer Protocol Secure (HTTPS) is an extension of Hypertext Transfer Protocol (HTTP) that negotiates a secure encrypted connection between the client and server before sending HTTP data.
- Although this provides confidentiality, it causes visibility gaps for a Cyber Defense Analyst (CDA) seeking to analyze web traffic.
- Although legacy SSL standards exist, the more modern term for this process and associated protocols is Transport Layer Security (TLS).

#### RSA Encryption
- The underlying workings of Rivest–Shamir–Adleman (**RSA**) encryption are beyond the scope of this lesson, but a basic understanding of it is required to understand TLS encryption.
- RSA encryption is **asymmetric** and uses the relationships between prime numbers to perform mathematical operations.
- These relationships between prime numbers form the private and public keys used for encrypting and decrypting data.

#### Certificate Authentication Chains
- RSA private–public key pairs can be used to sign values by encrypting a checksum of a value using the private key.
- This methodology is used to allow for trusted certificates to sign other certificates that should also be trusted.
- The usage of this methodology in modern browsers has allowed the creation of Trusted Root Certificate Authorities (**CA**), which can sign **Intermediate CA certificates**, and both **Root and Intermediate CAs can sign individual certificates**.

#### TLS Handshake
- HTTPS handshakes occur after the normal Transmission Control Protocol (TCP) handshake. Figure 10.5-1 provides a simplified version of the flow of a typical handshake.
<img width="1999" height="1000" alt="f768d126-5e41-4693-81be-b7e2eaf173ef" src="https://github.com/user-attachments/assets/88940144-07c3-459a-a17c-21e8d2c3f29d" />

1. **ClientHello**: The client sends a message to the server to initiate the connection, which includes the list of cipher suites the client supports as well as proposed TLS versions and a random sequence of bytes (the “client random”).
2. **ServerHello**: After receiving the ClientHello message, the server responds with the cipher suite to be used, the public certificate for the server, the TLS version to be used, and a random sequence of bytes (the “server random”). The server may also request a client certificate.
3. **Pre-Master Secret**: Using the public key sent to the client, the client encrypts another sequence of bytes and sends it to the server (the “pre-master secret”). If a client certificate was requested, it is also sent at this time.
4. **Session Keys**: Using the same methodology on both server and client, both sides use the “client random,” “server random,” and “pre-master secret” to derive the encryption keys to use for this session.

- Because the session key is derived partially from the “pre-master secret” – a value that is encrypted using the server’s public key – a third party cannot decrypt this traffic without access to the server’s private key or access to the client or server machine to extract the session key.
- Although this represents a typical handshake, the steps can vary.
- For example, if the Diffie–Hellman (DH) key exchange is used, the steps differ in such a way that the client and server encrypt the byte sequences and DH parameters using their private keys, and the session key is calculated differently.
- Using this methodology, two private–public key pairs (client and server) are used during the handshake.

### Decrypting Packet Captures
- Wireshark can decrypt TLS sessions within PCAPs under either of the following conditions:
  - Session keys are available for the particular TLS session.
    - This requires preconfiguration of the server or client to log all session keys.
  - The server’s private key is available.

- The method involving the private key has some extra conditions, due to how TLS sessions are negotiated:
  - The server’s private key, which resides on the web server, is required.
  - The initial handshake must be present in the PCAP. For the session keys to be calculated, DH key exchange must not be used.
  - The TLS version used must be a version prior to TLS 1.3.
  - TLS 1.3 introduced forward secrecy, which requires a DH key exchange to operate, by default. 

- The ability to decrypt traffic at rest is impacted by the increasing adoption of TLS 1.3.
- Modern browsers and other web clients are typically configured to use TLS 1.3.
- However, servers can be configured to use TLS 1.2 only, and as long as the client is configured to still accept TLS 1.2 connections, those clients can connect

<img width="776" height="594" alt="image" src="https://github.com/user-attachments/assets/3fe6d5ef-c0ba-4678-97f6-e9d060d6f5ed" />

### Decrypt a PCAP File
1. In Wireshark, Select `Edit > Preferences` and Select `Protocols > TCP`
2. Select the check Box `Reassemble out-of-order segments`
3. Select `Protocols > TLS`
4. Select `Edit` next to "RSA keys list"
5. Select the (+) and enter foe following:
  - IP Address: 192.168.124.178
  - Password: CyberTraining1!
6. Select `...` under Kyey File, and open `Desktop\server certificate.pfx`
7. Select the GET Frame (Frame 16)
8. Select `Decrypted TLS`
9. Scroll down to Frame 20 (TLS Segment) This contains the response to the GET Request. If the Reassembled SSl tab is selected, the entire response can b viewed.

### Sensor Configurations
- Network sensors can be configured to gather decrypted TLS data – either by decrypting traffic on the fly using the server’s private key or by proxying traffic.
- The approaches differ based on positioning of the sensor: collecting all client-initiated traffic leaving the network and collecting all traffic bound for specific servers.
- This lesson refers to the categories as client-initiated collection and server-bound collection, respectively.

#### Client-Initiated Collection
- The most practical setup for collecting outbound TLS traffic involves proxying all that traffic through a network device. (This practice is also known as break and inspect.)
- Under this proxy setup, the client connects and establishes a TLS connection to the proxy device, which also establishes a TLS connection to the intended server.
- The device forwards requests and responses back and forth after either performing inspection or forwarding the unencrypted data to a sensor.
<img width="1999" height="853" alt="15d7bc2b-9bd1-46ba-8e8f-e98d82bdc234" src="https://github.com/user-attachments/assets/b9c035fe-ef58-4d51-9c8d-80c774d62b16" />

- For the proxy device to be trusted by the clients, it must be able to issue trusted certificates on the fly, which means that it must have a Root or Intermediate CA certificate installed that the network clients trust.
- Although configuring such a network has overhead, the tradeoff of being able to have visibility into TLS traffic is a decision that network administrators and other interests make when reviewing and enhancing the security of their networks.

- **TLS 1.3** impacts client-initiated collection due to **changes in the ServerHello step of the handshake**: all steps past the initial ClientHello step are encrypted.
- Because of these changes, a network device cannot easily inspect the server’s certificate before deciding if it will act as a proxy for that particular connection or not.
- The inability to make these decisions impacts allowlist strategies in which network administrators may allow specific domains that are trusted or whose traffic should not be inspected, such as banks or sites that have certificates known by specific browsers. 

#### Server-Bound Collection
- Collecting and decrypting all traffic bound to a server or web application can be accomplished in two primary ways: by using the private key or using a proxy server.
- Using the private key involves configuring sensors with the private key so that the sensors can perform the same decryption steps that the earlier Wireshark examples used.
- The proxy server setup involves a “reverse proxy” (present in the back end and dealing with inbound traffic rather than the typical proxy setup) that handles all encryption for a particular website or application and forwards the traffic to the actual target server, typically unencrypted.

- Using sensor-based decryption with supplied private keys requires that the server be configured to disallow TLS 1.3 and DH key exchanges; otherwise, the sensor is unable to decrypt that traffic.
- Because of this limitation, new installations must balance the benefits and downsides of this type of configuration.

- Reverse proxy setups use a proxy server – often a web server that is configured as a proxy, such as Nginx or Apache.
- This server has the server certificate, including the private key, and is able to establish TLS connections with clients.
- The connection to the server or web application is typically transmitted in plaintext over a trusted internal connection.

<img width="1600" height="1492" alt="c69e04ea-9429-4e6c-8b47-b73f39dbb4e5" src="https://github.com/user-attachments/assets/cca1ec3f-12ee-47fd-9c4d-2ab4b3737490" />

<img width="779" height="383" alt="image" src="https://github.com/user-attachments/assets/55f5ff65-4dc5-419a-b05a-a73765eb9dc5" />













