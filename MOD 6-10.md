# MOD 6
## Operationalizing Threat Intelligence
### Cyber Threat Intelligence Overview
- At its heart, intelligence is the science and art of understanding the enemy.
- As a discipline, intelligence is as old as warfare itself.
- Commanders have striven to predict the actions of their adversary, whether on a kinetic battlefield or in a futuristic cyberwar.
- Understanding the enemy’s capability and intent is the only way to make informed decisions on one’s own tactics to employ in response.
- Understanding the Operational Environment (OE) — the **conditions**, **circumstances**, and **influences** that affect the employment of capabilities — is paramount to making sound decisions in both offensive and defensive engagements.
- Traditionally in military intelligence, this refers to such things as the enemy’s **firepower**, **aggressiveness**, or **infrastructure** available to resupply their troops.
- In the cyber domain, direct correlations can be made to “traditional” intelligence. The product of these correlations forms CTI.

#### Intelligence in the Cyber Domain
- CTI is analyzed, actionable information, derived **internally** or **externally**, that aids an organization in **identifying**, **assessing**, **monitoring**, and **responding** to cyber threats.
- Because the cyber domain is now a major battlefield for crime, activism, and espionage, CTI has become increasingly essential to cyber defense.
- Relying on current and historical attack knowledge, an analyst can leverage CTI to prepare for future attacks on a defended terrain.
- Almost every tenet of traditional military intelligence can be applied to the cyber domain, not just in a military application.
- The cyber intelligence landscape is vast.
- It is unique in that there are thriving communities in both private and public sectors.
- The **private**, or **open-source**, intelligence community can be found in many different forums; **social media**, **vendor websites**, **blogs**, and **subscription services** are good examples.
- The government intelligence community, simply known as the “Intelligence Community” (**IC**), is less available to the public.
- Both these communities provide valuable resources to an analyst on a Cyber Protection Team (CPT).
- In fact, the CPT construct has an entire work role — the All-Source Analyst (ASA) — dedicated to interfacing with both the IC and open-source intelligence communities.
- Thanks to its countless devices and large intelligence communities, cyber, in general, has no shortage of “data.”
- Analysts use applications to collect data, they store data on servers, and they sift through telemetry data generated by anything that can ship a log.
- It is this data, or information, that eventually becomes intelligence.

#### Data vs. Intelligence
- Joint Publication 2-0, Joint Intelligence, Executive Summary, states the following about information (or “data”) vs. intelligence:
  - **_“Information on its own may be of utility to the commander, but when related to other information about the operational environment and considered in the light of past experience, it gives rise to a new understanding of the information, which may be termed ‘intelligence.’” _**

- In other words, analysis is what separates information from intelligence.
- For cyber defense, analysis is what makes data about threats in the cyber domain become CTI.
- For example, a Domain Name System (**DNS**) lookup for example[.]com is a piece of data.
- Further analysis may indicate that the domain is registered to Threat Actor, LLC, and can be tied to an ongoing espionage campaign.
- That extra analysis step is what makes data useful intelligence.

### The Intelligence Cycle
- Analysts need to understand the application and cyclical nature of CTI to be effective in countering active and future threats on a network.
- CDAs who are part of a CPT are not expected to perform formal intelligence analysis, but they are the consumers and, in some cases, collectors of intelligence.
- Maintaining realistic expectations and knowing what can be provided to the IC increases mission effectiveness throughout the execution of an operation.
- CTI follows the Intelligence Cycle, in which CDAs play an active role.
- Figure 6.1-1 illustrates the six steps of the cycle: **Direction**, **Collection**, **Processing**, **Analysis**, **Dissemination**, and **Feedback**.
  ![eed5e1d5-9f2e-481b-90d8-763547cd5c61](https://github.com/user-attachments/assets/a039dfb1-cadd-4fd9-be57-6a05e7358973)

#### Direction
- CTI analysis is always based on requirements.
- The **first step** of the Intelligence Cycle is to **define a clear requirement or question** for which intelligence should provide an answer.
- Often, for external intelligence, this requirement or question is submitted to an intelligence analyst in the form of a **Request for Information (RFI)**.
  - Uses for intelligence are numerous, so RFIs may ask a wide variety of questions.
- CDAs likely are most concerned with obtaining or collecting tactical intelligence for immediate use defending a network.
- Sometimes, a CPT may be employed to answer or collect information on intelligence requirements given by a higher tasking authority.

#### Collection
- The collection of cyber intelligence is typically the **process of collecting data from networks useful in tracking a threat**.
- However, it may come in other forms as well, such as reading a news article for political or economic information pertaining to a mission partner’s organization or viewing data from an open honeypot provided by researchers on the internet.
- At this point in the cycle, this is just data and not yet intelligence.
- **Collection is not a one-time effort**.
  - It comes in cycles of its own.
  - For example, a CDA may find a suspicious Internet Protocol (IP) address thought to be Command-and-Control (C2) communication to a malicious server.
  - The next cycle of collection on that data might be identifying what domain resolves to that address, followed by a cycle of collecting certificate information on that domain.
  - The cycle continues until enough data is collected to meet the requirement defined prior to collection.

#### Processing
- An analyst should not overlook the importance of processing data to be usable.
- A CDA’s analysis platform should be flexible and configurable enough to view a multitude of different data types in a normalized way.
  - For example, collected logs from firewalls may have different formats, depending on their type or brand.
- It is more useful if a CDA can process the log data to conform to a standard format than if they must manually interpret and compare the logs separately.
- The creation of **visualizations** also falls into the Processing step.
  - **Visualizing data** can greatly **assist a CDA or ASA in identifying trends** and **answering questions proposed** to the intelligence process as a whole.

#### Analysis
- **CTI analysis is based on requirements**.
- Without a requirement, the analysis has no question to answer.
- This is the step in which **data becomes intelligence**.
- Questions asked, or requirements presented, in the initial step of the cycle (Direction) are answered using analysis.
- The Analysis step is distinct from Processing.
  - Whereas the Processing step is usually automated or involves only manipulating data, analysis is done exclusively by a human.
  - This is where a CDA interprets the data that was collected and processed in previous steps.

#### Dissemination
- Intelligence is a shared commodity.
- It should be shared with the entity that requested the information, and it is often shared with a larger community of intelligence analysts.
- CDAs should concern themselves with this step because the format and completeness of the collected data and analysis directly affect the value of the dissemination of intelligence.
- ASAs are leveraged in the step to disseminate intelligence back to the IC.

#### Feedback
- The last step of the Intelligence Cycle is Feedback.
- Did the analysis properly answer the requirement?
  - This is a question that CDAs should answer if requesting or analyzing intelligence.
  - If the intelligence fails to meet the requirement, a new cycle is started in which the error is corrected or intelligence gaps are identified more fully.

### Levels of Intelligence
- When dealing with consuming or collecting intelligence on a network, intelligence comes in different forms and at different levels, depending on requirements given to an analyst.
- Consider the following examples:
  - The political motivations of a threat group.
  - A list of APTs assessed to have the capability and intent to target a mission partner’s network.
  - Domains tied to a specific APT for use in an Intrusion Detection System (IDS).
- Each example represents a different level of intelligence that a CPT may request or be required to fill.
- The three levels, or **taxonomic categories**, of intelligence are **strategic**, **operational**, and **tactical**.
  - The first example listed above represents strategic intelligence; the second, operational intelligence; and the third, tactical intelligence.

#### Strategic Intelligence
- Attacker **motivations**, **politics**, or **trends** that **inform an organization’s policy decisions and planning** represent the **strategic** level of intelligence.
- Often, strategic intelligence is the **trigger for the start of an operation** or a **change to network architecture**.

#### Operational Intelligence
- Attacker **Tactics**, **Techniques**, and **Procedures** (**TTP**) and **capability** are typically viewed as **operational** intelligence.
- This intelligence **informs collection plans** and **mission-planning efforts** **prior to an incident response or hunt engagement**.

#### Tactical Intelligence
- **Immediately applicable intelligence**, such as Indicators of Compromise (**IOC**), **antivirus signatures**, or **IDS rules**, fall into the **tactical** level of intelligence.
- This intelligence is **most useful for CDAs** on a mission **during on-network engagements and retroactive investigations**.

### Strategic Intelligence
- Cyberspace does not exist in a vacuum.
- Strategic intelligence deals with **external**, and **often global**, **influences** on the cyber threat landscape.
- The following are example intelligence requirements that pertain to the strategic level of intelligence:
  - Based on new political developments in the region, what threat groups may be motivated to attack the mission partner’s network?
  - Which trends in cybersecurity indicate a threat to the mission partner’s attack surface?
  - Should active cyber threats dictate a change in collaboration software for the organization?

- As illustrated in these examples, **strategic-level intelligence** deals with **threat actor trends and motivations** and **informs policy decisions**.
- Strategic-level questions have **real financial impact** and may **inform the movement of defense efforts**.
- This intelligence can assist all analysts by **giving context to defense efforts**, even at the most basic and direct levels.

#### Application of Strategic CTI
- During a CPT mission or planning of such a mission, **strategic intelligence** plays an important role in **comprehensively understanding the network environment**.
- In fact, a subset of strategic intelligence called **warning intelligence** is one of the **three main triggers for CPT employment**. (The other two triggers are **campaign planning and detected malicious cyber activity**.)
- Additional characteristics of strategic CTI are as follows:
  - Strategic CTI provides a comprehensive view of the target or attacker, to include motivations, resources, and capability.
    - This can come in the form of target profiles, briefings, or reports.
  - Strategic CTI seeks to predict future attacks based on trends and other forms of intelligence collection.
  - Strategic CTI provides input to planning and policy development.
    - Sometimes it can inform policies on mission partner networks as a by-product of a mission’s final report.
- This all adds up to the explanation of what a CDA should be worried about — or, more specifically, whom a CDA should be worried about — when defending a network.

#### Who Is the Enemy?
- Strategic intelligence boils down to one big question: **“Who is the enemy?”**
- This is not a question to completely craft collection plans around, but it is an **intelligence requirement** that is **fundamental to every decision made** when it comes to **employing CPTs** and other defense teams.
  - For example, assume that a specific organization’s most likely enemy, based on strategic-level intelligence, is motivated by financial gain and is known to sell stolen intellectual property.
  - Knowing nothing else about the network or organization, how would that inform mission planning or network architecture decisions?
    - If the organization in this example is attacked by the most likely enemy, the following is true:
      - Data exfiltration of intellectual property is likely.
      - The servers with the most valuable information are most likely the targets.
      - The attack comes from outside the organization (so the enemy is not an insider).
- These assumptions and intelligence most likely lead the organization to invest in greater protection of the sensitive data or servers and possibly to consider a standalone network for valuable intellectual property.
- The above conclusions from the example intelligence would likely drive follow-on requirements to narrow down TTPs or IOCs for analysis.

### Attribution to the Enemy
- **Attribution is not the job of a CDA**.
- However, attribution is an **important aspect of CTI**.
- Knowing (or assessing) who attacked, or will attack, a network is essential to policy makers and planners.
- Having no idea who attacked a network makes it impossible to track campaigns or anticipate future attacks from any particular entity.
- Furthermore, CDAs can use existing attribution information to more quickly identify and counter a threat actor on a defended network.
- Even though attribution is not the responsibility of a CDA, it is **important to know how CDAs contribute to the process** and **how attribution decisions are made by the IC**.
- By knowing the steps and information needed to attribute an APT, CDAs can distinguish between the threat groups more easily. 

#### Threat Group Tracking
- The concept of a threat group is often misleading.
- What makes up a threat group or APT?
  - Sometimes they are tied and attributed to government agencies, sometimes they are crime organizations, and sometimes they are synonymous with the malware they are known to use.
  - An even more perplexing question is: How is tracking a threat group possible? Why don’t APTs simply change their tactics after every attack?
- **Threat groups**, as a concept, are a **conglomerate of tendencies and identifiable tactics** that **allow for researchers and intelligence analysts to identify patterns**.
  - These patterns are possible, and necessary, for threat groups to operate.
  - After all, when APTs are thought of as someone’s job, patterns make sense.
  - For example, a fast-food restaurant does not serve different hamburgers if someone calls in sick. Threat groups operate similarly.
  - To achieve repeatable success with a team of operators, a repeatable process needs to be implemented. That repeatable process becomes a trackable threat group.
  - In fact, most tracked threat groups are an abstraction and primarily equate to a set of instructions that humans follow, not to the humans themselves.

- Attribution of APTs to organizations that exist outside cyberspace is more challenging than identifying patterns and tactics.
- **Motives**, **politics**, and other **non-technical attributes** need to be analyzed to make that kind of determination. 
- **Tracking threat groups, from a CDA perspective**, is important because it **creates sets of TTPs** that can be **searched** and **planned** for on a network.
- Without having a predictable set of behaviors to search for, analysts are left with virtually infinite combinations of TTPs to plan for.

#### What’s in a Name?
- **Naming APTs or threat groups** is necessary and is a **standard practice for every organization** that tracks them.
- Being able to identify a group with a **short name** is obviously **preferable** to using something like “that group that uses phishing and password spraying and is assessed to be from East Asia.”

- However useful the naming of threat groups may be, every entity that tracks these groups names the groups based on their own criteria.
  - Vendor A’s tracking of a group is not necessarily the same as Vendor B’s.
  - A common misconception of the CTI community is that direct equivalencies exist between threat groups tracked by different vendors.
  - There is a reason for the lack of equivalency: **perspective**.
  - For example, consider the following scenario.
    - Imagine two CTI entities looking at the same attack: one is a vendor who specializes in host-based antivirus, and another is a vendor specializing in firewalls.
    - They each have their own telemetry and data on the incident.
    - It is reasonable to think that the antivirus vendor focuses on the malware signatures on the host and the firewall vendor has a deep understanding of the C2 traffic leaving the network.
    - The use of the same malware in a follow-on attack, but with modified C2 patterns, may cause the firewall vendor to attribute these two attacks to different groups but the antivirus to continue attributing to the original group.
  - This is an unrealistically simplified example, but it illustrates how viewing threat groups from different perspectives causes attribution of APTs to diverge over time.
  - Zooming the example out even more, consider entities that deal in CTI but are not vendors.
  - Such entities as cybersecurity vendors are quite interested in detectable attributes that can protect their customers’ networks, but entities like government agencies tend to be more interested in attribution to humans or other governments.
  - This tends to make attribution by government agencies lean more toward nontechnical attributes of a threat group.
  - CPTs have to take all these perspectives into account when determining which TTPs and APTs to target.
- Keeping this naming and tracking perspective in mind, security researchers and intelligence analysts use a standard framework for attribution decisions: **Malware**, **Infrastructure**, **Control Server**, **Telemetry**, **Intelligence**, and **Cui Bono**, or **MICTIC**.

### MICTIC Framework
#### Overview
- CDAs need to know what type of information is helpful for the IC to have to make attribution decisions so that it can be collected while on mission.
- It is also important to know that a single artifact is often not enough to make official attribution decisions.
- The MICTIC Framework, as described in Timo Steffens’ book Attribution of Advanced Persistent Threats: How to Identify the Actors Behind Cyber-Espionage, is a standard method to tie attack campaigns and attributes to threat groups or sometimes even declare a brand new threat group discovered.
- The MICTIC Framework comprises six evidence categories of an attribution decision:
  - **Malware**
    - Intelligence analysts can track and attribute APTs based on the malware they employ on victim networks.
    - This may seem obvious. However, taking things a step further, some APTs outsource the authoring of malware for their own use.
    - That relationship between the author and the entity that employs the malware is also good evidence for attribution.
    - It may also assist in identifying nefarious relationships between malware authors and multiple APTs.
    - Evidence that CDAs can provide includes **malware samples** and **sandbox reports**.
  - **Infrastructure**
    - The pathways that C2 takes to and from victim networks provide a good indication of who is attacking a network.
    - In fact, some larger nation-state APTs have teams with the sole purpose of setting up infrastructure.
    - By investigating things like **domain name registration** and **IP address ownership**, a pattern may emerge that points to known bad actors.
    - Evidence that CDAs can provide includes **packet captures**, **domain names**, **IP addresses**, and **certificate information**.
  - **Control server**
    - Typically, the Control Server evidence is seized by law enforcement or government agencies from attacker-owned or -rented servers.
    - Certain things like language **settings**, **certificates**, **user accounts**, and **saved files** are quite useful in positively identifying not only threat groups but also human operators.
    - Evidence that CDAs can provide includes **packet captures to C2 servers from victim networks**.
  - **Telemetry**
    - Information logged from **inside victim networks**, such as **command-line logs**, **IDS alerts**, and **antivirus messages**, provide analysts with a **treasure trove of information** useful for attribution.
    - For example, evidence of attacker interactivity on a network that happens only during working hours in Moscow and stops on Russian national holidays might be a good indication the threat group is based in Russia.
    - Evidence that CDAs can provide includes **log information ingested into analysis platforms** and **crew logs indicating observed attacker behavior**.
  - **Intelligence**
    - Of the six evidence categories in the MICTIC Framework, **Intelligence** and **Cui Bono** are the two that **CDAs have little to do with**.
    - **Governments** and **researchers** have the ability to **aggregate many pieces of information** and **collect intelligence that may indicate an attacker's identity** outside a victim’s network.
    - **Intelligence** may be from a **human source**, **intercepted communications**, or any other **traditional intelligence source**.
    - **Cui Bono**
      - An **analysis** of **who would benefit** from the attack **politically**, **financially**, or otherwise.

#### MICTIC Application
- By analyzing each evidence type and comparing it to a known set of APTs, researchers and intelligence analysts can make informed attribution decisions.
- Some attributions are made with more confidence, whereas others may be made with less confidence but still show ties and similarities between attacks.
- Again, making attribution decisions is not a CDA’s job.
- It is, however, highly important from a strategic standpoint for leaders to assess who has attacked, or might attack, in the future.
- Also, **attribution** allows for an analyst to **more effectively plan for**, or **quickly react to**, active **intrusions** on **mission partner networks**. 


### Operational Intelligence
- **Strategic intelligence**, by definition, is **long term** and, as illustrated in the previous section, **takes considerable effort to track and apply**.
- Once stakeholders and analysts are aware of the strategic intelligence for their operating environment, more granularity is needed for operationalizing that intelligence.

- **Operational intelligence** is **higher-level information** on **campaigns** and threat actor **TTPs**.
  - It deals with more **immediately applicable intelligence** to **mission planning and operations**.
  - For example, **strategic intelligence** may **indicate an APT** to initiate a hunt mission for, but **operational intelligence** provides the **TTPs** and **capabilities** of that APT to plan the mission around.

- As each level of intelligence is explained, strong connections between each level are evident.
- For example, operational intelligence on TTPs of attackers is not possible without first using strategic intelligence to attribute and anticipate the adversary.
- Likewise, tactical intelligence relies on operational intelligence to determine what directly pertains to a defended terrain.

- Operational intelligence is **high-level**, but **short-term**, **intelligence**.
- It does not get into IOCs or specific commands an attacker might run on a victim network, but it does **deal with fluid topics like TTPs**.
- It is **observable** and relates to **specific attacks or attack campaigns**, as opposed to the broader approach that strategic intelligence provides.

#### Intelligence in Operations
- Attacker capabilities and intent are the cornerstones of operational intelligence.
- After identifying who the enemy is, the next logical questions asked are “What can the enemy do?” and “What does the enemy want?”
- Based on intelligence and data collected from previous attacks, repeated patterns related to TTPs and actions on objectives by the attacker can give analysts in CPTs insight into what to expect while on mission.

#### Attacker Capabilities
- The capabilities of an APT directly relate to the resources they have available.
- Do they create their own custom malware? Are they able to purchase hundreds of domains on the internet? Do they enhance their operations with their own intelligence collection?
- APTs with the most resources will obviously have the most advanced capabilities.
- When defending against or hunting for attackers with advanced capabilities, it is reasonable to assume that traditional methods of detecting malicious activity on a network will not work.
  - As an example, for a well-resourced APT, antivirus will most likely be ineffective. 
- Similarly, detecting attackers with advanced capabilities is resource intensive.
- Resource intensive might mean it is **expensive**, requires **deeper analysis**, or takes **more time to detect**.
- This creates an imbalance if a highly capable attacker targets a resource stretched organization.

#### Attacker Intent
- Knowing or assessing what an attacker is attempting to accomplish on a victim network makes defending against that attack possible.
- Operational intelligence seeks to determine what an attacker or attack campaign is trying to accomplish based on strategic intelligence indicators.


### Applied TTPs
- Dealing with capabilities and intent goes only so far when planning the defense of a network.
- Knowing, for example, that an attack campaign indicates that an APT is using open-source malware and is altering valuable information on victim networks indicates attacker intent and capabilities.
  - However, this gives only vague information.
- There are more intelligence requirements to be met when planning a defense.
  - Specifically, how does the attacker operate?
- Consider the following, for example:
  - How does the attacker typically gain initial access?
  - What method is usually used for persistence on a host?
  - Will the CPT be able to see clear text communications in network traffic?
  - How much time passes between initial foothold and actions on objective?

- These intelligence requirements all point to TTPs.
- Knowing and mapping TTPs of an APT give an analyst something concrete to build their defense around.

- Using such tools as the **MITRE ATT&CK® framework**, intelligence analysts can effectively communicate operational level intelligence to a CDA in the form of TTPs.
- In essence, an **ATT&CK matrix describing an APT is that threat group’s fingerprint**.
- After all, as discussed early in this lesson, threat groups are merely an abstraction of linked TTPs.

- **Combining APT capability**, **intent**, and **known TTPs** can provide a CPT with the following information:
  - The amount of resources needed to defend a network.
  - Where on the network will be targeted.
  - How the attacker will attempt to accomplish their goal.
- Absent specific IOCs and signatures, this is enough information for a CPT to plan an entire operation around. 


### Tactical Intelligence
- In cyber defense, tactical intelligence is low-level and short-lived intelligence that allows direct response to cyber threats.
- As detailed in the sports example above, it is also the **easiest intelligence type for an attacker to alter**.
- Its perishable nature requires that it is refreshed often, but accurate and timely tactical intelligence can prove invaluable in an **active situation** or **retroactive investigation**.

#### Uses for Tactical Intelligence
- In a cyber defense application, tactical intelligence can be applied directly to an analysis platform and other tools that a CDA uses to interact with an environment.
- Most notably, it is conveyed in the form of IOC.
- Other forms of tactical intelligence are exact commands run on a system by an attacker or other observable tendencies like usernames or network traffic signatures.
- Tactical intelligence has two main uses: **active hunting or engagements** and **retroactive investigations**.
  - Using it during an active engagement can assist CDAs in directly countering a threat.
  - Tactical intelligence during retroactive investigations can provide evidence that malicious activity occurred.

#### Active Hunting
- Using IOCs, IDS signatures, and other readily applicable intelligence on a network during an active intrusion or hunt can deliver quick wins for an analyst.
- By using intelligence on active campaigns or attacks currently happening in cyberspace, relevant indicators can be ingested into an analysis tool for low-effort discovery and countering of attackers on a network.

- However, challenges exist with tactical intelligence in an active environment.
- One challenge is that tactical indicators are easy for attackers to change.
  - By accumulating a large number of IOCs from online repositories or internal intelligence databases, the processing and storage overhead is often not commensurate with the value of the data.
- Another challenge is that commonly, by the time tactical intelligence becomes processed and disseminated, it has already become stale.
  - Also, related to that point, many attackers change their identifiable tactical level signatures between every attack.

#### Retroactive Investigations
- Using known IOCs and other observables when reviewing logs for an investigation of a confirmed attack can be highly valuable.
- The inherent issues with tactical-level intelligence applied to active situations do not exist with retroactive forensics.
- In fact, stale IOCs may be more useful than current information.
- Looking back in time using known indicators may reveal more and provide for a more complete picture of the incident because the more time that passes, the more likely that information will be available for intelligence analysts to disseminate.


### RFI Process
- The process of requesting information or intelligence from the IC or a commercial intelligence team may be formalized, depending on the assigned unit, command, or organization.
- Typically, this function is performed by the ASA in a CPT, but the act of initiating a requirement may be the responsibility of a CDA.
- Although CPTs have assigned intelligence personnel, certain research and information gathering tasks still fall on CDAs.
- This section introduces the process of submitting an RFI and how to interpret its response.

#### The RFI Submission
- To receive specific intelligence about any particular topic or mission, a request must be submitted to the IC.
- Typically, submitting something to the IC means that an intelligence analyst assigned to a higher command does research on the topic and submits a response.
- Stating “Submitted to the IC” really means that it is given to another analyst or team of analysts to query for information on another system.
- If the intelligence requested does not exist already to answer a requirement, the Intelligence Cycle begins, with the RFI serving as the direction.
- Understandably, the entire Intelligence Cycle may take some time to complete, so it is important to be flexible but up front if the due date is swiftly approaching.

#### RFI Criteria
- The request itself should meet certain criteria.
- The IC expects certain items, as follows, to be included in order to respond with a useful answer.

#### Requestor/Requestee
- The RFI should indicate who requested the information, along with good contact information for any follow-up questions or clarifications needed by the intelligence analysts working the request.
- An alternate contact for the request is also helpful, especially in time-sensitive requests.

#### Request
- A request is a straightforward, simple question or requirement that needs to be answered.
- There is an art to crafting a request that leads to an appropriate response.
- After all, intelligence analysts are not mind readers.
- CDAs should be as precise and direct as possible and include an example of the response they are expecting.

#### Due Date
- Depending on where the RFI is addressed, it could be competing with a long list of other requests. Due dates are used to prioritize competing requests.

#### References
- For intelligence analysts to expedite research, it is important that they know which resources have already been checked.


## Threat Actor Research
### CTI Models and Requests for Information
#### CTI Models
- CTI data includes **tools**, **exploitation strategies**, **victims**, **strategic priorities**, **technical and organizational mechanisms**, **cyber infrastructure information**, and other factors that build an understanding of threat actors.
- CTI models are used by cybersecurity personnel to organize data about those threat actors.
- By gathering, processing, and studying CTI, CDAs can make decisions more quickly and accurately. 

- Once CTI is obtained, it can be organized into a CTI model.
- Three common CTI models are as follows:
  - **MITRE ATT&CK**: A knowledge-based CTI model that has users **match TTPs with sequential stages** in the ATT&CK matrix.
  - **Cyber Kill Chain®**: A linear series of stages, derived from a similar model used by the military, that depicts the **steps in a cyber attack**.
  - **Diamond Model**: A model in which, as the name suggests, **incidents are depicted as a diamond**.
    - Unlike other models, the Diamond Model **emphasizes relationships** between components of an attack.
- Although these three models are not the only CTI models used in cybersecurity, they represent some of the most commonly used and prolific ones.
- Professionals use other models, such as Find, Fix, Finish, Exploit, Analyze, Dissemination (**F3EAD**); the Observe, Orient, Decide, Act (**OODA**) loop; and the **Intelligence Cycle**, to organize information as well. 

- **TTP** is a catchall term for describing the **behaviors and tools of a cyber threat actor**, and TTPs **represent a significant topic** when discussing CTI models.
- By identifying the TTPs of an (APT) or other attacker, security analysts may better understand the threat as well as predict future cyber attacks and advise on appropriate mitigating controls.

#### Requests for Information
- An RFI is a term that describes the process of acquiring information about intelligence from intelligence analysts.
- An ideal RFI should contain at least the following aspects:
  - The names of those who requested the information and a backup contact in the event the primary person is not available.
  - A precise description of the information needed.
    - If the request is too broad or vague, the returned intelligence might not cover the depth of information needed.
  - A list of sources that has been checked for information prior to creating the RFI.
    - This helps the analyst avoid receiving information they already possess.
  - A due date. 
- Although these are general requirements, the RFI may change, depending on the specific scenario.

### MITRE AT&CK Matrix
- In 2013, MITRE introduced the ATT&CK matrix, a knowledge-based CTI model.
- This utility allows users to categorize threat actor behaviors based on real-world data.
- By understanding and using the ATT&CK matrix, analysts gain a greater perspective of attacker behaviors and become better prepared for mapping defensive controls in their network.
- Additionally, ATT&CK provides a common language for communication between internal security teams and other personnel outside the organization. 

- To use the ATT&CK matrix, open the ATT&CK website, https://attack.mitre.org, in a web browser.
- Matrices for mobile devices and Industrial Control Systems (ICS) also exist, but for this lesson, the Enterprise Matrix is used.
- The matrix is updated periodically as the cybersecurity landscape changes; as of early 2022, the matrix has 14 Tactics categories.
- A tactic, which is the goal of an adversary’s action, is the broadest component in the matrix.
- Each tactic includes several associated Techniques.
- A technique represents the actions taken by a threat actor to accomplish their tactic.
- Each technique may have associated Sub-Techniques, as a greater degree of granularity may help categorize some malicious behaviors.
- Finally, each technique or sub-technique contains Procedures, which are specific examples of how an adversary carries out their technique or sub-technique.
- Below is the hierarchy within the ATT&CK matrix:
  ![45668aeb-a7c4-4089-9338-26ce2613efa5](https://github.com/user-attachments/assets/88c79efb-55bb-4ccb-b36a-54efa972d9bf)

#### Bandook and ATT&CK
- By cybersecurity standards, the Bandook Remote Access Tool (RAT) is an ancient piece of malware.
- Having emerged in 2007, this malware is written in C++ and Delphi and has been used in cyber attacks against various sectors globally.
- Although its use subsided for some time, such attacks as Operation Manul (2015) and such groups such as Dark Caracal (2017) have brought back some of its popularity.

- Bandook is primarily distributed via phishing emails containing an infected Microsoft Word document with a trojan embedded inside.
- To increase the chances of successful execution, the malicious document instructs the victim to enable macros.
- Once enabled and opened, the malicious macros are downloaded to the victim’s machine.
- Finally, the macro executes a PowerShell payload that downloads and executes the Bandook backdoor disguised as Internet Explorer.
- At that point, the Command-and-Control (C2) server, controlled by the attacker, can issue commands to the Bandook backdoor.
- Examples of attacker capabilities include taking **screenshots**; **uploading**, **downloading**, and **executing files**; and **processing shell** **commands**. 

### Cyber Kill Chain
- The Cyber Kill Chain®, first released by Lockheed Martin Corporation in 2011, is a CTI model that uses a seven-stage timeline, placing a greater emphasis than the ATT&CK matrix or the Diamond Model on the processes and sequence of events in an attack.
- The objective of using the Cyber Kill Chain is to first identify the current stage of the attack and then attempt to disrupt the current “link” of the chain.
- This causes a cascading effect on the adversary.
- An important, additional distinction of the Cyber Kill Chain is its focus on APTs.
- The stages of the Cyber Kill Chain are illustrated in Figure 6.2-2 and summarized as follows.
  ![5ddbc0c0-3276-4713-8f2f-56d7afc2c9f9](https://github.com/user-attachments/assets/3ba8ed58-8153-4c97-a230-2765398c763b)
1. **Reconnaissance**: The first stage of the Cyber Kill Chain, involving multiple processes of information gathering, both Open-Source Intelligence (**OSINT**) and possible **social engineering**.
2. **Weaponization**: A pre-attack stage in which the adversary composes the exploit and vulnerability.
3. **Delivery**: The stage in which the threat actor initiates the first step of the attack. The exploit is delivered to the target via email, Server Message Block (SMB), physically, or via other methods.
4. **Exploitation**: The stage in which the exploit has been delivered and the adversary executes the attack on the target systems.
5. **Installation**: The stage in which the system has been compromised and the attacker places persistence malware on the target.
6. **C2**: The stage in which the adversary establishes communication between a malicious server and the victim’s systems. This allows for backdoor commands and communications to be exchanged.
7. **Actions on Objectives**: The final stage of the Cyber Kill Chain, comprising any activities after the initial attack, aiming to further the interests of the adversary. This stage is relatively open ended. 

- The use of the Cyber Kill Chain is linear, focusing on the attack narrative.
- When using the Cyber Kill Chain, it is important to have a clearly defined stage for any event in a cyber attack, as detecting activity in one stage of the Cyber Kill Chain allows network defenders to prepare for the next actions of the adversary.
- Even if the attack has taken place in the past, using the Cyber Kill Chain structure provides guidance to forensic personnel in determining the previous malicious activities. 

### Diamond Model
![fc88a68e-0e47-41c5-bd0b-580c1082f2fa](https://github.com/user-attachments/assets/1e854a4b-e5d2-4c61-97a7-e7c5a278740c)

- A third CTI model is the Diamond Model of Intrusion Analysis, or, more simply, the **Diamond Model**.
- According to the Diamond Model, every cyber incident may be depicted as a diamond with four “points,” as illustrated in Figure 6.2-3 and described as follows:
  - **Adversary**: The threat actor responsible for the cyber attack or any other malicious activity.
  - **Capabilities**: The TTPs associated with the adversary.
  - **Infrastructure**: The adversary’s assets, both physical and logical, that are leveraged in their operations.
  - **Victim**: The target of attacks, which may be organizations, individuals, or specific vulnerabilities.
- Unlike the ATT&CK and Cyber Kill Chain models, the Diamond Model emphasizes the relationships of each component in the model.
  - The relationships of the four points of the diamond may be analyzed, and greater insight into the threat actor may be gained.
  - These points may be converged into a single phrase:
    - **“An [ADVERSARY] uses a [CAPABILITY] over a(n) [INFRASTRUCTURE] against a [VICTIM].”**

- This is useful, as it illustrates how exactly an attacker employs their skills against a certain victim.
- The **vertical** and **horizontal** axes are important parts of the Diamond Model.
  - The **vertical axis**, connecting the Adversary and Victim components, depicts the **sociopolitical axis**.
    - This axis shows the **reasons and goals for the adversary’s taking action against the victim**.
  - The **horizontal axis**, connecting the Capabilities and Infrastructure components, depicts the **technical axis**.
    - This axis shows the **resources and technological** aspects of the attack.
  - Plotting data into the Diamond Model, unknown information can present itself.
    - For example, knowing the IP address of a C2 domain may indicate the identity of an adversary.
    - The more points of the diamond that have information, the easier it is to infer knowledge of the missing pieces. 

- This gives analysts the ability to look at future attacks and see how behaviors of the past can indicate potential future actions.
- The Diamond Model also offers the ability to discover knowledge gaps, as plotting data into the model can help with highlighting missing information more clearly.
- Finally, the Diamond Model can be incorporated into mitigation planning and security frameworks. 

- Using the same scenario from the Cyber Kill Chain section, a Diamond Model can be created and filled with information, as the following example shows:
  - **Adversary**: Currently unknown.
  - **Capabilities**: Phishing, FTP exploit, logic bombs, persistence malware, credit card information theft.
  - **Infrastructure**: C2 server (IP and registered domain name), email addresses.
  - **Victim**: A financial institution, vulnerable FTP server.
- In the above example, the Adversary component of the Diamond Model is unknown.
- However, by using the information gathered about the capabilities, infrastructure, and victim, it can be reasonably surmised that the adversary is a group of attackers who aim to create financial gain for themselves.
- Through forensic examination of attacker email addresses, domain names, coding styles, IP addresses, and more, this adversarial persona may be further defined.  

### Comparing CTI Models
![94406738-e054-4695-9677-c68c89e5e7dd](https://github.com/user-attachments/assets/1a93d62b-efbd-4fd3-be1c-d3505c6206f4)


## MITRE ATT&CK FRAMEWORK
### ATT&CK Matrix | Techniques
- **Reconnaissance**: Techniques that involve adversaries actively or passively gathering information that can be used to support targeting (for example, staff/personnel, infrastructure).
- **Resource Development**: Techniques that involve adversaries creating, purchasing, or compromising/stealing resources that can be used to support targeting.
- **Initial Access**: Techniques that use various entry vectors to gain an initial foothold within a network (for example, a spear-phishing link).
- **Execution**: Techniques that result in running attacker-controlled code on a local or remote system (for example, PowerShell).
- **Persistence**: Techniques used to maintain persistent access to a system (for example, logon scripts).
- **Privilege Escalation**: Techniques used to gain higher-level privileges on a system or network (for example, process injection).
- **Defense Evasion**: Techniques used to avoid detection (for example, Dynamic Linked Library [DLL] side-loading).
- **Credential Access**: Techniques for stealing credentials, such as account names and passwords (for example, Kerberoasting).
- **Discovery**: Techniques used to gain knowledge about the system and internal network (for example, network sniffing).
- **Lateral Movement**: Techniques used to enter and control remote systems on a network from the already compromised host (for example, Pass the Ticket).
  - Attackers typically have to pivot through multiple machines — usually the weakest link in the chain of machines — to ultimately reach their end objective.
- **Collection**: Techniques used to gather information relevant to following through on the attacker’s objectives (for example, input capture).
- Command and Control (**C2**): Techniques attackers may use to communicate with systems under their control; often disguised to look like normal Hypertext Transport Protocol (HTTP) traffic (for example, domain fronting).
- **Exfiltration**: Techniques used to steal data from a network (for example, exfiltration over web service).
- **Impact**: Techniques used to disrupt availability or compromise integrity by manipulating business and operational processes (for example, firmware corruption).
  - Impact is the result on a system after the attacker accomplishes their ultimate goal and is the most recently added tactic.


## Threat Mitigation
### Cyber Kill Chain
#### Phase 1: Reconnaissance
- The first phase of the kill chain, Reconnaissance, covers initial information gathering before the adversary begins an attack.
- Reconnaissance can be leveraged in an attack cycle in several ways, from identifying potential access vectors to determining the most viable target for conducting an attack.
- When attackers survey a target, they look for as much information as possible, such as network technologies used and services exposed.
- Network technologies include unique **protocols**, **hardware**, and **software** standards such as **optical**, **broadband**, and **radio**, which all provide varying access vectors.
- Examples of services that may be exposed to publicly available networks or even the internet include the following:
  - File Transfer Protocol (**FTP**)
  - Simple Mail Transfer Protocol (**SMTP**)
  - Remote Desktop Protocol (**RDP**)
  - Telnet

- Examples of key data elements that an attacker can leverage that may be gathered by initial reconnaissance methods include the following:
  - Enumeration (list, identify, and research) of internet-facing assets.
  - Internet Protocol (IP) ranges and associated domain names, ports, and services.
  - Implemented technologies or application platforms.
  - Leaked credentials or keys.
  - Leaked documents, logs, configuration files, and backup files.

- One type of attack reliant on the Reconnaissance phase is spear-phishing.
- Through reconnaissance, the adversary can gain knowledge through press releases and an organization’s or employee’s social media presence.
- This can aid attackers in identifying persons to target and topics to address ineffective phishing messages that are likely to trick victims.

- Threat actors can aggregate actionable data about a target entity through the use of Open Source Intelligence (OSINT) collection.
- **OSINT** is the method of **gathering information through publicly available sources** on the internet. 

- Three types of information gathering exist in security:
  - **passive**
    - Passive OSINT techniques are those that do not engage or interact with the target entity.
    - The requirement of these methods is that no artifacts or traces of the queries are left behind for the target to discover.
    - Techniques of passive reconnaissance include the following:
      - Collecting public information and records regarding network or person.
      - Aggregating unprotected social media information.
      - Recovering previous versions of the target's website (for example, via WayBack Machine).
      - Querying such search engines as Google or Shodan for exposed devices or services belonging to the target.
  - **semi-passive**
    - Semi-passive OSINT techniques directly interface with the target but in a way that is plausibly deniable and blends in with standard traffic.
    - Techniques may include interacting with a target's website or other online services but blending into the white noise of normal traffic and performing all actions from a non-attributable IP/domain.
    - If the attacker is detected in a later phase of the Cyber Kill Chain by the target, all evidence of these methods would be non-attributable.
  - **active**.
    - Active OSINT techniques are methods that directly interface with a target and can leave an identifiable signature.
    - An example of an active OSINT technique is a port scan.
    - Conducting a port scan rapidly queries available ports on a target and can provide a great deal of information to aid in an attack.
    - However, many modern security systems are configured to detect such activity and flag it as malicious.

#### Phase 2: Weaponization
- After identifying a viable entry into a target network, an attacker enters the Weaponization phase, in which they develop their attack with the goal of ensuring an initial point of presence in the victim’s network and enabling additional access.
- The activities that an attacker performs in this phase occur on a workstation or network controlled by the attacker, so a defender does not have a real-time view of the attacker's actions during this phase.
- The Weaponization phase may include an attacker crafting an enticing social engineering ploy to compel an employee to open an attachment or click a malicious link.
- Other times, the attacker may require a custom-crafted payload; in these cases, the Weaponization phase occurs when the parameters of this malware are **defined**, **compiled**, and **tested**.

- The Weaponization phase is informed by the results of reconnaissance.
- For example, with successful information gathering, a threat actor can determine the host endpoint protections in place in a victim's environment.
- At that point, upon compiling and testing their malicious payload, attempts can be made to evade that specific technology's detection.

- The Weaponization phase may take some time if there are special needs for the exploit or payloads that need to be developed.
- Advanced attacks may require research and development in an isolated lab environment prior to conducting an actual attack.
- Also, a series of unknowns exist when going into any remote cyber attack; even if the exploit works, a chance exists that the payload may fail and need to be reworked.

- To exploit an identified vulnerability in the target's network, an attacker uses private, custom, or open-source exploits.
- In any case, the actor needs to weaponize their tool to match the unique system and network architecture requirements of the target they intend to attack.
- In addition, they usually implement obfuscation techniques to evade network and host detection capabilities.

- There are no guaranteed defenses against some weaponization techniques, such as with a zero-day technique.

- Examples of weaponization techniques include the following:
  - **Obfuscate** (for **example**, **encrypt**, **encode**, or **reorder**) a malicious executable to evade antivirus (AV) detection.
  - **Modify** open-source exploits to target a specific target's IP and security configuration.
  - **Build** a novel exploit against a target device or platform (**zero-day**).

#### Phase 3: Delivery
- In the Delivery phase, an attacker attempts to circumvent any controls in the defended network to deliver their weaponized payload to their target.
- A host of TTPs exists that an adversary may use to deliver their exploit.
- The chosen TTP is dependent on information gained about the target from the Reconnaissance phase.
- For example, the attacker may identify a vulnerable web server in the target’s network and then deliver the exploit by sending specially crafted packets over the internet.
- Or the attacker may learn that their target regularly visits a certain website and therefore delivers their payload via a watering hole attack.
- Given the ubiquity of email, attackers most likely attempt to deliver their exploit via malicious attachment or to entice a user to click a link to prompt a malicious download via phishing. 

#### Phase 4: Exploitation
- The Exploitation phase takes advantage of an identified vulnerability in the target environment with the intent of triggering the attacker’s delivered code.
- The vulnerability could be technical and target a vulnerable application or Operating System (OS), or it could be non-technical and target the user.
- For a technical vulnerability, two exploit vectors generally exist:
  - **remote**
    - Remote exploits target network-enabled services and devices that are not properly patched or configured.
    - These exploits allow an attacker to achieve Remote Code Execution (RCE), where they can run code on a remote target.
    - Web servers, FTP servers, email servers, RDP, Server Message Block (SMB), and a host of other popular services all have been the target of RCE attacks.
  - **local**
    - Local exploits target local services and leverage attack methods to execute malware or escalate privileges.
    - An attacker needs to gain an initial foothold on their target to take advantage of a local exploit.

- Aside from the vector, exploits take advantage of three types of vulnerabilities:
  - **Public**: Publicly disclosed vulnerability that has an available patch.
  - **N-day**: Publicly disclosed vulnerability with no current patch.
  - **Zero-day**: Undisclosed vulnerability.

- The National Vulnerability Database (**NVD**) maintains a repository of publicly disclosed vulnerabilities alongside references that provide supplemental information related to the vulnerability, such as the **vendor advisory**, **technical details**, **workarounds**, and **mitigations**.
- Often, a vulnerability disclosure may lead to publicly available exploit code as attackers and security researchers analyze the vulnerability’s technical details or reverse-engineer the patch. 

- Undisclosed vulnerabilities (**zero-days**) have an inherently greater cost to exploit because they are **less readily available** and **difficult to discover**.
- Therefore, attackers exploit zero-days sparingly due to fear that the more they exploit zero-days, the more likely they are eventually detected.

#### Phase 5: Installation
- During the Installation phase, the attacker installs a Remote Access Trojan (**RAT**) or reconfigures a system to maintain persistence on a target machine.
- This way, should they want future access, an attacker does not have to repeat phases 1 through 4.

- To enable persistent access, an attacker must choose an appropriate technique that varies from target to target and may depend on their level of access.
- A normal user does not have the same privileges and access as a root or administrative user.
- Even the most adept attackers might have trouble picking the perfect approach for each situation.
- An attacker may choose to install malware such as a RAT.
- Alternatively, on a Windows machine, an attacker may simply add a remote desktop user if the service is available or leverage a similar technique with other remote management services, such as (WinRM).

- Non-malware-based installation on a Unix system could include adding an authorized Secure Shell (SSH) key to the target system, enabling the attacker to connect to the target remotely with SSH without needing a valid password.
- Additionally, many Unix systems come with Netcat, which can be used to open a port and listen for traffic.
- An attacker could daemonize (run in the background as a service) a Netcat connection on a given port, which provides the attacker on-demand Command and Control (C2).

- Because an attacker may want to maintain a presence for a long time, attackers try to masquerade their activities to make them appear legitimate or benign to users and security tools.
- Examples of masquerading include naming a RAT with a filename that matches the name or location of legitimate files or resources, adding a double extension in the filename to disguise the true file type (for example, .zip.exe), and modifying a malicious file’s metadata to match that of a legitimate file.

#### Phase 6: C2
- Regardless of the method of communication, the purpose of C2 is consistent: **maintain control of compromised assets to perform additional operations**.
- This requires three components:
  <img width="3334" height="776" alt="3d01eb95-23c4-41fa-bba0-3791b06fbd2a" src="https://github.com/user-attachments/assets/3880c7a0-61ac-4aa3-80e7-3c308920e23f" />

- Generally, C2 traffic conforms to one of two paradigms:
  - **scheduled**
    - Scheduled C2 generally involves an installed program or service that reaches out to an attacker-controlled resource for its management instructions.
    - This type of control requires the attacker to have procured or compromised another resource on the internet.
    - This C2 method through an outbound connection means that this type can be installed on devices that are not directly reachable from the internet (such as those behind a firewall).
  - **on-demand**.
    - On-demand C2 requires that the service or port be accessible to the attacker, which may not always be possible, depending on the network topology.
    - Installations that respond to on-demand C2 can include malware configured to listen on an otherwise-unused port or remote control services like RDP or SSH, if associated accounts have been created or compromised.

- The desirable attributes of a C2 channel are generally consistent and should include the following:
  - **Be Stealthy**: The data flow in the channel should be either hidden under encryption or obscured by mimicking normal network traffic through similar channels (or both).
  - **Be Indistinct**: The command source and victim machine should appear to be related so traffic between them does not stand out.
  - **Be Redundant**: The channel itself should be difficult to shut down or block if discovered.
- Rarely are all three attributes available to an attacker.
- For instance, access by SSH provides encryption, but SSH traffic from an unknown source through a network edge likely appears suspicious to network defenders.
- As with installation, knowing which C2 channel is appropriate for which environment is both an art and a science and might require an extensive amount of triage. 

#### Phase 7: Actions on Objectives
- If the attack was successful, the attacker then begins the next phase of the attack: Actions on Objectives.
- This phase is largely defined by the goal of the attacker’s campaign.
- During this phase, the attacker uses their C2 channel to control their target and achieve their purpose.
- Typical actions taken at this phase include **enumerating attached devices**, **credential harvesting**, **Active Directory (AD) compromise**, and **stealing sensitive files**.
- Extreme examples of next steps by threat actors in this final phase include disruption of network services and data destruction.
- Alternatively, the initially compromised host may just provide a pivot point to move laterally further into the target network.
- Depending on the objectives of the attacker, this phase may include direct “hands-on keyboard” interaction with the target. During this interactive portion of the attack, the attacker experiences a high risk of exposure; therefore, tradecraft is essential.
- Advanced Persistent Threats (APT) are skilled at hiding, whereas less-skilled attackers or those with a shorter timeline do not use stealth and may not care about being caught.

### Disrupting the Chain
- Each phase of the kill chain presents an opportunity for a defender to disrupt, deny, and degrade an adversary’s attack.
- If a defender can put a halt to an attack by enacting a mitigating control, it forces the attacker to either change tactics, move on to another target, or stop their attack.
- Ideally, an organization should strive to be a “hard target,” where the effort involved to penetrate their defenses convinces a would-be attacker to concentrate elsewhere.
- Furthermore, the earlier in the chain that a defender can detect and prevent an attack, the less impactful that attack is.
- All organizations wish they could have an endless budget and ability to hire and retain an outstanding security team to protect their networks.
- However, not all organizations are well equipped, in which case the organization needs to perform a proper terrain analysis and identify their key terrain.
- From there, the organization should put in the proper mitigating controls to thwart the attacker at each phase of the chain.
- A full rundown of how to perform a terrain analysis is beyond the scope of this lesson, but the next section examines a few examples of how to disrupt the chain at each phase.

#### Disrupting Reconnaissance
- Disrupting the Reconnaissance phase requires limiting the public exposure of information for an organization and its employees.
- The less an attacker can learn about a target through passive and active reconnaissance, the more difficult it is for them to craft their weapon.
- In addition, an organization that seems “boring” on the web is more likely to be passed over by an attacker than an organization that appears “flashy.”

- The most extreme defense is for an organization to have no web presence and to not allow their employees to share information about their roles or the company on social media platforms.
- For most businesses, however, it is a great benefit to have an online presence to advertise themselves, gain more business, and increase revenue.
- Therefore, it behooves an organization to find the appropriate balance between sharing too much and too little.

- There are sensible steps that an organization can take to combat passive reconnaissance.
- Some of these steps are as follows:
  - **Enforce a social media acceptable use policy**, which is an organizational policy that prescribes what information employees can share on the web (for example, social media profiles, blogs, wikis).
  - **Follow Operations Security (OPSEC) best practices** to prevent the adversary from observing or stealing sensitive or critical information.
  - **Restrict web crawlers**; that is, prevent indexing by search engine spiders, or limit their access to only certain web pages and content. 
- Inevitably, as an organization grows in size, it becomes exponentially more difficult to control the release of sensitive information.
  - Therefore, regular training of employees to be conscious of what they share online is needed, along with providing an appropriate reporting channel for employees to report concerns and potential violations.

- Whereas disrupting passive reconnaissance requires a human element, disrupting active reconnaissance typically takes a technical approach.
- Here an organization must try to limit their attack surface as much as possible and restrict an attacker’s ability to conduct scanning against their network.

- Some protective measures an organization can take to disrupt active reconnaissance are as follows:
  - Disable unused ports and services.
    - This limits the number of paths that an attacker can take to infiltrate a network.
    - Modify server error messages.
      - Reduce the amount of information that an attacker can glean from scanning.
    - Implement a firewall Intrusion Detection System/Intrusion Prevention System (IDS/IPS).
      - Provide early detection of scanning activity, and block connections from The Onion Router (Tor) and third-party Virtual Private Network (VPN) exit nodes.

#### Disrupting Weaponization
- It is impossible to stop an attacker from weaponizing their payload because this act occurs on the attacker's side.
- However, it is possible to defend against techniques used for weaponization. 

- The first and most effective defense against weaponization for an organization is to ensure that its systems are patched for known vulnerabilities.
- This prevents an attacker from coupling their payloads with known exploits.
- Patch management exponentially raises the difficulty for an attacker to penetrate an organization’s network by forcing them to research and develop novel exploits.
- An effective patch management process begins with having an accurate Asset Inventory Management System (AIMS), where version and patch information for each system and its components are stored and updated.

- In addition to patch management, an organization should create secure baselines, meaning that any unneeded capabilities should be disabled from the onset.
- For example, attackers commonly use Microsoft Office macros as a technique for weaponizing. An effective mitigation is to configure a system to disable macros as part of a secure baseline.
- Other configuration settings that should be modified or hardened include the following:
  - **Disabling browser plugins**.
  - **Disabling Microsoft Office add-ins**.
  - **Enabling behavior prevention on endpoints**, such as Attack Surface Reduction (**ASR**) on Windows 10.

- Weaponization, along with later phases in the kill chain, can also be disrupted with the addition of defensive tools.
- This helps an organization to create defense-in-depth.
- For many attackers, the act of weaponizing their payload is done with an automated tool, or weaponizer.
- Unless the attacker has the resources to spend in creating their own weaponizer, they likely use a publicly available one.
- Well-known, publicly available weaponizers include the Veil Framework, Social Engineering Toolkit, Burp Suite, and Metasploit.
- Generally, every weaponizer, even custom-made ones, leaves artifacts and patterns in their use that can be detected by an IDS or prevented by an IPS.
- Organizations should ensure that they regularly update the signatures for these security tools.

#### Disrupting Delivery
- Once an attacker has weaponized their payload, they deliver it into the targeted organization’s network.
- Numerous technical controls can be put in place to disrupt delivery, regardless of the vector used by the attacker.
- Similar to the Weaponization phase, organizations should invest in adding defensive layers with IDS and IPS tools on the host and network side.

- An effective mitigation to disrupt delivery by web is to employ web filtering via a secure web gateway or web proxy.
- Security technology has greatly improved for this control type over the years.
- Many solutions in this market today can statically and dynamically analyze web pages for malicious content and then make an informed decision to allow or disallow the traffic to an employee’s web browser. 

- Because email is such a popular delivery mechanism for an attacker, it is highly recommended that an organization put in measures to inspect emails before the messages make it to their intended recipient.
- One technique that attackers do to add legitimacy to their email phishing attacks is to spoof the sending address.
- To combat this, it is possible to configure email authentication methods like DomainKeys Identified Mail (**DKIM**) and Sender Policy Framework (**SPF**).
- A phishing email may contain a malicious attachment for the user to open or a Uniform Resource Locator (URL) for a recipient to click on.
- Solutions are available that can automatically detonate the attachment or URL in a malware sandbox and report the analysis.
- To combat malicious attachments delivered via email, an organization, if possible, should create an allowlist of acceptable file extensions and deny or strip any attachment that does not meet that criterion.
- It is also common practice for URLs to be sanitized to prevent employees from automatically clicking the links without first inspecting them or to even direct the link to an isolated web browser to prevent possible infection. 

- Organizations, particularly ones in air-gapped networks, should be prepared to defend against malware delivered physically via removable media (for example, Universal Serial Bus (USB), CD/DVD, or Secure Digital [SD] cards).
- The most effective approach to deter this threat is to disable the use of removable media altogether unless allowed through exception.
- When allowed through exception, it is recommended to check for the presence of malware by scanning the device on an isolated workstation prior to inserting it into the intended system.

#### Disrupting Exploitation
- The Exploitation phase occurs once the attacker’s malware has slipped through all the defenses.
- This is when the intent is to run the delivered code.
- Because most of this phase is dependent on the techniques used during the Weaponization phase, many of the same defenses apply.
- These defenses include **patch management**, **configuration of secure baselines**, and **addition of tools** that could help detect and prevent exploitation attempts.
- Those steps help mitigate against exploit attempts against the system, but what about exploit attempts against the user?

- For phishing attacks attempting to exploit a user, a critical defense is user awareness.
- Employees must be regularly trained so they know how to spot a potential phish, whether it comes from email, Short Message Service (SMS), web delivery (for example, a social media post with a malicious link), or some other medium.
- Part of the user awareness training must include a reporting mechanism to report potential threats.
- Although humans are well known for falling victim to phishing attacks and remain the most-used vector for delivery, they can also be one of the best sensors for spotting an attack.
- It takes only one employee from a pool of targeted employees to report the phishing attempt, which, in turn, brings attention to the phishing attempts that were missed.

#### Disrupting Installation
- Armed with a successful exploit, the attacker focuses on gaining persistent access, typically through the installation of a RAT or through reconfiguring the system and creating a backdoor.
- At this phase, a critical component to disrupting installation is to **limit user privileges as part of the system’s secure baseline**.
- Normal users should not be able to make registry changes or install new software.
- If possible, system administrators should **disable macros and PowerShell for Windows-based systems** and **confine users on Linux systems to chroot jails**.
- In addition, organizations should ensure that **monitoring is in place** to catch abnormal process activity via a User Behavioral Analysis (**UBA**) or Endpoint Detection and Response (**EDR**) engine.
- Examples of abnormal activity include the following:
  - Microsoft Word spawning PowerShell
  - Apache Tomcat spawning Bash
  - Java calling rundll32

- When an attacker’s malware is discovered or installation is prevented, if conditions permit, the malware should not be immediately destroyed.
- It is important to study it and understand its capabilities.
- Gathering Indicators of Compromise (IOC) related to the malware helps to find additional victims and put the necessary protections in place to disrupt future attacks and later phases of the kill chain by the actor.

#### Disrupting C2
- At the C2 phase of the kill chain, the actor has managed to install their chosen persistence mechanism and wants to open a channel to remotely manipulate the system.
- Usually, actors choose common protocols and ports to create this channel for two reasons: **to blend in with normal traffic** and **to avoid being blocked by network edge devices**.
- Therefore, the most common C2 protocols used by attackers are web ([HTTP/HTTPS]) and Domain Name System (**DNS**), as these are generally always open within a network. 

- All traffic leaving an organization’s network should be scrutinized for its destination and protocol.
- As in the Delivery phase, web filtering is an effective mitigation to prevent an attacker from setting up a C2 channel.
- Organizations should **configure clients to proxy their DNS and web requests** and **block domains and IP addresses that are known as bad**.
- In addition, it is highly recommended that a **proxy block connections to recently registered domains**, as threat actors frequently stand up new infrastructure to perform their attack.

- When possible, organizations should invest in Deep Packet Inspection (DPI) tools that break apart the header and analyze the content of network packets.
- DPI provides defenders with a more robust opportunity to detect and prevent threats by providing additional capabilities, such as the ability to do the following:
  - **Reconstruct delivered malware**.
  - **Identify potential data leaks**.
  - **Recognize protocol traffic over a mismatched port** (for example, DNS request over Transmission Control Protocol [TCP] port 80).

- Most attackers encrypt their communication channel using Secure Sockets Layer (SSL) or Transport Layer Security (TLS) protocols.
- As a result, organizations that do not add SSL/TLS decryption to their security stack are leaving an open door for an attacker to take advantage of.
- A drawback of decrypting traffic, however, is network latency because it adds processing time, so it becomes a balancing act for each organization to weigh the threat against the disadvantages of implementing any new control.

- Defenders can also disrupt encrypted communications by gathering and pivoting on the information contained in the SSL/TLS certificates used by the adversary.
- The approach here, which does not require DPI, is to fingerprint the SSL/TLS negotiation between an infected host and the attacker’s C2 infrastructure.
- One method of fingerprinting this negotiation is known as JA3.
- JA3 creates an MD5 hash based on the decimal values of bytes in certain fields of the Client Hello packet: **Version**, **Accepted Ciphers**, **List of Extensions**, **Elliptic Curves**, and **Elliptic Curve Formats**.
- Armed with this fingerprint, a defender can bolster their hunting to prevent future compromises or identify active ones.

#### Disrupting Actions on Objectives
- Once the attacker has set up their communication channel, they move toward completing their objective.
- This is the last opportunity that a defender has to disrupt their activity before serious impacts can be inflicted.
- Those impacts depend on the attacker’s intent, so defenders should try to understand the threat scenarios that they face before an attacker ever reaches this phase.

- Unless the attacker was lucky enough to immediately land on a system that meets their objectives, a typical goal for attackers is to further their access by elevating their permissions and moving more deeply into the victim’s network.

- For example, a bank would be concerned with an attacker who is financially motivated and intent on modifying wire transfers and bank accounts.
- The attacker may manage to phish a bank employee and compromise that employee’s workstation, but that may not be enough access for them to fully achieve their goals.
- They need to elevate their privileges and gain access to the system that handles such transactions.
- This is where proper **network segmentation** and **authentication controls** are a defender’s best defenses to disrupt the attacker at this phase.
- These controls at least slow the attacker down, increase their dwell time (the length of time an attacker is present on a victim’s network), and give the defenders more time to discover the adversary and eradicate them. 

- Even the most secured organization may have an attacker eventually reach this phase.
- It is critical for defenders to have **incident response playbooks** in place for the possibility of such an occurrence.
- Having a preplanned course of action helps to reduce the defender’s Mean Time To Respond (MTTR), which is the average time required to restore a system to an operational condition after learning about a cyberattack or intrusion.




# MOD 7
## Defensive Posture
### Undserstanding Defensive Posture
- **Defensive posture** is the _overall state of cybersecurity defenses for an organization and is determined by reviewing the entire picture of security within an organization_.
- This includes the following categories:
  - **Security policies**: All operating policies and how they relate to security, as well as security policy and procedures.
  - **Security training**: Security personnel training as well as entire organizational staff training.
  - **Security architecture**: All assets, asset configurations, and the network topology of an organization.
  - **Risk management**: Vulnerability management, risk analysis, and patch management.
  - **Security controls**: Physical and logical security controls to defend the organization.
  - **Security personnel**: The training and experience level of security personnel for an organization.
- Assessing defensive posture gives an overall idea of the security currently in place and efforts needed to ensure proper protection.
- Of the categories above, a **CDA should focus on security architecture**, **risk management**, and **security controls**.
- All other areas fall under the scope of organizational leadership and are handled at the organizational level.

#### Measuring Defensive Posture
- Organizational leadership is in charge of deciding the level of defensive posture of their organization.
- Leaders should work in conjunction with cybersecurity personnel to analyze all organizational aspects of security.
- Risks should be analyzed alongside all defensive measures in order for leadership to dictate the overall level of defensive posture.
- The MITRE (ATT&CK®) framework can be used to analyze defensive posture.
  - This process consists of reviewing all aspects of threats through the different sections of the ATT&CK matrix.
  - All areas from within the ATT&CK matrix are compared against defensive capabilities. 

- Analysis of these common tactics is performed in two ways.
  - The first way is from a high-level review, without a specific threat in mind.
    - This high-level analysis focuses on preventing or mitigating the overall threat of common adversarial tactics.
    - For example, high-level analysis of the common tactic of privilege escalation should ensure that mitigating controls exist to prevent general privilege escalation threats and monitor for the threat of privilege escalation occurring within the organization.
  - The second type of analysis is far more granular and is applied on a per-threat basis.
    - This level of analysis is used to review every aspect of a specific threat and ensure defenses are in place to help mitigate or eliminate the threat.
    - This is far more resource intensive and is generally prioritized for threats that pose extremely high risks to the organization.
- Another method for analyzing defensive posture is use of a Security Information and Event Manager (**SIEM**).
  - SIEMs can be used to gather an overarching picture of the defensive posture for the security architecture.
  - This picture can be gathered by parsing through logs and then generating searches that correlate many statistics about security operations.

- A defensive posture dashboard can provide defensive posture and architecture information at a glance.
- This dashboard typically includes the state of devices on the network, graphs and charts of key log and network activity, details from security assets, vulnerability metrics, alerts, and statuses of all potential incidents.
- One issue with this type of measurement is that getting a SIEM tuned to accurately provide the defensive posture can take significant time — even years — depending on the size and complexity of an organization.
- SIEM dashboards can be prone to false positives, so time and resources are required to tune dashboards to an adequate baseline to reflect the organization’s defensive posture.

#### Network Defensive Posture
- When performing Enable Harden functions while on mission, Network Analysts are primarily focused on strengthening the defensive posture of the network.
- Whereas Host Analysts are more concerned with specific controls and patches to strengthen the defensive posture of individual devices, Network Analysts must look at defenses from the network perspective. 
- When addressing overall defensive posture, a holistic approach must be taken.
  - However, Host or Network Analysts may create more specific dashboards that can be used to narrow the scope to just the host activity or network activity.  
- A network defensive posture dashboard should include **port**, **protocol and service information**, **IP** address information, **network traffic trends**, **anomalous network activity**, and **signatures or alerts specific to network traffic**

#### Defensive Posture and Cyber Threat Intelligence
- Knowing an organization is at risk from threats or threat actors alters defensive posture.
- An organization may have an extremely strong defensive posture, but that could completely change if new vulnerabilities are found that impact their operations.
- If an unknown vulnerability or threat is identified, the defensive posture may be weakened until adjustments are made to mitigate the new threat.
- Due to this, a need arises to integrate knowledge of new threats and vulnerabilities into operations.
- This process is called intelligence-driven Defensive Cyberspace Operations (DCO). 

- Intelligence-driven operations have always been an integral part of all operations in the U.S. Department of Defense (DoD), including DCO.
- In essence, the goal of intelligence-driven DCO is to leverage Cyber Threat Intelligence (CTI) to establish and strengthen the defensive posture of an organization.
- The understanding of threat actors and their techniques is key to building secure architecture and finding new detection measures for threats.
- Using intelligence-driven DCO minimizes risk and emboldens defensive posture.

### Implementing Intelligence in Operations
- Intelligence should be ingested into operations to adequately defend against known threats.
- Using intelligence, security analysts can **identify patch recommendations for known threats**, **devices that need additional security controls**, and **monitoring/alerting priorities**.
- When using CTI, cybersecurity personnel should gather an understanding of TTPs used by threat actors that are related to operations.
- Leveraging TTPs can help to identify areas where threat activity can be discovered and defensive measures can be implemented.
- Additionally, knowledge of TTPs can be used to create multiple defensive measures.
- Creating layered defenses for known threats can drastically improve defensive posture.
- When a new threat is identified, the ATT&CK framework can break down the individual phases of the attack.
- Using this knowledge, security controls can be implemented, altered, or created to increase defenses for the new threat.

### Deliberate Defense Posture
- Cyber Protection Teams (CPT) may need to establish a deliberate defense posture to adequately defend against adversary activity.
- In some instances of an engaged attack, threat actors may be in an area with poor visibility or network gaps that result in a weakened defensive posture and defensive capabilities.
- The CPT modifies the environment in cyberspace to disrupt actions in those areas and force threat actors into areas where CPTs have better visibility and capability to respond to threat actor activity.

### Securing Device Configs through CTI
- When analyzing any network, ensuring secure configurations on assets is a priority.
- Threat intelligence can provide even further insight into different vulnerabilities that are actively being exploited within an industry.
  - This can lead to prioritization of vulnerabilities over others due to a known active threat.

- Intelligence may direct that certain **ports**, **protocols**, **software**, and **devices** are part of a threat actor’s **TTPs**.
- This can lead to **immediate patching priorities** for these devices, **alterations to current configurations**, **creation of new alerting or response actions**, or even **removal of devices**.
- In some cases, recently patched or upgraded devices may be the target of a newfound threat. This can lead to rolling back to previous versions that had fewer security issues if it is deemed safer for operations.

### Establishing Monitoring Priorities with CTI
- SIEMs can be used to implement intelligence directly into operations by creating specific searches and alerts that monitor for cyber threats found through intelligence.
- Through root cause analysis of malicious activity, specific alerts and monitoring capabilities are generated to avoid repeat offense of this activity.
- Leveraging SIEM tools, intelligence can be used to drive the creation of new searches and alerts as soon as threats become known.

- Additionally, (**IDS**) and (**IPS**) can be used to create or highlight alerts that monitor for specific threats that have been identified through intelligence.
- These systems use signatures for known threats that create a log or block traffic if the signature’s requirements are met.
- The signatures generally look for specific traffic patterns or file accesses that are associated with known malicious activity.
- They can generate false positives as well, which means the system needs to be tuned and catered to each organization’s architecture.

- Many SIEM and IDS/IPS tools have intelligence capabilities that allow them to connect to a CTI feed.
- These provided threat feeds can be used to ingest known alerts or searches to identify new or previously unidentified threats or capabilities of threat actors.
- CTI feeds are great solutions for organizations that may not have an internal intelligence capability.
- However, integrated CTI feeds are often a paid service, so organizational leadership must determine if this method is worthwhile.

### Training with CTI
- Personnel within an organization are always at risk of being targeted by an attack from social engineering, phishing, and similar attacks.
- Developing training in conjunction with threat intelligence can help to keep personnel informed of possible threats that may affect an organization.
- If an organization is at risk of being attacked by a specific APT, then training should be made showcasing their attack methods and how these threats could be directed toward the organization.
- Using this intelligence in training is a critical portion of increasing the defensive posture of an organization.



## IOCs in Search and Filters
### Types of IOCs
<img width="1801" height="1166" alt="e16f03f2-58c8-400a-83b3-fcf93bfa4885" src="https://github.com/user-attachments/assets/1f595e9a-94fc-403a-948c-9f24f5706e32" />

#### Hash Values
- A hash value is an output from a hashing algorithm whenever a specific input is given.
- It is the most trivial type of IOC for malicious threat actors to change.
- For example, a virus might make trivial nonfunctional changes to itself that change the hash value of the file, or a payload being delivered by a malicious threat actor might be trivially padded, encrypted, or compressed in order to evade hash-based detection methods.
- Although hash values can be incredibly useful to locate malicious files, more sophisticated attackers can easily evade any method of detection using hash values.

#### IP Addresses
- IP addresses indicate the origin or destination of potentially malicious network traffic.
- Due to the ubiquity of Virtual Private Network (VPN) servers and other factors, IP addresses can be easily masked or changed by a malicious threat actor.
- Restricting traffic from known malicious threat actors might still hinder or delay them, and these values can still be useful in post-compromise analysis.

#### Domain Names
- Malware, (C2), and data exfiltration schemes may use domain names for various reasons.
- These reasons include rapidly changing IP addresses, submitting (DNS) queries to exfiltrate data via nontraditional channels, and attempting to hide as legitimate traffic by using legitimate-looking domain names.
- New domain names can be easily registered by a malicious threat actor; however, this generally requires a monetary expenditure.

#### Network/Host Artifacts
- Network and host artifacts consist of the portions of network traffic or data found on hosts that indicate potential malicious activity.
- Examples of network or host artifacts include **client requests**, **server responses**, **files**, **registry entries**, and **system log entries**.
- Although this encompasses every file on a system and every piece of traffic passing over the wire, this category of IOCs focuses on specific pieces of data that can be used to determine if activity is legitimate or malicious.
- Denying a malicious threat actor specific network or host artifacts can require more of an investment in time or effort to evade.
- For example, if a specific user agent string is identified as malicious, blocking all traffic from that source whenever that user agent is encountered might require the malicious threat actor to reconfigure or recompile the tool that generates that IOC.

#### Tools
- Tools are the actual software that a malicious threat actor uses to perform their attacks, such as password-cracking utilities, exploitation frameworks, and Remote Access Tools (RAT).
- Allowing only legitimate software to execute via allowlisting can be very effective in preventing the usage of tools by a malicious threat actor.
- From a networking perspective, analysis of the upper layers of the Open Systems Interconnection (OSI) model can be used to fingerprint malicious tools and deny their access.
- Denying entire tools to a malicious threat actor can require pivoting to new, less-understood tools or significantly slow down discovery, exfiltration, or exploitation of a network.

#### Tactics, Techniques, and Procedures
- Detecting and denying entire (TTP) is, as its position on the pyramid indicates, the most effective level to be operating at when hindering malicious threat actors.
- Operation at this level often relies on collections of IOCs from various sources to be able to detect, alert, and deny their usage.
- The ability to block or detect entire TTPs can not only hinder malicious threat actors; it can also laser-focus responses or detection of malicious activity, allowing for rapid response.

### Leveraging IOCs with Suricata
#### Overview
- Suricata rules are used for creating and sharing network rules across multiple platforms and device vendors.
- The ubiquity of the Suricata format makes it a common choice for sharing network rules related to IOCs in threat intelligence reports with various databases maintained for emerging threats.
- Knowing how to read and write these rules can help analysts to perform their duties.

#### Usage
- Many vendors use Suricata rules for generating alerts or filtering traffic.
- A rule can indicate the intended action to be taken for a match, although some vendors’ products may ignore (or be configurable to ignore) an action and use it entirely for notification purposes only.
- The following actions are valid for a particular rule, according to Suricata’s documentation:
  - **alert**: Only generates an alert.
  - **pass**: Stops processing packet.
  - **drop**: Drops the packet; generates an alert.
  - **reject**: Rejects the packet, sends a (TCP) (RST) packet or an (ICMP) unreachable response to the source or sender depending on the type of packet.
  - **rejectsrc**: Same behavior as reject.
  - **rejectdst**: Same behavior as reject, but sends the response to the destination.
  - **rejectboth**: Same behavior as reject, but sends the response to both source and destination.
- These actions provide flexibility in the usage of rules, allowing any particular rule to be modified to either alert or filter with various behaviors for rejecting or dropping the packet.

#### Format
- Example 1

```
alert tcp $HOME_NET any -> $EXTERNAL_NET 6969 (msg:"ET P2P BitTorrent Announce"; flow: to_server,established; content:"/announce"; reference:url,bitconjurer.org/BitTorrent/protocol.html; reference:url,doc.emergingthreats.net/bin/view/Main/2000369; classtype:policy-violation; sid:2000369; rev:6; metadata:created_at 2010_07_30, updated_at 2010_07_30;)
```
- This rule matches (and by default, alerts) whenever a BitTorrent client connects to a server and announces its availability.
- The matching criteria for this rule can be broken down as follows:
  - **tcp**: Alerts on TCP packets rather than User Datagram Protocol (UDP) packets. Some application protocols, such as DNS, are also valid options.
  - **$HOME_NET any -> $EXTERNAL_NET 6969**: Matches a connection on the internal network portion, with any source port, connecting to any external network on port 6969.
  - **flow**: to_server,established;: Matches only on packets sent to the server, for TCP connections after the initial handshake.
  - **content:"/announce";**: Inspects the payload of this packet, looking for specific content (/announce, in this case).

- The particular artifact that is the potential IOC for this rule consists of a connection to a server on port 6969 and a payload being sent to the server containing the value /announce.
- The presence of matching network traffic is a very strong indicator that a BitTorrent client is in use.
- Although legitimate uses of BitTorrent exist, its unexpected existence in traffic when forbidden by usage policies indicates potentially unauthorized or unwanted usage of the network by a user.
- This rule also contains some meta information about this rule:
  - msg: Indicates what message this rule should display in the alert output.
  - reference: Flexible and varies from rule to rule, but some sort of reference relating to this rule.
    - In this example, BitTorrent’s documentation as well as a meta entry in the emergingthreats.net database for this rule.
  - classtype: This keyword gives some additional information about this rule by classifying it.
  - sid: A numeric Identifier (ID) that can uniquely identify this particular rule.
  - rev: Typically incremented whenever a newer version of a particular rule is created and distributed.
  - metadata: Freeform keyword that allows extra data to be associated with this rule; used in this case to give information about the dates associated with this particular rule.

- Example 2

```
alert http $HTTP_SERVERS any -> $EXTERNAL_NET any (msg:"ET ATTACK_RESPONSE MySQL error in HTTP response, possible SQL injection point"; flow:from_server,established; file_data; content:"Warning"; content:"mysql_"; fast_pattern; distance:0; threshold:type both,track by_src,count 1,seconds 60; classtype:web-application-attack; sid:2020507; rev:3; metadata:affected_product Web_Server_Applications, attack_target Web_Server, created_at 2015_02_24, deployment Datacenter, signature_severity Major, tag SQL_Injection, updated_at 2016_07_01;)
```
- The particular artifact that is the potential IOC for this rule consists of a (HTTP) connection in which the server sends content to the client that is consistent with the output of a MySQL error message.
- Error messages like this should never be sent to a client; the server might be misconfigured or poorly secured.
- These alerts might indicate that a malicious threat actor is actively testing for a Structured Query Language (SQL) injection vulnerability. 

- The new matching criteria are broken down as follows:
  - **file_data**: This keyword, valid for HTTP rules, indicates that the body should be examined.
  - **content:"Warning"; content:"mysql_";**: Multiple content values are allowed in a single rule.
  - **fast_pattern**;: Modifies the rule to have it match on the previous entry first. This can be used to optimize the Central Processing Unit (CPU) usage of a rule.
  - **distance:0;**: Indicates that this content should be found immediately following the previous content for this rule.
  - **threshold**:type both,track by_src,count 1,seconds 60;: The threshold keyword allows for a rule to alert only when enough triggering values are found within a specific timeframe.
    - With an alert threshold of 1, this alert triggers on any match; however, this rule can be tweaked to the needs of the network by changing this value.


## CTI-Based Hunts
### Threat Hunting Refresher
#### The Threat-Hunting Loop
- Threat hunting is one of the primary functions of CPTs.
- The Threat Hunting Loop phases, in order, are as follows:
1. Creating a hypothesis.
2. Investigating the idea.
3. Uncovering new patterns and Tactics, Techniques, and Procedures (TTP).
4. Informing and enriching analytics and detection capabilities.

- To summarize the lesson Vulnerability Based Hunts, vulnerability-based hunts use vulnerabilities identified within an organization to drive the hypothesis for planning an approach to hunting threats.
- Once an indicator has been found, analysts are able to inform and enrich an organization’s detection and defensive capabilities through detection signatures and intelligence.
- Similar to how vulnerability-based hunts use vulnerabilities, CTI-based hunts leverage such attributes as motivations of threat actors to drive the approach to hunting threats on a mission partner’s network.
- If an Advanced Persistent Threat (APT) is motivated to target the mission partner, intelligence can be used to leverage TTPs and Indicators of Compromise (IOC) attributed to that APT to uncover evidence of an attack.

#### Targeting Strategy
- The methodology of identifying areas within the mission partner’s network that are likely to be targeted is defined as a targeting strategy.
- Essentially, accounting for such factors as network inventory, Key Terrain in Cyberspace (KT-C), intelligence-related vulnerability reports, and threat assessments helps in deciding how to prioritize the effort spent conducting analysis during a hunt.
- Whereas vulnerability-based hunts more heavily emphasize vulnerabilities, CTI-based hunts rely more on intelligence to determine prioritization of analysis during a hunt.


### Exploitation Indicators
- Vulnerabilities comprise two main categories: those that can be exploited locally and those that can be exploited remotely.
- An analyst must know what to look for regarding both types of exploitation categories when conducting a hunt.

#### Local Exploits
- Local exploits, as the name suggests, are exploits that can be triggered only from within the system being exploited.
- One of the most common examples of a local exploit is the variety of privilege escalation attacks that target Operating System (OS) mechanisms.
- Indicators of local exploitation are likely observed from endpoint detection solutions and system logs.
- This means that indicators from network metadata are follow-on activities, such as Command-and-Control (C2) traffic, pivoting, reconnaissance, and exfiltration. 

#### Remote Exploits
- Remote exploitation is not to be confused with remote code execution, although remote code execution exploits provide the most obvious example of remote exploitation.
- Remote exploitation is one method APTs leverage to gain a foothold or move laterally.
- Many (**NIDS**) and (**IPS**) signatures focus on identifying this type of activity, as triggering these types of exploits requires some type of sequence or amount of data to be sent through a network in a predictable manner.
- Although network signatures represent the simplest method of detecting remote exploits, they should not be relied on, as signatures need to be tuned.
- Additionally, it is not feasible to have a signature for every exploit variant, and the signature databases must be kept up to date.
- On top of that, the appliances that use them must be implemented correctly.
- Finally, in many instances, signatures are incapable of inspecting a remote exploit’s payload as threat actors use encryption to obfuscate their activity. 

#### Combining Exploits
- One assumption analysts make is that vulnerabilities not labeled as critical or high are less likely to be harmful and, therefore, are less likely to be used by threat actors.
- Analysts should consider how the vulnerabilities uncovered on an operation are leveraged in combination to cumulatively pose a larger threat.
- This use of several “lesser” vulnerabilities in succession is more likely to occur when an adversary is acting against a mature environment. 

#### Observation of Exploitation Alerts
- Although many signatures are written to identify exploitation within an environment, finding evidence of direct exploitation is unlikely.
- Understanding how different types of exploits can be used to identify malicious activity not directly determined by a signature’s alerts is critical.
- Anticipating how successful exploits could be used to advance an attacker's goal is also critical.

### Open Source-Research
- Analysts should supplement their current understanding of any given situation with further research.
- Research regarding hunting commonly deals with TTPs, IOCs, past and currently trending vulnerabilities, and potential indicators of malicious activity. 

#### Publicly Available Exploits
- Vulnerability reports almost universally provide some type of reference to related Common Vulnerabilities and Exposures (CVE), which are a great starting point for searching for publicly available exploits.
- Both code and tutorials on how to exploit vulnerabilities are highly accessible with a simple query through any search engine, using such keywords as the specific CVE and exploit or tutorial.
- Some resources are more reputable than others; the Exploit Database is a good choice.
- The ability to review exploit source code is beneficial in identifying what data is being sent to trigger the exploit and providing insight into what an analyst should look for during a hunt.

#### The C2 Matrix
- Understanding the tools and techniques available to adversaries is critical to an analyst’s success.
- Adversaries use some form of a C2 framework to efficiently orchestrate their efforts.
- Although more advanced adversaries use customized frameworks for this, it is important to understand how these frameworks are designed.
- To accomplish this, the C2 Matrix is a useful tool.
- The C2 Matrix catalogs many C2 frameworks and summarizes their features in a matrix.

#### Security Product Vendors and Social Media
- Many larger security product vendors have blogs that provide a reputable source of information about current topics in cybersecurity, often with varying levels of technical detail.
- Additionally, the employees of these vendors may use social media platforms (for example, Twitter) to promote themselves and their companies by sharing information on their personal research in cybersecurity.
- It is beneficial for analysts to follow these sources of information to stay current with the constantly evolving battlefield of cyberspace.

### Anomaly-Based Detection
- The key to anomaly-based detection is identifying anything outside well-defined patterns.
- An observed baseline of network behavior, the defined rules of a protocol, inventoried devices, system and service uptime, and even documented problems are all well-defined patterns with regard to hunts.
- The following should be considered when using an anomaly-based detection strategy.

#### Traffic Volume
- The fluctuation of traffic volume normally falls within a predictable pattern in any given 24-hour period.
- Any deviation from that pattern is of interest and can indicate a range of concerns, including C2 traffic, reconnaissance, lateral movement, data staging for exfiltration, data exfiltration, or tool staging.
- Much of the context is dependent on such factors as whether an established baseline exists, how current that baseline is, network accountability, and perspective of the statistical information, such as ingress/egress or internal/external traffic.

#### Data Flow
- The direction of communication and context of data flow may be indicators of abnormal activity.
- Is a server acting as a client for a system that is identified as a workstation?
- Is peer-to-peer traffic standard or unusual?
- Abnormal flow of communications may indicate lateral movement, pivoting, Man-in-the-Middle (MitM) or spoofing attacks, or C2 chains.

#### Protocol Standards
- Protocol standards are documented through Requests for Comments (RFC). However, their implementation in practice is left to the application designer.
- Typically, application designers adhere to the standards defined for the protocol they are working with.
- However, threats have found that certain definitions within a given protocol may be ignored in order to carry control data within them.
- Common examples of this are C2 frameworks that use Hypertext Transfer Protocol Secure (HTTPS) request fields to transfer data.
- Additionally, malformed protocol data may be used to trigger remote exploits.

#### Endpoint Accountability
- Although servers within an organization’s environment are fairly easy to maintain inventory of, network accountability of workstations within an organization may be challenging.
- Many security vendors provide some type of solution that tracks systems and attempts to identify any systems unaccounted for.
- However, the effectiveness of those solutions may vary.
- An unaccounted-for system may be unpatched, provide an obsolete service, miss a mandated security solution, or even be a malicious device on the network.
- Effort must be taken to identify the purpose and ownership of any unaccounted system.

#### Change Management
- Any changes outside the change management chain should be questioned.
- Attackers may make any changes they deem necessary to better enable their success.
- These changes may actually include fixes to avoid unwanted attention of a key system, modifications of networking equipment to allow access to a network, log erasure, or service configuration changes to support such an effort as an MitM attack.

#### Errors and Crashes
- Service downtime and errors may be indications of a direct exploitation attempt, as many exploits leverage application flaws to create a preferred state of execution.
- The conditions that cause these desired flaws to be leveraged normally cause an application to crash or throw errors.
- If a device has a history of crashing or a short period of throwing errors unexpectedly, those events may be indicators of exploitation attempts.

### Synergies
- Although CTI-based threat hunting is the immediate focus of this lesson, the following are a few added benefits that apply to each role of a CPT.

#### Cyber Threat Emulation
- Being able to understand and emulate threats by leveraging such CTI-fed data as motivation, IOCs, and TTPs is a core function of Cyber Threat Emulation (CTE).

#### Threat Mitigation
- Discovery and Counter Infiltration (D&CI) produces signatures and additional intelligence from CTI and CTI-based hunts, which, in turn, can be applied to such mitigations as custom IPS signatures and configuration-based hardening.

#### Training
- Any threat hunt that attributes observed and new TTPs of threat actors can add value to future training by maintaining documentation of those TTPs.


### Applying CTI to Hunts
#### Cyber Threat Intelligence
- Maintaining realistic expectations and knowing what can be provided by the intelligence community increase mission effectiveness.
- Although CDAs in a CPT are not expected to perform intelligence analysis, they are the consumers of intelligence.
- Therefore, it is beneficial for those analysts to understand the different levels of intelligence and their significance at each level.
- By understanding and applying CTI, analysts are most effective in countering active and future threats on a network. 

##### Levels of Intelligence
- When dealing with consuming or collecting intelligence on a network, intelligence comes in different forms, depending on requirements given to an analyst.
- These forms of intelligence range from domains tied to a specific APT for use in an Intrusion Detection System (IDS) to a list of APTs assessed to have the capability and intent to target a mission partner’s network or even the political motivations of a threat group.
- Ultimately, intelligence can be broken down into three levels:
  - **strategic**
    - Strategic intelligence is associated with attacker motivations or politics, trends, or information that might inform policy changes at the organizational level.
    - Often, strategic intelligence is the trigger for the start of an operation or the change to network architecture.
  - **operational**
    - Operational intelligence includes such information as attacker TTPs and capability.
    - This intelligence informs collection plans and mission-planning efforts prior to an incident response or hunt engagement
  - **tactical**
    - Tactical intelligence is immediately applicable intelligence, such as IOCs, antivirus signatures, or IDS rules.
    - This intelligence is most useful for CDAs on a mission during on-network engagements.

#### Capabilities and Defensive Posture
- A key factor to success with any hunt mission is gaining situational awareness of the mission partner’s Operational Environment (OE).
- Part of the situational awareness includes the mission partner’s ability to feed network metadata into the CPT’s analysis framework as well as their overall detection capabilities and defensive posture in relation to threats identified by CTI.
- This process parallels the evaluation and analysis of the Area of Operations (AO) logical map and mission partner’s defense capabilities.

#### Collection Capabilities
- Depending on the situation (for example, limited storage capacity), it may be impossible to have all collection points feed into a Security Information and Event Manager (SIEM).
- During such scenarios, an analyst may need to separately review data from individual collection points to account for any gaps in their analysis.
- Having an understanding of how service data is processed in a network aids in accounting for these gaps.
- The key to an analyst’s success is being able to identify where these blind spots are and, if possible, how to compensate for them.

#### Detection Capabilities
- To identify whether malicious activity has occurred, a minimum amount of relevant data must be collected (where minimum depends on the malicious activity).
- If such data does not exist or cannot be collected, then compensating analysis must occur or a particular level of uncertainty must be accepted.
- A key task in hunting is being able to determine existing limitations in order to increase a conclusion's certainty to the extent possible.

#### Efficiencies
- In addition to identifying indicators of malicious activity within a mission partner’s network, an objective of CDAs should be to build efficiencies into processes.
- One notable example of this is developing signatures from queries that are used during a hunt.
- Typically, it is a trivial task to convert a search query for an IOC into a signature that can be shared both with the mission partner and with the security and intelligence communities at large.
- Another example is to build a dashboard around detecting several TTPs within the CPT’s analysis framework.
- The concept of computational efficiency should also be considered.
- This concept of computationally efficient queries has to do with the required overhead needed to perform a query.
- If a query used within a dashboard requires a high computational load, then the time required to process that task has the potential to hinder a dashboard’s resolution.

#### Defensive Posture and CTI
- In addition to identifying the mission partner’s collection and detection capabilities, an effort must be made to measure their overall defensive posture.
- Defensive posture includes such technical controls as **IPSs**, **patching**, and **anti-malware**; administrative controls like security training and usage policies; and other mechanisms, such as intelligence-driven operations. 

#### Establishing Monitoring Priorities with CTI
- Monitoring priorities should align with input from intelligence sources on threats to the mission partner and the capabilities of the mission partner.
- It would be counterproductive to plan on using capabilities the mission partner does not have or collecting network metadata without relevance to input from intelligence.
- When establishing which data sources to use, leverage what is available in the OE that accounts for the TTPs an APT uses.

#### Situation Analysis
- Part of the mission analysis phase in preparation for a CTI hunt mission is the situation analysis.
- A situation analysis takes into account the insight intelligence provides on APT TTP applications in relation to the AO.
- The objective of situation analysis is to describe potential adversary courses of action by accounting for critical systems, applications, and IOCs, and identify areas of interest.
- Areas of interest allow for prioritization of collection efforts on specific data, establishing alerts, and enabling automated responses. 

### CTI-Based Hunt
#### Preparation
- In any hunt, some time must be allotted for preparation.
- For CTI hunting, part of preparation includes defining a hypothesis around the intelligence received and integrating CTI into analysis tooling.
- Thus far in this lesson, trainees have prepared by using TheHive to receive an RFI response and apply the observables as a search filter.
- Trainees then cloned an existing dashboard to make a mission-specific dashboard with the search filter applied.
- The remaining step in the preparation process is to develop an initial hypothesis based on intelligence.

#### Conducting the Situation Analysis 
- In the RFI response, aside from the provided observables, it was mentioned that the APT is likely to use psexec, Distributed Component Object Model (DCOM), (WMI), and tainted share access for lateral movement.
- Additionally, the APT is most likely to gain initial access through phishing email attachments and attempt to gain access to data on file shares.
- The mission partner’s network is a Windows domain environment that does have shares in place, and they do use psexec for administrative tasks.
- All detection and data collection capabilities are limited to the packet collection provided through port mirroring through the mission partner’s infrastructure.
- All PCAPs have been fed into Security Onion and Arkime for analysis.

- Based on these factors, it can be concluded that if the APT gains access to the mission partner’s network through phishing, then the APT has some level of domain user access.
- With domain user access, the adversary can likely enumerate and access the file share and also laterally move.
- They are most likely to use psexec to laterally move due to the availability of the utility, as it already exists in the OE. 


# MOD 8
## Intro to Suricata
### Intrusion Detection Systems
- Information systems use IDSs as a mechanism to monitor for threats on a network.
- Similar to an alarm that sounds when an intruder is detected in a secure physical area, IDSs do not stop an intrusion, but they do seek to alert defenders when an intrusion is detected or suspected.
- **IDSs** are _hardware or software applications that monitor for malicious activity or policy violations based on specific criteria_.
- The criteria could be based on signatures or anomalies, depending on the configuration of the detection system.
- There are two classifications of IDS:
  - Host Intrusion Detection Systems (HIDS)
    - Host-based IDSs are software applications that are able to monitor for suspicious activity and malware on a host.
    - Typically, they monitor internal systems in addition to network traffic traversing the network interfaces of an endpoint.
    - Different from antivirus or Endpoint Detection and Response (EDR) agents, HIDS applications monitor and log actions rather than prevent the action altogether.
  - Network Intrusion Detection Systems (NIDS)
    - When searching for anomalies or signatures in network traffic, NIDS use a preset list of indicators or rules to log behavior to a Security Information and Event Management (SIEM).
    - Dozens of open-source solutions are on the market for NIDS, most notably Suricata and Snort®.

### Benefits and Drawbacks of IDSs
#### Benefits of NIDS
##### Flexible Deployments
- Intrusions can be detected in many ways.
- NIDS can be deployed almost anywhere that network traffic can be mirrored.
- This means that lateral, client-to-client traffic can be monitored as easily as traffic at an ingress/egress point of an enterprise network.
- It all depends on the capability of the network infrastructure.
- Typically, technologies like IPSs are deployed similarly to a firewall at security boundaries of a network.
- IDSs have the flexibility to sit inside a network between assets.

##### Custom Rulesets
- Given the standard ruleset and syntax of most IDSs available (including those included in the Joint Deployable Mission Support System [JDMSS] kit), rules are easily written to be tailored to a specific environment.
- This gives analysts the ability to write custom rules conforming to intelligence given to the team before a mission.
- It also gives analysts the option of writing rules on the fly as indicators are observed on the network during operations.

##### Protocol Awareness
- IDSs have the capability of detecting, inspecting, and dissecting most standard protocols on a network.
- This means that protocol headers and specific protocol information can be used in alerts.
- For example, Suricata can inspect Kerberos traffic in a Windows Active Directory (AD) environment to alert on specific errors or ticket requests.

##### Passive Nature
- Being placed on a network where traffic is only mirrored gives analysts the opportunity to monitor for a larger array of signatures or anomalies.
- As opposed to IPSs, they do not block any traffic.
- Therefore, rules can be tuned and modified without fear of blocking legitimate traffic from a larger network.
- Passive deployments give analysts freedom to be as general or as granular as a mission requires.

#### Drawbacks of NIDS
##### Inability to Prevent Attacks
- As the name suggests, NIDS software only detects intrusions; it does not prevent anything.
- Analysts are required to monitor logs and alerts to determine if anything needs further investigation or action.
- This results in the need for a robust automated response system or a workforce to manually parse logs.

##### Frequent False Positives
- The downside of being flexible and passive is that rules tend to be noisy.
- Tuning rules is a necessity for efficient use of an open-source ruleset or a default list of policy-based rules.
- Furthermore, an analyst must manually determine if a suspected false positive is, in fact, a false positive, which takes time and effort.

##### Frequent Signature Updating
- Rulesets of NIDS rules require frequent updating to keep up with the emerging malicious activity in the cyber world.
- This requires an internet connection, which might not be possible, depending on the system being used.
- Also, every round of updates likely causes the need for more tuning and the likelihood of false positives occurring.

### Suricata
- Since its creation in 2009 by the Open Information Security Foundation (OISF), Suricata has steadily climbed to become the cybersecurity community’s most popular IDS engine.
- Suricata is an open-source tool used in many hardware appliances and software suites that use rule syntax similar to Snort (the original IDS engine, currently developed by Cisco®).
- Suricata has many features outside the IDS engine that can be leveraged for security operations by a CDA.
- Configuration of all the features is out of scope of this lesson, but the functionality of each feature is described below.

#### Intrusion Detection System Features
- The IDS features of Suricata are extensive. Given its standard rule syntax and easy installation, Suricata can be implemented on any network with the ability to mirror network traffic to a monitoring server.
- Additionally, the logging format is universal, meaning that it can be sent to any log collector that accepts messages formatted in JavaScript Object Notation (JSON) (for example, Elasticsearch or Splunk).

- As illustrated later in this lesson, the IDS rule format allows for flexible monitoring and alerting.
- Multiple protocols can be dissected and advanced searching can be implemented through the use of hexadecimal characters and regular expressions.

- Not only are the rules easy to create, but a number of open-source and subscription rulesets are also available.
- The most popular ruleset available is the Emerging Threats Open ruleset curated and distributed by Proofpoint®, which provides thousands of rules covering multiple categories and severity levels.

#### Intrusion Prevention System Features
- The Intrusion Prevention System (IPS) features of Suricata are similar to the IDS features.
- The IPS functionality uses the same rule format, protocol dissection, and logging interoperability.
- The greatest difference is in its deployment scenarios and keywords in the rulesets.
- Unlike an IDS, an IPS needs to be deployed inline, meaning that network traffic must flow through the IPS as opposed to being mirrored.
- This requires a hardware device that can sit between senders and receivers on a network.
- Many vendors use Suricata to determine what network traffic is allowed or blocked as a part of their IPS functionality.
- Typically, when Suricata is deployed in an IDS capacity, there is only one action that each rule takes when traffic matches a rule: alert.
- However, when deployed as an IPS, the rest of the available action keywords are as follows:
  - **alert**: Only logs an alert signifying that a rule was matched.
  - **pass**: Passes the traffic without further inspection.
  - **drop**: Drops the matching traffic and logs an alert.
  - **reject**: Drops the traffic and sends an unreachable error message to the sender.
  - **rejectsrc**, **rejectdst**, and **rejectboth**: Do the same as reject, but send the error message to the source, destination, or both.

#### Network Security Monitoring
- Past just the IDS/IPS use cases, Suricata can also perform logging of traffic characteristics based on protocol observed.
- Nineteen total protocol dissectors are built into Suricata.
- The following are examples of the dissectors useful for Network Security Monitoring (NSM):
  - Hypertext Transfer Protocol (HTTP): Inspects HTTP sessions and extracts header field information, such as hostname, content-type, method, user-agent, and cookie.
  - Domain Name System (DNS): Logs DNS queries, answers, types of query, and other information useful for monitoring DNS on a network.
  - Transport Layer Security (TLS): Monitors TLS session for information that can be seen in cleartext and logs it. For example, hashes of certificates along with issuer and certificate chain are logged.
- In addition, network flows can be logged similar to other popular NSM engines like Zeek.
- Suricata logs the beginning and end of sessions, along with source and destination IP addresses and ports.

### Suricata Rule Anatomy
- Every Suricata rule follows the same structure.
- Some Suricata rules are longer and more complex than others, depending on the signature or criteria they are attempting to find.
- There are virtually endless possibilities for matching on traffic and protocols flowing through a network.
- The following is an example of a simple Suricata rule from the Emerging Threats Open ruleset:
  ```
  alert dns $HOME_NET any -> any any (msg:"ET DNS DNS Query for Suspicious .co.be Domain"; dns_query; content:".co.be";
  fast_pattern; nocase; isdataat:!1,relative; classtype:bad-unknown; sid:2013124; rev:5;)
  ```
- As illustrated in the example, Suricata’s rule format and language are human readable.
- This rule does the following:
  - The alert keyword at the beginning signifies that the rule only creates a log of the event and takes no action on the traffic itself.
  - It only analyzes DNS traffic, as seen early in the rule.
  - $HOME_NET any -> any shows that traffic direction is from the home (or internal) network to any network.
  - The remainder of the rule becomes more difficult to decipher.
- In summary, it matches on any DNS query that includes, but does not begin with, .co.be, without regard for case of the letters.

- Each rule is broken into three distinct sections — **Action**, **Header**, and **Options** — that are described below.
- These sections have their own standards for structure and syntax that must be followed exactly. Otherwise, the rule does not work, and Suricata does not process any matches.

### Action
- Every rule begins with an action.
- The actions were described earlier in the lesson but are repeated here for clarity:
  - alert: Only logs an alert signifying that a rule was matched.
  - pass: Passes the traffic without further inspection.
  - drop: Drops the matching traffic and logs an alert.
  - reject: Drops the traffic and sends an unreachable error message to the sender.
  - rejectsrc, rejectdst, and rejectboth: Do the same as reject, but send the error message to the source, destination, or both.

- The action determines what happens to the matching traffic when detected by Suricata.
- Because the most common use case for a Cyber Protection Team (CPT) for Suricata is IDS services, alert is the best action to focus on.
- Most importantly, the action must be all lowercase and be at the beginning of the rule.
- Of the three Suricata rule sections, the action is the simplest.
- It is always one word, and only a typographical error can cause any real problems.

### Header
- The next section, header, defines **protocol**, **source**, **destination**, and **direction** of matching traffic.
- It must be written in a **specific order**, and, unlike the action section, the header has some **specific syntax rules** that must be followed.

#### Protocol
- The second word in every rule is always the protocol that it is written to match.
- **It is a required item**.
- Twenty-seven options exist in the current version of Suricata, and each protocol has its own corresponding options that come later in the rule.
- The most common protocols used are as follows:
  - tcp (Transmission Control Protocol)
  - udp (User Datagram Protocol)
  - ip (Internet Protocol)
  - http (Hypertext Transfer Protocol)
  - dns (Domain Name System)
  - tls (Transport Layer Security)

#### Source and Destination
- Every rule requires a source and destination, even if the rule should alert on any traffic.
- Additionally, the source and destination both require an IP address (or addresses) and a port.
- The typical syntax is as follows:
  - `<SOURCE-IP> <SOURCE-PORT> -> <DESTINATION-IP> <DESTINATION-PORT>`
- Each item in the source and destination section of the header is separated by a space.
- Such shortcuts as Classless Inter-Domain Routing (CIDR) notation are accepted.
- Operators may be added to source and destination fields that help group and add efficiencies to the rules
  <img width="1667" height="459" alt="16a1e9d3-4eee-4ffd-9c4f-1ec52ff33dc2" src="https://github.com/user-attachments/assets/bd98581b-bf80-4c7d-898a-0e8394e336a0" />

- Suricata uses variables often in the source and destination fields that can also be paired with the above operators.
- These variables are defined in the configuration file called suricata.yaml, usually found in the /etc/suricata/ directory.
- The most common variables are **HOME_NET** and **EXTERNAL_NET**.
- By default, **HOME_NET** is set to **all private IP address ranges** and represents the **inside of a network**.
- **EXTERNAL_NET** should always be set to the negation (that is, opposite) of HOME_NET, resulting in **all IPs not included in the internal network**.
- If EXTERNAL_NET is not set to the negation of HOME_NET, some IP addresses may be excluded from inspection.

#### Direction
- Every rule must define in which direction the traffic must flow to match.
- Only two valid directions may be defined in a rule: left to right, and both directions.
- The proper syntax for signifying left-to-right traffic flow is a hyphen followed by a greater-than symbol: **->**.
- Using a less-than (<) symbol instead of a hyphen signifies bidirectional traffic flow: <>.

- Every section of the header is required.
- The example rule uses variables for source and destination IP addresses, but a rule may also use actual IP addresses or the term any.
- In the source port field, any is signified, and a list of ports is specified for the destination (ports 80 and 8080).
- Direction flow is left to right. The example is representative of many common prewritten rules in available rulesets.


### Options
- The Options section defines what Suricata looks for in the network traffic after the header matches its source and destination.
- Few required fields exist in the Options section, but a strict syntax must be followed when writing rule criteria.
- Some requirements when writing options are as follows:
  - The Options section must be surrounded by parentheses.
  - Each option (including the last option) must be trailed by a semicolon.
  - Every rule must have a Signature ID (sid) option.
    - This is usually the last or second-to-last option defined by convention (not by rule).
  - Some options have settings that require a colon separating the option name and the settings it requires.
    - The settings also must be wrapped in quotes if there are spaces in the text.

#### Meta Keywords
- Meta keywords have no effect on the behavior of Suricata when matching traffic, but they define how Suricata reports and tracks each rule. The most common meta keywords are described below.

##### msg (Message)
- For all practical purposes, the msg field is the **name of the rule**.
- It should give some contextual information to alert the analyst as to which traffic was matched.
- This is a free-form option, but as the rules for options state, this requires a colon and the settings wrapped in quotes if there are spaces in the text.

##### sid (Signature ID)
- Every rule requires a sid. This is a locally significant, but arbitrary, number.
- The option requires a number to identify each rule (or signature) in order for Suricata to internally keep everything organized.
- If two rules have the same sid specified, Suricata does not process them and has unpredictable behavior throughout the entire ruleset.
- It is conventional for this to be the last option (or second to last if a revision [rev] number is specified).
- The Options section still ends with a semicolon, even though no option follows the sid field.
- Analysts can also specify a revision number by using the rev option and the same format. This is omitted from the example in the interest of brevity.

#### Payload Keywords
- Payload keywords appear in almost every Suricata rule.
- The following are a few keywords that an analyst should be able to identify and properly implement in a signature.
- They identify what Suricata is searching for inside network traffic.

##### content
- The content keyword is the most common keyword outside the meta keywords described previously.
- This keyword defines the data that Suricata looks for in network traffic to match the rule.
- It is added to the rule in a similar fashion as the msg and sid keywords.
- The example (as described in the msg option) is looking for a cURL HTTP user agent.
- Thus, the content option signifies curl as a string to look for.
- cURL (Client Uniform Resource Locator) is a command line utility for requesting contents of hosted website data.

##### nocase
- The option nocase is inserted after the content keyword to signify that Suricata should match on all cases of letters in the content.
- For example, nocase after content:cURL would match on curl, CURL, or cUrL (among other combinations).

##### startswith/endswith
- Suricata gives the ability to search for content at the beginning or end of a piece of network traffic data.
- The startswith and endswith keywords do exactly that.
- For example, if an analyst were searching for a DNS request for the .biz top-level domain, using the endswith keyword would ensure that it matches only on that top-level domain and not another domain name with .biz in another portion of the domain.

- The ongoing example does not include startswith or endswith, but the use of these keywords is common.
- Other payload keyword examples are **depth**, **offset**, **distance**, **isdataat**, and **pcre** (Perl Compatible Regular Expressions). 

#### Modifier Keywords
- Each protocol has an abundance of its own keywords used to narrow what Suricata is looking for in the network traffic.
- For example, there are TLS keywords that tell Suricata to only look for a certain certificate or certificate serial number.
- The example uses HTTP keywords because the user agent field is a part of an HTTP header in web traffic.

- There are two types of modifiers: **content modifiers** and **sticky buffer modifiers**.
- Unfortunately, there is no way to tell if the modifier used is a content modifier or sticky buffer other than to reference the Suricata documentation.

##### Content Modifiers
- Of the two types of modifiers, content modifiers are the older style.
- They look “**backward**” in the rule to create a match.
- An example of a content modifier is http_uri, which looks for the Universal Resource Identifier (URI) of an HTTP request. Using that modifier, it is written after the content option.

##### Sticky Buffer Modifiers
- Contrary to how content modifiers look backward in the rule, sticky buffer modifiers go forward.
- For example, http_response_line is a sticky buffer keyword that looks for HTTP responses from the web server.

## Introduction to NetFlow
### NetFlow Introduction
#### Packet Capture
- Packet Capture (PCAP) is a method of capturing traffic in a computer network and then analyzing the contents, or packets.
- This is critically important for CDAs, as it provides oversight to a network’s operations.
- PCAPs have a wide range of uses, such as building audit trails, monitoring bandwidth, and tracking devices.
- Additionally, PCAPs are an essential tool in maintaining network security.
- For example, PCAP data can be used to track malware activity, study past attacks, and define network-based Tactics, Techniques, and Procedures (TTP). 

- With the vast advantages and capabilities provided by PCAP data, an important fault exists that is inherent to the format and depth of the data: **size**.
- Full packet capture files grow quickly as information is captured by a sensor or interface.
- Although the number of packets captured depends on the activity level of the network, PCAP files can grow from kilobytes (KB) to megabytes (MB) to gigabytes (GB) and beyond very quickly.
- When storage capacities are limited, the sizes of these files can become a problem.
- Although wiping PCAP logs is a solution, losing potential past forensic data is a risk.
- Additionally, examining massive quantities of PCAP data is time consuming for analysts.
- To navigate these challenges, a method of capturing packets from a high level that requires less storage space and analysis time is needed.

#### NetFlow
- Created by Cisco in 1996, NetFlow is a feature that, like PCAP, provides the capability of capturing IP traffic that is entering or exiting a routed interface on a router or switch.
- However, unlike PCAP, NetFlow records only the high-level data, such as IP addresses, destination and source ports, and timestamps.
- This allows an analyst to quickly comb through large amounts of captured information as well as view traffic patterns and anomalies.
- Because NetFlow data does not contain the contents of each packet, the storage demand is substantially smaller than PCAP data. 

- NetFlow uses session data, which is a high-level, low-detail form of information.
- Specifically, it uses the 5-tuple, which comprises the following:
1. Source Internet Protocol (IP) address 
2. Source port number
3. Destination IP address
4. Destination port number
5. Protocol in use
- These five attributes are derived from the header of the IP packet.
- In addition to these, the identifier (ID) of the input interface and the type of service (ToS) byte are included.
- Together, these seven attributes form a flow. 

- But how does NetFlow gather this data?
- In a three-component system, data is collected, transported, and analyzed.
- These components are as follows:
  - **NetFlow Exporter**: Not a dedicated physical device. Rather, it is a router, switch, or firewall, for example. This device has an interface (or interfaces) configured to gather packets into flows. Once gathered, the flows and flow records are sent to the NetFlow Collector.
  - **NetFlow Collector**: A system that receives the flows from the Exporter and then stores and processes the flow data.
  - **NetFlow Analyzer**: A software application that provides the statistics and insights into the flow data. 
  <img width="1999" height="1630" alt="e7f0023d-418d-4479-83b3-9a7ff745d99e" src="https://github.com/user-attachments/assets/bf367480-b060-417a-863a-4e87a082b35f" />

- Using NetFlow allows for greater visibility into a network’s traffic.
- Whereas using full packet captures or NetFlow can be seen as an either/or situation, many industry experts agree that combining the two to properly utilize their advantages is the best solution.

### Using NetFlow to Analyze Traffic
1. From the terminal, cd into the netflow directory
2. run the following command to view a summary of a file: `nfdump -r netflow_1 -I`
  - `-r` specifies the file nfdump will be receiving input from
  - `-I` prints the summary statistics.
   <img width="522" height="431" alt="ebddcf71-b070-4f7a-80d3-81535e38fe88" src="https://github.com/user-attachments/assets/85d85051-6d1d-4b00-acd2-f44bebc40bc8" />

3. Given the summary, run the following to view top 10 IP addresses that sent highest number of packets: `nfdump -r netflow_1 -s ip/packets -n 10`
  - `-s` prints statistics for a specified record type
  - `-n` prints the number of records that are needed.
  <img width="1148" height="345" alt="59586695-f4c1-4617-9c48-4db695d1fd85" src="https://github.com/user-attachments/assets/d24126e6-b439-4b7f-ae49-64a9f43f502d" />

4. Run the following to view port information: `nfdump -r netflow_1 -s dstport/packets -n 10`
  <img width="1149" height="357" alt="5acc55ba-2e2d-4311-a629-8a6f30a09f18" src="https://github.com/user-attachments/assets/1b867306-1ccf-4a92-8730-21d0c6ad8a26" />

5. Run the following to view IP addresses that sent most HTTPS traffic: `nfdump -r netflow_1 -s srcip/packets 'port 443'`
   <img width="1152" height="283" alt="40536caa-fe2a-4378-b9d8-f8b18bcacce7" src="https://github.com/user-attachments/assets/f13563da-4685-455d-8c47-0ff73b140e49" />

6. Run the following to view IP addresses receiving most IP traffic: `nfdump -rnetflow_1 -s dstip/packets 'port 443'`


### Netflow Versions
<img width="1999" height="1472" alt="88ae65c1-6cf2-4dc9-a201-df3abbb643f7" src="https://github.com/user-attachments/assets/0dfef5f3-050f-4184-baf7-8c026f043ea0" />

#### Netflow v5 vs. NetFlow v9
- The two most commonly used versions of NetFlow are v5 and v9.
- Despite being much older and less capable than v9, NetFlow v5 is significantly more popular.
- This largely is due to the fact that **more devices support v5 than v9**.
- A reality of technology adoption is that many people and organizations continue to use what works (such as NetFlow v5) and refrain from “unnecessary” changes and hassle.
- Additionally, the process of changing to a newer version could create a gap in traffic collection, leading to temporarily reduced network visibility.
- Because of this, NetFlow v5 lives on and is only slowly being replaced by v9. 
  <img width="1999" height="1472" alt="e5a12901-2477-43e8-b346-ff1d4538fbea" src="https://github.com/user-attachments/assets/58db03b2-ec8c-40c3-8441-f2e9c18a901f" />

#### IPFIX and NetFlow 
- **IPFIX** (IP Flow Information eXport), as its name suggests, is a non-Cisco standard for exporting IP flow information.
- **IPFIX** is commonly mistaken for being just NetFlow v10.
- This is primarily due to the fact that IPFIX is directly based on NetFlow v9, supporting backwards compatibility for v9 traffic, and was created by several individuals who worked on v9.
- However, the differentiating factor that separates IPFIX from NetFlow is flexibility.
- **IPFIX** is **compatible with more devices and vendors**, allowing it to exist in more networks.
- Additionally, **IPFIX** (with its backwards compatibility) supports **up to 238 data field types**, whereas **NetFlow v9 recognizes only 79**.
- However, a side effect of IPFIX’s flexibility is a **lack of specialization toward a particular type of device or traffic**.
- Despite this, IPFIX remains a robust and adaptable solution to analyzing network flows. 

##### sFlow
- Standard Flow (sFlow) is a **packet-sampling technology**.
- Whereas NetFlow does not capture full packets, **sFlow copies and samples packets with a user-defined frequency**.
- By sampling packets, resource utilization is reduced.
- Additionally, sFlow packets contain more information than NetFlow data due to the inclusion of partial or full packet details.
- Finally, sFlow packets are sampled and exported in real time, whereas NetFlow must cache its data before exporting it to be processed.
- A large downside of sFlow is its accuracy, as packets may be missed between samples. 

## Network Traffic Visualization
### Visualizing Network Traffic
- Network visualization is the process of visually presenting networks via logs and dashboards, such as histograms and line graphs.
- To understand network issues, Network Analysts examine data at the packet level.

- Network visualizations assist analysts with displaying data that cannot be easily comprehended by analyzing raw data.
- A Network Analyst must know what connections are being made and traversing a network environment in order to form a full picture of the defended terrain.
- Although not a standalone Cyber Protection Team (CPT) function, a prerequisite to successful CPT missions is the validation and verification of the mission partner’s documentation to ensure an accurate common operating picture.
- During this step, CPTs analyze the supported organization’s network diagrams, terrain information, and configuration documents.
- However, these documents are not always present to close information gaps.
- Using tools to create network visualizations is imperative to a successful CPT operation.

### Anomalies
- PCAPs contain network data that allow the CDA to perform network forensics and deep packet inspection.
- Problems within PCAPs can be detected as traffic anomalies.
- Anomalies generate alerts by such detection tools as Intrusion Detection Systems (IDS) or Security Information and Event Management (SIEM) servers and can be visually represented.
- For example, an anomaly may be a system crash, malformed packets, unusual contact with an unknown host, or a large amount of data being transferred over a short period of time.
- The following are some ways to classify anomalies:
  - **Non-human error**: Interrupted or abnormal communications due to equipment failure.
  - **Human error**: Interrupted or abnormal communications due to misconfigurations or mistakes.
  - **Malicious Cyber Activity (MCA)**: Network activity infiltrated and attacked by an adversary. Networks can also be victims of insider attacks.  

#### False Positives
- For Network Analysts, distinguishing between normal operation and suspicious activity in which an anomaly is present may be challenging.
- Network detection mechanisms may be used to find Indicators of Compromise (IOC) or anomalies within a network.
- A network-based IOC is information that can be captured on the network and may objectively describe an intrusion of a network.
- The following are some common network-based indicators:
  - IPv4 address
  - IPv6 address
  - X509 certificate hash
  - Domain name
  - Text string
  - Communication protocol
  - File name
  - Uniform Resource Locator (URL)

- These network indicators can be seen at the host level as well; however, they are classified based on where they are initially found within a network.
- Classifying indicators as either host or network assists with operational approach and analytical scheme.
- Because host-based indicators are not in the role or expertise of a Network Analyst, it is imperative that Network Analysts seek assistance from Host Analysts within the team to conduct host-level investigations.   
 

### Creating Visualizations Using WireShark
- Wireshark captures – or sniffs – the network to view traffic that goes through a network adapter.
- Network data streams are sent to specific addresses but can be seen by using a packet sniffer on a network interface that is in promiscuous mode, which informs the (NIC) to accept all traffic it sees on the network.
- **Libpcap** and **WinPcap** are the **kernel-level drivers** that enable sniffing. 

- Wireshark provides analysts with an assortment of options to evaluate network performance graphically based on multiple variables.
- Wireshark is extremely valuable; however, it is not the best tool to understand what is actually traversing the network at a higher level

### Creating Visualizations Using Arkime
- Arkime, formerly known as Moloch, exposes APIs and augments security infrastructure to store and index network traffic.
- The network traffic is captured in standard PCAP format and cataloged in an Elasticsearch database, which provides fast indexed access.
- Because Arkime stores and exports all packets in standard PCAP format, such PCAP-ingesting tools as Wireshark can be used with Arkime to conduct traffic analysis.
- Arkime can ingest the PCAP data and visually display it in different graph formats, such as histograms or line graphs, to easily parse through network data visually to detect outliers within a network environment. 

- PCAP retention is based on available sensor disk space within the network environment.
- Metadata retention is based on the Elasticsearch cluster scale, but these settings can be altered. 

- Arkime consists of three components:
  - **Network Traffic Capture System**: Monitors network traffic, writes PCAP files to disk (permitted storage), parses data, and sends metadata to Elasticsearch.
  - **Viewer**: Transfers PCAP files through Elasticsearch metadata.
  - **Elasticsearch**: Provides statistics and network visualizations of network data from PCAP. 

- Arkime and Security Onion sensors can be configured on a network to have overlapping coverage of network traffic.
- This allows the CDA to pivot to Security Onion, where the information can be gleaned from Arkime analysis. 

<img width="1999" height="1192" alt="5aeddb6d-0c04-4917-95af-d5e14ae56a50" src="https://github.com/user-attachments/assets/eb016c4b-c6f6-41e9-af07-78f3ed2683f4" />


### Creating Visualizations Using Kibana
- Kibana is a search portal and data visualization tool in the Elastic Stack.
- When a network is configured with Elasticsearch and Logstash, Kibana can index log data from network endpoints and aggregate it for search and visualization.
- Kibana can function as a SIEM by correlating data across multiple sources to provide insights about user activity in a network.

- One of Kibana’s most powerful features is the ability to deploy visualizations to help quickly identify anomalous or malicious behavior.

- On production networks, there are often too many logs to parse manually.
- Visualizations allow users to analyze a large amount of data simultaneously and identify patterns that would not have been evident using individual searches.


# MOD 9
## Packet Capture Tools
### Capturing and Processing Network Data
- The most important hardware component for monitoring network traffic is the sensor.
- A sensor is a combination of hardware and companion software used to perform collection of network traffic. 
- This lesson compares the tools **Stenographer**, **Arkime**, **Wireshark**, and **Tcpdump** included in the JDMSS kit.
- Although all the tools mentioned provide Full Packet Capture (FPC) functionality, each tool has key, distinctive features that warrant their use, depending on mission requirements. 

### Environmental Impact of Enabling PCAP
- Packet Captures (PCAP) provide a copy of network data, potentially providing insight into Malicious Cyber Activity (MCA) on a network.
- Newer hardware may automate certain investigation processes, with larger datasets stored and indexed. 
- In hindsight, teams often filter large quantities of data to extract the same streams of network data while not necessarily needing the payload itself.
- There are several disadvantages to having FPC enabled within the network environment, including the following:
  - **PCAPs are voluminous, requiring significant storage space.**
  - **The storage space for PCAPs can be costly.**
  - **PCAPs can be difficult to store, manage, and index.**

- FPC data can cause the sensor to exhaust available hard drive space or malfunction if data retention is not handled properly, as FPC data requires more disk space than other traffic capturing options.
- For example, the FPC data may try to write to the disk more quickly than it can delete, which may cause sensors to malfunction.
- To mitigate this, a CDA should create a separate noncritical partition or system drive.

### Navigating Pitfalls
- A pitfall is anything that results in an unexpected negative outcome within a network environment.
- Several tips and best practices can help CDAs to navigate and avoid common pitfalls. 

#### Use Proper Resources
- The document that usually provides CDAs with the most insight into the overall design of a mission partner’s network environment is the network map.
- Concurrent with CPT Mission Analysis (MA), CPTs must employ passive and active measures to generate current, accurate information regarding MCA and (MRT-C) to produce a logical network map and close information gaps.
- The network map forms the basis of the CPT’s visualization tools and must scale appropriately to the situation.
- As critical input, the network map forms the foundation on which courses of action are developed based on the operational approach and analytic scheme of maneuver employed by the CPT. 

#### Network Ingress/Egress Points
- When the appropriate resources are available, a sensor should be placed at each distinctive ingress/egress point in the network.
- Ultimately, any MCA occurring on a mission partner’s network involves data being communicated into or out of the network environment.
- Sensors attached to these ingress/egress points capture this data. 

#### Visibility of Internal Internet Protocol (IP) Addresses
- When performing detection and analysis, it is critical to determine which sensor is the subject of an alert.
- If sensors are placed in the wrong spot within the network environment, these alerts may be undetected. 

#### Proximity to Critical Assets
- Over-collecting network data can be problematic.
- This should be taken into account when placing sensors within a network environment.
- Driven by operations and intelligence, CPT functions focus on enabling the security of critical assets and MRT-C critical asset function.
- **MRT-C** is defined as, but not limited to, all **devices**, **internal or external links**, Operating Systems (**OS**), **services**, **applications**, **ports**, **protocols**, **hardware**, and **software** on servers **required to enable the function of a critical asset**. 

#### Securing Sensors
- Analysts need to ensure the integrity of the data used for securing critical assets.
- FPC data files contain sensitive information pertaining to the network environment.
- An adversary could leverage this data to expand their foothold within the network environment.
- Key steps to keep sensors intact include the following:
  - Application of the latest updates and security patches.
  - OS hardening.
  - No internet access to sensors.
  - Virtual Local Area Network (VLAN) segmentation.
  - Network/host-based Intrusion Detection Systems (IDS).
  - Two-factor authentication (2FA).


### Specific Requirements
- When selecting a PCAP tool for a mission, requirements for using a specific tool vary greatly from site to site.
- This is because specific system and network requirements depend on the **total bandwidth being monitored**, the **type of network traffic**, **rule sets**, and **policies loaded**.
- Depending on the size and threats faced by a network, sensors may have varying roles within the phases of the collection process.
- The concepts presented help strengthen the decision-making that goes into defining collection requirements. 

#### Characteristics
- The right FPC tool is needed to operate in environments that may contain extremely high sustained network traffic rates.
- Without scaling collection tools to meet throughput requirements within the network environment, Central Processing Unit (CPU) cycles are wasted and incomplete data is gathered. 
- The **most efficient capture tool** is one that **drops the smallest number of packets on the sensor** and **contains enough features to ensure that the network data is accessible and stored**. 

- Every tool has specific requirements needed to properly function and collect data as a standalone sensor within a network environment.
- The FPC tools discussed in this lesson typically have the following characteristics but vary due to traffic rates and data retention:
  - **Large amounts of hard disk space** in a Redundant Array of Independent Disks (**RAID**) configuration for storing network traffic and associated data.
  - A **minimum of 4GB of RAM**, with **at least 1 GB extra for every interface connected** to a Switched Port Analyzer (**SPAN**) port or **network tap**.
  - **One CPU core per interface**.
  - **Multiple network interfaces**, with the **appropriate number and media type required by the SPAN ports** or network taps. 

#### Collection-Only Sensors
- A collection-only sensor **logs collected FPC data and session data to disk** and sometimes generates other data, such as statistical and Packet String (**PSTR**) data based on the collection.
- A collection-only sensor is **barebones**, with **no extra software**.
- Analysis is done separately from the sensor as relevant data is pulled to other devices as needed.

<img width="1999" height="591" alt="fd3afa5d-4d2a-46d5-86a6-54657c78bb9d" src="https://github.com/user-attachments/assets/acfc8a0f-1c72-4c61-9a74-00b1d0b91535" />

#### Half-Cycle Sensors
- A half-cycle sensor is the **most common type of sensor** deployment and performs **detection tasks**.
- For example, a half-cycle sensor **logs PCAP data to disk** but also **runs a NIDS**, either in **real time from the NIC** or in **near-real time against the PCAP written to disk**.
- **Data is pulled back to another device** when analysis is done rather than being performed on the sensor itself. 

<img width="1999" height="555" alt="cd931dd4-7a2f-4d86-804a-f3ffea7bbea9" src="https://github.com/user-attachments/assets/28fb25e7-dab5-4efd-9f98-0f174f022172" />


#### Full-Cycle Sensors
- A full-cycle sensor implements a **full suite** in which **collection**, **detection**, and **analysis** are **all performed on the sensor**.
- Examples include **profiles**, a **User Interface (UI)**, and the **installation of a NIDS UI**.
- A full-detection sensor **performs all the tasks on the sensor** rather than on another device. 

<img width="1999" height="555" alt="bd51923e-6b50-4a87-9916-73ad64030842" src="https://github.com/user-attachments/assets/921e518b-e9ba-4cfb-ba8c-e9e702a283c3" />

### Specific Limitations
- Every tool capable of FPC may be limited due to availability of **compute**, **memory**, or **storage resources**.
- Sensors’ resource limits can be calculated to ensure they meet the minimum requirements to accomplish the mission.

#### CPU
- The amount of CPU resources required to efficiently run an FPC tool depends on the type of sensor being deployed.
- A **collection-only sensor** **does not require a significant amount of processing power**, as the tasks are not processor intensive.
- **Detection** and **analysis** functionality **pulls the most CPU resources**, so **half-cycle and full-cycle sensors require additional CPU cores**.
- The **cores should be mapped to the tools** being deployed in the network environment **before deployment occurs**.  

#### Memory 
- **Collection of network traffic requires a smaller amount of memory** allocated than analyzing network traffic.
- A large amount of memory allocated to FPC tools improves their performance under larger data loads.
- **Stenographer**, however, is a **less resource-intensive tool** and **benefits CDAs in a resource-constrained environment**. 

#### Hard Disk Storage
- No two networks are the same, and proper storage must be allocated for full content stored in PCAP format as well as logs stored in databases such as Arkime.
- Berkeley Packet Filters (**BPF**) can also be **applied to sensors to filter out unwanted data** and **minimize compute resources**. 
- **NOTE**: Networks grow over time, and storage may have to be reevaluated at some point to adhere to a feasible retention period for each data type. 
- To roughly estimate full content data requirements, the formula below can be used to determine the storage needed to store FPC data:
  - **Hard drive storage for one day = Average network utilization (in megabits/second) ✕ 1 byte/8 bits x 60 seconds/minute ✕ 60 minutes/hour ✕ 24 hours/day**
- Hard drive space used by databases may be calculated as follows:
  - **Hard drive storage for databases = 1/10 ✕ full content storage size** 

#### Summary
- Quantifying the differences in the use of various tools can be helpful, especially when attempting to determine specific requirements to avoid limitations.
- However, requirements and limitations vary drastically based upon the network environment, as no two networks are the same.


### Stenographer
- Stenographer is a lightweight FPC utility consisting of several processes for buffering packets to disk for intrusion detection and incident response purposes.
- Stenographer provides a high-performance implementation of NIC-to-disk packet writing, handles deleting those files as the disk fills up, and provides methods for reading back specific sets of packets quickly and easily.
- Stenographer is integrated into Security Onion to essentially function as a sniffer on the sensor.

#### Why a Stemographer?
- Stenographer consists of a **stenographer server**, which **serves user requests** and **manages disk storage.**
- A **stenotype child process** is used to **sniff packet data** and **write the data to disk**, communicating with stenographer by unhiding files when they are read. 
- The user scripts **stenocurl** and **stenoread** provide **simple wrappers around curl** that allow analysts to **request packet data from the stenographer server with ease**.
- Once a set of packet positions is computed for each index file, the user scripts seek the data, read the packets out, and merge them into a single PCAP file.
- The resulting PCAP file is returned via stenocurl as a stream to standard output (**STDOUT**).
- **stenoread** passes the PCAP through **Tcpdump** in order to allow for additional filtering, writing to disk, and printing to readable format.

#### Pros and Cons of Stemographer
<img width="684" height="275" alt="image" src="https://github.com/user-attachments/assets/b343901e-c96c-4372-be4e-0aa08cf1a195" />

#### Use Cases and Limitations
- The design of Stenographer makes it operate poorly with performance characteristics in network environments under certain circumstances.
- Limitations to using the FPC tool include the following:
  - Large PCAPs (above 4 GB) are not efficiently supported.
  - Packets do not show up immediately.
  - Packets are stored in 1-megabyte (MB) blocks.
  - Stenographer flushes one block of data every 10 seconds. 

#### Summary
- Stenographer is a lightweight FPC utility comprising several processes.
- It is best used in situations in which data needs to be written to disk quickly and efficiently.
- Stenographer is also best suited for mission requirements in which PCAP files are not excessively large due to retention policies. 


### Arkime
- Arkime exposes APIs and augments security infrastructure to store and index network traffic, providing fast, indexed access.
- The network traffic is captured in standard PCAP format and is cataloged in an Elasticsearch database, which provides fast indexed access.
- Because Arkime stores and exports all packets in standard PCAP format, such PCAP-ingesting tools as Wireshark can be used with Arkime for deep traffic analysis.
- Arkime can ingest the PCAP data and visually display it in different graph formats, such as histograms or line graphs, to easily parse through network data for detecting outliers within a network environment.
- Arkime is best suited for an FPC system with metadata parsing and searching and can be scaled to handle tens of gigabits per second of traffic. 
- PCAP retention is based on available sensor disk space within the network environment.
- Metadata retention is based on the Elasticsearch cluster scale, but these settings can be altered. 
- Arkime consists of three components:
  - **Network Traffic Capture System**: Monitors network traffic, writes PCAP files to disk (permitted storage), parses data, and sends metadata to Elasticsearch.
  - **Viewer**: Transfers PCAP files through Elasticsearch metadata.
  - **Elasticsearch**: Provides statistics and network visualizations of network data from PCAP. 

#### Requirements
- To use Arkime efficiently, minimum system requirements must be met.
- However, this is dependent on traffic rates and data retention.
- Minimum system requirements are as follows:
  - Enough disk space pertinent to the network environment.
  - Total bandwidth must include Receive (RX) and Transmit (TX) bandwidth.
  - Minimum 6 cores.
  - At least 16 GB of memory. 
- The amount of disk space needed is based on the average network traffic rate, the number of days of data retention required, the number of machines being used to capture data, and the average amount of traffic each machine can handle. 

#### Pros and Cons
<img width="692" height="387" alt="image" src="https://github.com/user-attachments/assets/13818c89-65a6-4776-9bdb-74e8d5b6bf4a" />

#### Best Capabilities
- Arkime uses Viewer to provide data tables to CDAs based on a specific time range within a PCAP file.
- These data tables are best suited to parse and analyze network traffic.
- Below are some best capabilities that make Arkime efficient to use as a CDA. 

##### Sessions
- The primary view is the Sessions page, which contains many controls for filtering sessions.
- When viewing Arkime session details, an additional packets section is visible under the metadata sections denoted by the green plus (+) icon.
- When expanded, the PCAP data can be analyzed.  
<img width="2001" height="1198" alt="61b1a7f8-7053-483e-9ea6-e5cb3421c6f3" src="https://github.com/user-attachments/assets/dc1c9f99-513d-4056-86bd-fa17e836484c" />

##### SPIView
- Session Profile Information (SPI) displays unique values with session counts for each of the captured fields.
- This page lists session/log metrics such as protocol, source and destination IP addresses, and source and destination ports.
- The values can be expanded for further analysis to display the top network values along with each value’s cardinality for the fields of interest.
<img width="2001" height="1189" alt="d623eaf6-c6fb-4299-ba86-dc8bfb1f84ad" src="https://github.com/user-attachments/assets/882169be-aa98-419a-8116-db769af64ec5" />

##### SPIGraph
- SPIGraph displays a temporal view for the top values of any field.
- The top values are visually represented over time as well as geographically.
- This can be useful for identifying trends within a network environment over time.
- For example, traffic using a particular protocol when seen sparsely at regular intervals on the histogram may indicate an IP check by an adversary, polling, or beaconing. 
<img width="2001" height="1189" alt="1b12d4cc-b2b9-4036-b5df-a3c5843839f5" src="https://github.com/user-attachments/assets/c5c2db75-50d2-478a-a762-970426e5d9d7" />

##### Connections
- Connections displays the connection points of the IP addresses communicating across the network environment.
- This makes it easy to visualize logical relationships between network hosts. 
<img width="2001" height="1189" alt="e2fbe201-1bf0-41cb-9265-f075ce9154eb" src="https://github.com/user-attachments/assets/bf197568-cc4a-44d1-9079-852f84e8360a" />

##### Files
- File sizes of PCAPs on disk, imported PCAPs, and all files that are viewable can be seen under the Files tab. This is useful for resource purposes.
<img width="2001" height="805" alt="cb760402-665f-4a37-bd89-eee0a54b1fc9" src="https://github.com/user-attachments/assets/c23fed19-8498-47f3-bbf9-f6141dc767de" />

##### Stats
- Statistics for each node and index may be seen under the Stats tab.
- The sub-tabs that follow, such as Capture Stats, display capture resources that a certain FPC uses. 
<img width="2001" height="412" alt="4234f2ce-83c2-4b28-890c-fd45bcadd49b" src="https://github.com/user-attachments/assets/6ed98d45-06fb-40e6-8312-5f713f0483e7" />

#### Summary
- Arkime can be used to apply search queries to analyze suspicious network activity within a mission partner’s network environment.
- Arkime also parses Hypertext Transfer Protocol (HTTP) source content and decodes data.
- This assists trainees in identifying MCA across a network while identifying OSs of a network device based on traffic and pinpointing OS fingerprinting activities.

### Wireshark
- Wireshark captures – or sniffs – the network to view traffic that goes through a network adapter.
- Network data streams are sent to specific addresses but can be seen by using a packet sniffer on a network interface that is in promiscuous mode, which informs the NIC to accept all traffic it sees on the network.
- **Libpcap** and **WinPcap** are the kernel-level drivers that enable sniffing. 
- Wireshark first captures the data from a network interface and then processes the capture by breaking it down into segments, packets, and frames.
- The data is then presented in the context of addressing, protocols, and data.
- Wireshark provides analysts with an assortment of options to evaluate network performance graphically based on multiple variables.

#### Requirements
- To use the Wireshark FPC tool effectively, minimum system requirements must be met.
- However, this is dependent on traffic rates and data retention.
- The minimum system requirements are as follows:
  - 32-bit x86 or 64-bit CPU.
  - At least 400 MB of available RAM; must be more to retain larger PCAP files.
  - At least 300 MB of available storage space; must be more to retain larger PCAP files.
  - NIC that supports promiscuous mode.
  - WinPcap/libpcap capture driver.
  - Use Cases and Limitations
- Wireshark can handle a wide range of protocols, but its primary functions are to capture packets and display them.
- Like any other tool, Wireshark is best suited when it is the right tool for the job.
- Below are scenarios when the use of Wireshark is ideal:
  - To search for a certain protocol or stream between devices.
  - To search for the root cause of a known problem.
  - To analyze time stamps, protocol flags, or bits.
- Although not ideal, Wireshark can also be used for the following:
  - To discover which devices or protocols are top talkers.
  - To view a rough network traffic picture.
  - To analyze conversations and endpoints between devices.
- Wireshark tends to require large amounts of storage for large PCAP files, making it less than ideal in some situations.   

#### Pros and Cons
<img width="691" height="271" alt="image" src="https://github.com/user-attachments/assets/d9ed9dd2-b5db-4ad0-89f1-6d652c790b11" />

#### Summary
- Wireshark may be ideal for determining the root cause of an understood problem.
- However, the tool may not be ideal for browsing network traffic or making high-level judgments about a mission partner’s network.
- Wireshark can be overwhelming to use and is best used when the problem or situation is understood beforehand. 


### Tcpdump
- Tcpdump is a popular FPC and analysis tool included in the JDMSS kit.
- It is useful for command-line packet analysis to parse through data rapidly.
- Tcpdump is integrated into Security Onion but not enabled by default.
- The libpcap library allows Tcpdump the ability to capture packets from the wire, analyze packets, troubleshoot connection issues, and apply filters as it uses the BPF syntax. 

#### Pros and Cons
<img width="693" height="321" alt="image" src="https://github.com/user-attachments/assets/34a84c20-027a-4502-a7a4-eb059bd732ec" />

#### Use Cases and Limitations
- Tcpdump can handle a wide range of protocols.
- Unfortunately, some limitations exist when it comes to using Tcpdump, as the tool relies on the analyst for much of the interpretation of individual packets.
- This may seem counterintuitive, but it challenges the analyst and provides a fundamental understanding that can be better applied to any packet analysis tool. 

##### Uses
- Uses of Tcpdump include the following:
  - A web browser is hanging and cannot load pages from its server. Run Tcpdump to check the following:
    - Domain Name System (DNS) query
    - HTTP request to server
    - Server response
- Debug an attack, such as Denial of Service (DoS). Run Tcpdump to check the following:
  - Source address
  - Destination address
  - Type of traffic

##### Tcpdump on an Interface
- When running Tcpdump on an interface, the packet is copied on a switch ingress to the Switch Card Control Processor (SCCP), which then sends the packet to the host to be captured. 

##### Limitations
- One of Tcpdump’s greatest limitations is that it does not use Wireshark’s protocol dissectors.
- This means that Tcpdump does not interpret Layer 7 protocol information and is limited by network hardware.
- When running Tcpdump, a hardware switch interface **limits packets to 200 per second**.
- Therefore, if Tcpdump is running on an interface processing more than 200 packets per second, the output PCAP file does not include all the packets.
- A capture filter cannot be used to avoid this limitation, as it refers to the total volume of packets traversing the interface at the moment of capture. 


### Read Packets from a File
- If CDAs are engaged reading network data from large PCAP files, it may become imperative to use filters to parse through relevant data only.
- TCP uses the BPF format like Wireshark capture filters. 

#### BPF
- The BPF syntax uses primitives, which consist of a single statement combined with one or more qualifiers, followed by a value such as a name or ID number.
- Primitives refer to a section of a protocol header such as port, host, or TCP port.
- Table 9.1-5 details which qualifiers are used to form an expression.
  ![c1874f3c-c79d-4ac8-9473-36a3cb392f21](https://github.com/user-attachments/assets/7808e2cf-5deb-422e-8f95-25e0b557431f)

  ![7615574b-c0f8-46a3-85af-456738be9f68](https://github.com/user-attachments/assets/6b6f4bcf-ddfc-4ee8-9de3-1235ff4a3507)

#### TCP
<img width="694" height="77" alt="image" src="https://github.com/user-attachments/assets/3f4c2fb3-b99a-4992-88b6-2c185294017c" />

#### UDP
<img width="613" height="60" alt="image" src="https://github.com/user-attachments/assets/b1f509ca-88c8-425a-85de-7b3d77753110" />

##### tcpdump SYNTAX
- Read the network traffic from the PCAP: `tcpdump -nr hunt.pcap`
- Triple the Verbosity: `tcpdump -nvvvr hunt.pcap`
- Display each packet in hex: `tcpdump -nvxr hunt.pcap`
- Display in ASCII and hex: `tcpdump -nvXr hunt.pcap`
- Create a new file containing only packets matching BPF Filter: `tcpdump -nvr hunt.pcap 'tcp dst port 80' -w hunt-2.pcap`
- Read newly created file: `tcpdump -nvr hunt-2.pcap`
- 
